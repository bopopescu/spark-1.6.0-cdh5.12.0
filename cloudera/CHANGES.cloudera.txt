commit 442ab5bbc1e2749adf0700eb542446db9d746c70
Author: Marcelo Vanzin <vanzin@cloudera.com>
Date:   Fri Jun 2 16:30:52 2017 -0700

    CDH-54549. Propagate user-defined Hadoop config to Hive client.
    
    (cherry picked from commit 968431f6c81790b516c16006f67626d6d7c9ddf8)

commit 686984455c2758bd4baae7c9f082f85e13b60743
Author: Marcelo Vanzin <vanzin@cloudera.com>
Date:   Thu Jun 1 16:45:31 2017 -0700

    [SPARK-20922][CORE][HOTFIX] Don't use Java 8 lambdas in older branches.
    
    Author: Marcelo Vanzin <vanzin@cloudera.com>
    
    Closes #18178 from vanzin/SPARK-20922-hotfix.
    
    (cherry picked from commit 0b25a7d93359e348e11b2e8698990a53436b3c5d)
    
    Cloudera ID: CDH-54803
    
    (cherry picked from commit 1c0f8ca7d9e86114cc81300258f22f77d8f9efd1)

commit 24ad538f5815245e072fc0ea78cf1b1de6bbbf6a
Author: Marcelo Vanzin <vanzin@cloudera.com>
Date:   Thu Jun 1 14:44:34 2017 -0700

    [SPARK-20922][CORE] Add whitelist of classes that can be deserialized by the launcher.
    
    Blindly deserializing classes using Java serialization opens the code up to
    issues in other libraries, since just deserializing data from a stream may
    end up execution code (think readObject()).
    
    Since the launcher protocol is pretty self-contained, there's just a handful
    of classes it legitimately needs to deserialize, and they're in just two
    packages, so add a filter that throws errors if classes from any other
    package show up in the stream.
    
    This also maintains backwards compatibility (the updated launcher code can
    still communicate with the backend code in older Spark releases).
    
    Tested with new and existing unit tests.
    
    Author: Marcelo Vanzin <vanzin@cloudera.com>
    
    Closes #18166 from vanzin/SPARK-20922.
    
    (cherry picked from commit 8efc6e986554ae66eab93cd64a9035d716adbab0)
    
    Cloudera ID: CDH-54803
    
    (cherry picked from commit 5cc0c5b29e6dae307e8e876a2f197abcf74bcaee)

commit f5a85ed79f119cc7a66525a08c82422bab83c186
Author: Jenkins <dev-kitchen@cloudera.com>
Date:   Thu May 25 10:26:43 2017 -0700

    Branch for CDH5.12.0

commit d219c4f107e59e826662d95ccd538a49c88e2a00
Author: Jenkins <dev-kitchen@cloudera.com>
Date:   Thu May 25 09:49:01 2017 -0700

    Branch for CDH5.12.x

commit cdc2b81238bf31aff1ccc5d2e25cc70d9ecb2536
Author: Peter Ableda <abledapeter@gmail.com>
Date:   Thu Jun 23 09:00:31 2016 -0500

    [SPARK-16138] Try to cancel executor requests only if we have at least 1
    
    Adding additional check to if statement
    
    I built and deployed to internal cluster to observe behaviour. After the change the invalid logging is gone:
    
    ```
    16/06/22 08:46:36 INFO yarn.YarnAllocator: Driver requested a total number of 1 executor(s).
    16/06/22 08:46:36 INFO yarn.YarnAllocator: Canceling requests for 1 executor container(s) to have a new desired total 1 executors.
    16/06/22 08:46:36 INFO yarn.YarnAllocator: Driver requested a total number of 0 executor(s).
    16/06/22 08:47:36 INFO yarn.ApplicationMaster$AMEndpoint: Driver requested to kill executor(s) 1.
    ```
    
    Author: Peter Ableda <abledapeter@gmail.com>
    
    Closes #13850 from peterableda/patch-2.
    
    (cherry picked from commit 5bf2889bfcfd776e7ee1369443a0474421a800bd)
    
    Cloudera ID: CDH-53104

commit 636449f4da07f5048c7a24c293d2ed04604ac8a1
Author: Imran Rashid <irashid@cloudera.com>
Date:   Mon May 22 14:31:08 2017 -0500

    Revert "[SPARK-12297][SQL] Hive compatibility for Parquet Timestamps"
    
    This reverts commit e7e4f8ca5603316729da81c29ce203c4f088732d.
    
    This is being reverted because the feature will not be included in
    CDH5.12 for any component, because of continued upstream discussion
    around the design.

commit f402bf58682db84e96799c197176ce122849b464
Author: Imran Rashid <irashid@cloudera.com>
Date:   Mon May 22 14:30:51 2017 -0500

    Revert "SPARK-12297.  Fix Parquet Timestamp interop for Impala."
    
    This reverts commit a5f8ab5d2c13344dc29b3d1bbd7d514ac3131cc3.
    
    This is being reverted because the feature will not be included in
    CDH5.12 for any component, because of continued upstream discussion
    around the design.

commit d00a8df61523622fa8a8bf0e62f213ab9fa48b35
Author: Imran Rashid <irashid@cloudera.com>
Date:   Mon May 22 14:26:20 2017 -0500

    Revert "SPARK-12297.  Fix Failing Unit test for Parquet Timestamps."
    
    This reverts commit c677303f423d433a16f74c7b78ea015bffcdb347.
    
    This is being reverted because the feature will not be included in
    CDH5.12 for any component, because of continued upstream discussion
    around the design.

commit c677303f423d433a16f74c7b78ea015bffcdb347
Author: Imran Rashid <irashid@cloudera.com>
Date:   Thu May 11 16:00:12 2017 -0500

    SPARK-12297.  Fix Failing Unit test for Parquet Timestamps.
    
    The last change introduced a new entry to hadoop conf which I forgot to
    update in this unit test in the last patch.
    
    Cloudera ID: CDH-53738

commit a5f8ab5d2c13344dc29b3d1bbd7d514ac3131cc3
Author: Imran Rashid <irashid@cloudera.com>
Date:   Wed May 10 10:08:47 2017 -0500

    SPARK-12297.  Fix Parquet Timestamp interop for Impala.
    
    Impala never applies conversion when writing parquet data, regardless of
    the table property, so do not use the table property if the file was
    created by Impala.
    
    Cloudera ID: CDH-53609

commit fb8cf397c32b87ad990621e0e86b448f2e1c459a
Author: Marcelo Vanzin <vanzin@cloudera.com>
Date:   Wed May 3 16:00:48 2017 -0700

    CDH-38538. Add Spark config for choosing locality of the YARN AM.
    
    Two options are added that allow the user to control where to launch
    the YARN AM (one for client mode, one for cluster mode). The code
    is basically a Scala translation of the code added in MAPREDUCE-6871.

commit f8cf3584cc3f03b3c7ac7f68988df8d2f405bc11
Author: hyukjinkwon <gurwls223@gmail.com>
Date:   Tue Jan 10 13:19:21 2017 +0000

    [SPARK-18922][SQL][CORE][STREAMING][TESTS] Fix all identified tests failed due to path and resource-not-closed problems on Windows
    
    This PR proposes to fix all the test failures identified by testing with AppVeyor.
    
    **Scala - aborted tests**
    
    ```
    WindowQuerySuite:
      Exception encountered when attempting to run a suite with class name: org.apache.spark.sql.hive.execution.WindowQuerySuite *** ABORTED *** (156 milliseconds)
       org.apache.spark.sql.AnalysisException: LOAD DATA input path does not exist: C:projectssparksqlhive   argetscala-2.11   est-classesdatafilespart_tiny.txt;
    
    OrcSourceSuite:
     Exception encountered when attempting to run a suite with class name: org.apache.spark.sql.hive.orc.OrcSourceSuite *** ABORTED *** (62 milliseconds)
       org.apache.spark.sql.AnalysisException: org.apache.hadoop.hive.ql.metadata.HiveException: MetaException(message:java.lang.IllegalArgumentException: Can not create a Path from an empty string);
    
    ParquetMetastoreSuite:
     Exception encountered when attempting to run a suite with class name: org.apache.spark.sql.hive.ParquetMetastoreSuite *** ABORTED *** (4 seconds, 703 milliseconds)
       org.apache.spark.sql.AnalysisException: org.apache.hadoop.hive.ql.metadata.HiveException: MetaException(message:java.lang.IllegalArgumentException: Can not create a Path from an empty string);
    
    ParquetSourceSuite:
     Exception encountered when attempting to run a suite with class name: org.apache.spark.sql.hive.ParquetSourceSuite *** ABORTED *** (3 seconds, 907 milliseconds)
       org.apache.spark.sql.AnalysisException: Path does not exist: file:/C:projectsspark  arget mpspark-581a6575-454f-4f21-a516-a07f95266143;
    
    KafkaRDDSuite:
     Exception encountered when attempting to run a suite with class name: org.apache.spark.streaming.kafka.KafkaRDDSuite *** ABORTED *** (5 seconds, 212 milliseconds)
       java.io.IOException: Failed to delete: C:\projects\spark\target\tmp\spark-4722304d-213e-4296-b556-951df1a46807
    
    DirectKafkaStreamSuite:
     Exception encountered when attempting to run a suite with class name: org.apache.spark.streaming.kafka.DirectKafkaStreamSuite *** ABORTED *** (7 seconds, 127 milliseconds)
       java.io.IOException: Failed to delete: C:\projects\spark\target\tmp\spark-d0d3eba7-4215-4e10-b40e-bb797e89338e
       at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1010)
    
    ReliableKafkaStreamSuite
     Exception encountered when attempting to run a suite with class name: org.apache.spark.streaming.kafka.ReliableKafkaStreamSuite *** ABORTED *** (5 seconds, 498 milliseconds)
       java.io.IOException: Failed to delete: C:\projects\spark\target\tmp\spark-d33e45a0-287e-4bed-acae-ca809a89d888
    
    KafkaStreamSuite:
     Exception encountered when attempting to run a suite with class name: org.apache.spark.streaming.kafka.KafkaStreamSuite *** ABORTED *** (2 seconds, 892 milliseconds)
       java.io.IOException: Failed to delete: C:\projects\spark\target\tmp\spark-59c9d169-5a56-4519-9ef0-cefdbd3f2e6c
    
    KafkaClusterSuite:
     Exception encountered when attempting to run a suite with class name: org.apache.spark.streaming.kafka.KafkaClusterSuite *** ABORTED *** (1 second, 690 milliseconds)
       java.io.IOException: Failed to delete: C:\projects\spark\target\tmp\spark-3ef402b0-8689-4a60-85ae-e41e274f179d
    
    DirectKafkaStreamSuite:
     Exception encountered when attempting to run a suite with class name: org.apache.spark.streaming.kafka010.DirectKafkaStreamSuite *** ABORTED *** (59 seconds, 626 milliseconds)
       java.io.IOException: Failed to delete: C:\projects\spark\target\tmp\spark-426107da-68cf-4d94-b0d6-1f428f1c53f6
    
    KafkaRDDSuite:
    Exception encountered when attempting to run a suite with class name: org.apache.spark.streaming.kafka010.KafkaRDDSuite *** ABORTED *** (2 minutes, 6 seconds)
       java.io.IOException: Failed to delete: C:\projects\spark\target\tmp\spark-b9ce7929-5dae-46ab-a0c4-9ef6f58fbc2
    ```
    
    **Java - failed tests**
    
    ```
    Test org.apache.spark.streaming.kafka.JavaKafkaRDDSuite.testKafkaRDD failed: java.io.IOException: Failed to delete: C:\projects\spark\target\tmp\spark-1cee32f4-4390-4321-82c9-e8616b3f0fb0, took 9.61 sec
    
    Test org.apache.spark.streaming.kafka.JavaKafkaStreamSuite.testKafkaStream failed: java.io.IOException: Failed to delete: C:\projects\spark\target\tmp\spark-f42695dd-242e-4b07-847c-f299b8e4676e, took 11.797 sec
    
    Test org.apache.spark.streaming.kafka.JavaDirectKafkaStreamSuite.testKafkaStream failed: java.io.IOException: Failed to delete: C:\projects\spark\target\tmp\spark-85c0d062-78cf-459c-a2dd-7973572101ce, took 1.581 sec
    
    Test org.apache.spark.streaming.kafka010.JavaKafkaRDDSuite.testKafkaRDD failed: java.io.IOException: Failed to delete: C:\projects\spark\target\tmp\spark-49eb6b5c-8366-47a6-83f2-80c443c48280, took 17.895 sec
    
    org.apache.spark.streaming.kafka010.JavaDirectKafkaStreamSuite.testKafkaStream failed: java.io.IOException: Failed to delete: C:\projects\spark\target\tmp\spark-898cf826-d636-4b1c-a61a-c12a364c02e7, took 8.858 sec
    ```
    
    **Scala - failed tests**
    
    ```
    PartitionProviderCompatibilitySuite:
     - insert overwrite partition of new datasource table overwrites just partition *** FAILED *** (828 milliseconds)
       java.io.IOException: Failed to delete: C:\projects\spark\target\tmp\spark-bb6337b9-4f99-45ab-ad2c-a787ab965c09
    
     - SPARK-18635 special chars in partition values - partition management true *** FAILED *** (5 seconds, 360 milliseconds)
       org.apache.spark.sql.AnalysisException: org.apache.hadoop.hive.ql.metadata.HiveException: MetaException(message:java.lang.IllegalArgumentException: Can not create a Path from an empty string);
    
     - SPARK-18635 special chars in partition values - partition management false *** FAILED *** (141 milliseconds)
       org.apache.spark.sql.AnalysisException: org.apache.hadoop.hive.ql.metadata.HiveException: MetaException(message:java.lang.IllegalArgumentException: Can not create a Path from an empty string);
    ```
    
    ```
    UtilsSuite:
     - reading offset bytes of a file (compressed) *** FAILED *** (0 milliseconds)
       java.io.IOException: Failed to delete: C:\projects\spark\target\tmp\spark-ecb2b7d5-db8b-43a7-b268-1bf242b5a491
    
     - reading offset bytes across multiple files (compressed) *** FAILED *** (0 milliseconds)
       java.io.IOException: Failed to delete: C:\projects\spark\target\tmp\spark-25cc47a8-1faa-4da5-8862-cf174df63ce0
    ```
    
    ```
    StatisticsSuite:
     - MetastoreRelations fallback to HDFS for size estimation *** FAILED *** (110 milliseconds)
       org.apache.spark.sql.catalyst.analysis.NoSuchTableException: Table or view 'csv_table' not found in database 'default';
    ```
    
    ```
    SQLQuerySuite:
     - permanent UDTF *** FAILED *** (125 milliseconds)
       org.apache.spark.sql.AnalysisException: Undefined function: 'udtf_count_temp'. This function is neither a registered temporary function nor a permanent function registered in the database 'default'.; line 1 pos 24
    
     - describe functions - user defined functions *** FAILED *** (125 milliseconds)
       org.apache.spark.sql.AnalysisException: Undefined function: 'udtf_count'. This function is neither a registered temporary function nor a permanent function registered in the database 'default'.; line 1 pos 7
    
     - CTAS without serde with location *** FAILED *** (16 milliseconds)
       java.lang.IllegalArgumentException: java.net.URISyntaxException: Relative path in absolute URI: file:C:projectsspark%09arget%09mpspark-ed673d73-edfc-404e-829e-2e2b9725d94e/c1
    
     - derived from Hive query file: drop_database_removes_partition_dirs.q *** FAILED *** (47 milliseconds)
       java.lang.IllegalArgumentException: java.net.URISyntaxException: Relative path in absolute URI: file:C:projectsspark%09arget%09mpspark-d2ddf08e-699e-45be-9ebd-3dfe619680fe/drop_database_removes_partition_dirs_table
    
     - derived from Hive query file: drop_table_removes_partition_dirs.q *** FAILED *** (0 milliseconds)
       java.lang.IllegalArgumentException: java.net.URISyntaxException: Relative path in absolute URI: file:C:projectsspark%09arget%09mpspark-d2ddf08e-699e-45be-9ebd-3dfe619680fe/drop_table_removes_partition_dirs_table2
    
     - SPARK-17796 Support wildcard character in filename for LOAD DATA LOCAL INPATH *** FAILED *** (109 milliseconds)
       java.nio.file.InvalidPathException: Illegal char <:> at index 2: /C:/projects/spark/sql/hive/projectsspark	arget	mpspark-1a122f8c-dfb3-46c4-bab1-f30764baee0e/*part-r*
    ```
    
    ```
    HiveDDLSuite:
     - drop external tables in default database *** FAILED *** (16 milliseconds)
       org.apache.spark.sql.AnalysisException: org.apache.hadoop.hive.ql.metadata.HiveException: MetaException(message:java.lang.IllegalArgumentException: Can not create a Path from an empty string);
    
     - add/drop partitions - external table *** FAILED *** (16 milliseconds)
       org.apache.spark.sql.AnalysisException: org.apache.hadoop.hive.ql.metadata.HiveException: MetaException(message:java.lang.IllegalArgumentException: Can not create a Path from an empty string);
    
     - create/drop database - location without pre-created directory *** FAILED *** (16 milliseconds)
       org.apache.spark.sql.AnalysisException: org.apache.hadoop.hive.ql.metadata.HiveException: MetaException(message:java.lang.IllegalArgumentException: Can not create a Path from an empty string);
    
     - create/drop database - location with pre-created directory *** FAILED *** (32 milliseconds)
       org.apache.spark.sql.AnalysisException: org.apache.hadoop.hive.ql.metadata.HiveException: MetaException(message:java.lang.IllegalArgumentException: Can not create a Path from an empty string);
    
     - drop database containing tables - CASCADE *** FAILED *** (94 milliseconds)
       CatalogDatabase(db1,,file:/C:/projects/spark/target/tmp/warehouse-d0665ee0-1e39-4805-b471-0b764f7838be/db1.db,Map()) did not equal CatalogDatabase(db1,,file:C:/projects/spark/target/tmp/warehouse-d0665ee0-1e39-4805-b471-0b764f7838be\db1.db,Map()) (HiveDDLSuite.scala:675)
    
     - drop an empty database - CASCADE *** FAILED *** (63 milliseconds)
       CatalogDatabase(db1,,file:/C:/projects/spark/target/tmp/warehouse-d0665ee0-1e39-4805-b471-0b764f7838be/db1.db,Map()) did not equal CatalogDatabase(db1,,file:C:/projects/spark/target/tmp/warehouse-d0665ee0-1e39-4805-b471-0b764f7838be\db1.db,Map()) (HiveDDLSuite.scala:675)
    
     - drop database containing tables - RESTRICT *** FAILED *** (47 milliseconds)
       CatalogDatabase(db1,,file:/C:/projects/spark/target/tmp/warehouse-d0665ee0-1e39-4805-b471-0b764f7838be/db1.db,Map()) did not equal CatalogDatabase(db1,,file:C:/projects/spark/target/tmp/warehouse-d0665ee0-1e39-4805-b471-0b764f7838be\db1.db,Map()) (HiveDDLSuite.scala:675)
    
     - drop an empty database - RESTRICT *** FAILED *** (47 milliseconds)
       CatalogDatabase(db1,,file:/C:/projects/spark/target/tmp/warehouse-d0665ee0-1e39-4805-b471-0b764f7838be/db1.db,Map()) did not equal CatalogDatabase(db1,,file:C:/projects/spark/target/tmp/warehouse-d0665ee0-1e39-4805-b471-0b764f7838be\db1.db,Map()) (HiveDDLSuite.scala:675)
    
     - CREATE TABLE LIKE an external data source table *** FAILED *** (140 milliseconds)
       org.apache.spark.sql.AnalysisException: Path does not exist: file:/C:projectsspark	arget	mpspark-c5eba16d-07ae-4186-95bb-21c5811cf888;
    
     - CREATE TABLE LIKE an external Hive serde table *** FAILED *** (16 milliseconds)
       org.apache.spark.sql.AnalysisException: org.apache.hadoop.hive.ql.metadata.HiveException: MetaException(message:java.lang.IllegalArgumentException: Can not create a Path from an empty string);
    
     - desc table for data source table - no user-defined schema *** FAILED *** (125 milliseconds)
       org.apache.spark.sql.AnalysisException: Path does not exist: file:/C:projectsspark	arget	mpspark-e8bf5bf5-721a-4cbe-9d6	at scala.collection.immutable.List.foreach(List.scala:381)d-5543a8301c1d;
    ```
    
    ```
    MetastoreDataSourcesSuite
     - CTAS: persisted bucketed data source table *** FAILED *** (16 milliseconds)
       java.lang.IllegalArgumentException: Can not create a Path from an empty string
    ```
    
    ```
    ShowCreateTableSuite:
     - simple external hive table *** FAILED *** (0 milliseconds)
       org.apache.spark.sql.AnalysisException: org.apache.hadoop.hive.ql.metadata.HiveException: MetaException(message:java.lang.IllegalArgumentException: Can not create a Path from an empty string);
    ```
    
    ```
    PartitionedTablePerfStatsSuite:
     - hive table: partitioned pruned table reports only selected files *** FAILED *** (313 milliseconds)
       org.apache.spark.sql.AnalysisException: org.apache.hadoop.hive.ql.metadata.HiveException: MetaException(message:java.lang.IllegalArgumentException: Can not create a Path from an empty string);
    
     - datasource table: partitioned pruned table reports only selected files *** FAILED *** (219 milliseconds)
       org.apache.spark.sql.AnalysisException: Path does not exist: file:/C:projectsspark	arget	mpspark-311f45f8-d064-4023-a4bb-e28235bff64d;
    
     - hive table: lazy partition pruning reads only necessary partition data *** FAILED *** (203 milliseconds)
       org.apache.spark.sql.AnalysisException: org.apache.hadoop.hive.ql.metadata.HiveException: MetaException(message:java.lang.IllegalArgumentException: Can not create a Path from an empty string);
    
     - datasource table: lazy partition pruning reads only necessary partition data *** FAILED *** (187 milliseconds)
       org.apache.spark.sql.AnalysisException: Path does not exist: file:/C:projectsspark	arget	mpspark-fde874ca-66bd-4d0b-a40f-a043b65bf957;
    
     - hive table: lazy partition pruning with file status caching enabled *** FAILED *** (188 milliseconds)
       org.apache.spark.sql.AnalysisException: org.apache.hadoop.hive.ql.metadata.HiveException: MetaException(message:java.lang.IllegalArgumentException: Can not create a Path from an empty string);
    
     - datasource table: lazy partition pruning with file status caching enabled *** FAILED *** (187 milliseconds)
       org.apache.spark.sql.AnalysisException: Path does not exist: file:/C:projectsspark	arget	mpspark-e6d20183-dd68-4145-acbe-4a509849accd;
    
     - hive table: file status caching respects refresh table and refreshByPath *** FAILED *** (172 milliseconds)
       org.apache.spark.sql.AnalysisException: org.apache.hadoop.hive.ql.metadata.HiveException: MetaException(message:java.lang.IllegalArgumentException: Can not create a Path from an empty string);
    
     - datasource table: file status caching respects refresh table and refreshByPath *** FAILED *** (203 milliseconds)
       org.apache.spark.sql.AnalysisException: Path does not exist: file:/C:projectsspark	arget	mpspark-8b2c9651-2adf-4d58-874f-659007e21463;
    
     - hive table: file status cache respects size limit *** FAILED *** (219 milliseconds)
       org.apache.spark.sql.AnalysisException: org.apache.hadoop.hive.ql.metadata.HiveException: MetaException(message:java.lang.IllegalArgumentException: Can not create a Path from an empty string);
    
     - datasource table: file status cache respects size limit *** FAILED *** (171 milliseconds)
       org.apache.spark.sql.AnalysisException: Path does not exist: file:/C:projectsspark	arget	mpspark-7835ab57-cb48-4d2c-bb1d-b46d5a4c47e4;
    
     - datasource table: table setup does not scan filesystem *** FAILED *** (266 milliseconds)
       org.apache.spark.sql.AnalysisException: Path does not exist: file:/C:projectsspark	arget	mpspark-20598d76-c004-42a7-8061-6c56f0eda5e2;
    
     - hive table: table setup does not scan filesystem *** FAILED *** (266 milliseconds)
       org.apache.spark.sql.AnalysisException: org.apache.hadoop.hive.ql.metadata.HiveException: MetaException(message:java.lang.IllegalArgumentException: Can not create a Path from an empty string);
    
     - hive table: num hive client calls does not scale with partition count *** FAILED *** (2 seconds, 281 milliseconds)
       org.apache.spark.sql.AnalysisException: org.apache.hadoop.hive.ql.metadata.HiveException: MetaException(message:java.lang.IllegalArgumentException: Can not create a Path from an empty string);
    
     - datasource table: num hive client calls does not scale with partition count *** FAILED *** (2 seconds, 422 milliseconds)
       org.apache.spark.sql.AnalysisException: Path does not exist: file:/C:projectsspark	arget	mpspark-4cfed321-4d1d-4b48-8d34-5c169afff383;
    
     - hive table: files read and cached when filesource partition management is off *** FAILED *** (234 milliseconds)
       org.apache.spark.sql.AnalysisException: org.apache.hadoop.hive.ql.metadata.HiveException: MetaException(message:java.lang.IllegalArgumentException: Can not create a Path from an empty string);
    
     - datasource table: all partition data cached in memory when partition management is off *** FAILED *** (203 milliseconds)
       org.apache.spark.sql.AnalysisException: Path does not exist: file:/C:projectsspark	arget	mpspark-4bcc0398-15c9-4f6a-811e-12d40f3eec12;
    
     - SPARK-18700: table loaded only once even when resolved concurrently *** FAILED *** (1 second, 266 milliseconds)
       org.apache.spark.sql.AnalysisException: org.apache.hadoop.hive.ql.metadata.HiveException: MetaException(message:java.lang.IllegalArgumentException: Can not create a Path from an empty string);
    ```
    
    ```
    HiveSparkSubmitSuite:
     - temporary Hive UDF: define a UDF and use it *** FAILED *** (2 seconds, 94 milliseconds)
       java.io.IOException: Cannot run program "./bin/spark-submit" (in directory "C:\projects\spark"): CreateProcess error=2, The system cannot find the file specified
    
     - permanent Hive UDF: define a UDF and use it *** FAILED *** (281 milliseconds)
       java.io.IOException: Cannot run program "./bin/spark-submit" (in directory "C:\projects\spark"): CreateProcess error=2, The system cannot find the file specified
    
     - permanent Hive UDF: use a already defined permanent function *** FAILED *** (718 milliseconds)
       java.io.IOException: Cannot run program "./bin/spark-submit" (in directory "C:\projects\spark"): CreateProcess error=2, The system cannot find the file specified
    
     - SPARK-8368: includes jars passed in through --jars *** FAILED *** (3 seconds, 521 milliseconds)
       java.io.IOException: Cannot run program "./bin/spark-submit" (in directory "C:\projects\spark"): CreateProcess error=2, The system cannot find the file specified
    
     - SPARK-8020: set sql conf in spark conf *** FAILED *** (0 milliseconds)
       java.io.IOException: Cannot run program "./bin/spark-submit" (in directory "C:\projects\spark"): CreateProcess error=2, The system cannot find the file specified
    
     - SPARK-8489: MissingRequirementError during reflection *** FAILED *** (94 milliseconds)
       java.io.IOException: Cannot run program "./bin/spark-submit" (in directory "C:\projects\spark"): CreateProcess error=2, The system cannot find the file specified
    
     - SPARK-9757 Persist Parquet relation with decimal column *** FAILED *** (16 milliseconds)
       java.io.IOException: Cannot run program "./bin/spark-submit" (in directory "C:\projects\spark"): CreateProcess error=2, The system cannot find the file specified
    
     - SPARK-11009 fix wrong result of Window function in cluster mode *** FAILED *** (16 milliseconds)
       java.io.IOException: Cannot run program "./bin/spark-submit" (in directory "C:\projects\spark"): CreateProcess error=2, The system cannot find the file specified
    
     - SPARK-14244 fix window partition size attribute binding failure *** FAILED *** (78 milliseconds)
       java.io.IOException: Cannot run program "./bin/spark-submit" (in directory "C:\projects\spark"): CreateProcess error=2, The system cannot find the file specified
    
     - set spark.sql.warehouse.dir *** FAILED *** (16 milliseconds)
       java.io.IOException: Cannot run program "./bin/spark-submit" (in directory "C:\projects\spark"): CreateProcess error=2, The system cannot find the file specified
    
     - set hive.metastore.warehouse.dir *** FAILED *** (15 milliseconds)
       java.io.IOException: Cannot run program "./bin/spark-submit" (in directory "C:\projects\spark"): CreateProcess error=2, The system cannot find the file specified
    
     - SPARK-16901: set javax.jdo.option.ConnectionURL *** FAILED *** (16 milliseconds)
       java.io.IOException: Cannot run program "./bin/spark-submit" (in directory "C:\projects\spark"): CreateProcess error=2, The system cannot find the file specified
    
     - SPARK-18360: default table path of tables in default database should depend on the location of default database *** FAILED *** (15 milliseconds)
       java.io.IOException: Cannot run program "./bin/spark-submit" (in directory "C:\projects\spark"): CreateProcess error=2, The system cannot find the file specified
    ```
    
    ```
    UtilsSuite:
     - resolveURIs with multiple paths *** FAILED *** (0 milliseconds)
       ".../jar3,file:/C:/pi.py[%23]py.pi,file:/C:/path%..." did not equal ".../jar3,file:/C:/pi.py[#]py.pi,file:/C:/path%..." (UtilsSuite.scala:468)
    ```
    
    ```
    CheckpointSuite:
     - recovery with file input stream *** FAILED *** (10 seconds, 205 milliseconds)
       The code passed to eventually never returned normally. Attempted 660 times over 10.014272499999999 seconds. Last failure message: Unexpected internal error near index 1
       \
        ^. (CheckpointSuite.scala:680)
    ```
    
    Manually via AppVeyor as below:
    
    **Scala - aborted tests**
    
    ```
    WindowQuerySuite - all passed
    OrcSourceSuite:
    - SPARK-18220: read Hive orc table with varchar column *** FAILED *** (4 seconds, 417 milliseconds)
      org.apache.spark.sql.execution.QueryExecutionException: FAILED: Execution Error, return code -101 from org.apache.hadoop.hive.ql.exec.mr.MapRedTask. org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z
      at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$runHive$1.apply(HiveClientImpl.scala:625)
    ParquetMetastoreSuite - all passed
    ParquetSourceSuite - all passed
    KafkaRDDSuite - all passed
    DirectKafkaStreamSuite - all passed
    ReliableKafkaStreamSuite - all passed
    KafkaStreamSuite - all passed
    KafkaClusterSuite - all passed
    DirectKafkaStreamSuite - all passed
    KafkaRDDSuite - all passed
    ```
    
    **Java - failed tests**
    
    ```
    org.apache.spark.streaming.kafka.JavaKafkaRDDSuite - all passed
    org.apache.spark.streaming.kafka.JavaDirectKafkaStreamSuite - all passed
    org.apache.spark.streaming.kafka.JavaKafkaStreamSuite - all passed
    org.apache.spark.streaming.kafka010.JavaDirectKafkaStreamSuite - all passed
    org.apache.spark.streaming.kafka010.JavaKafkaRDDSuite - all passed
    ```
    
    **Scala - failed tests**
    
    ```
    PartitionProviderCompatibilitySuite:
    - insert overwrite partition of new datasource table overwrites just partition (1 second, 953 milliseconds)
    - SPARK-18635 special chars in partition values - partition management true (6 seconds, 31 milliseconds)
    - SPARK-18635 special chars in partition values - partition management false (4 seconds, 578 milliseconds)
    ```
    
    ```
    UtilsSuite:
    - reading offset bytes of a file (compressed) (203 milliseconds)
    - reading offset bytes across multiple files (compressed) (0 milliseconds)
    ```
    
    ```
    StatisticsSuite:
    - MetastoreRelations fallback to HDFS for size estimation (94 milliseconds)
    ```
    
    ```
    SQLQuerySuite:
     - permanent UDTF (407 milliseconds)
     - describe functions - user defined functions (441 milliseconds)
     - CTAS without serde with location (2 seconds, 831 milliseconds)
     - derived from Hive query file: drop_database_removes_partition_dirs.q (734 milliseconds)
     - derived from Hive query file: drop_table_removes_partition_dirs.q (563 milliseconds)
     - SPARK-17796 Support wildcard character in filename for LOAD DATA LOCAL INPATH (453 milliseconds)
    ```
    
    ```
    HiveDDLSuite:
     - drop external tables in default database (3 seconds, 5 milliseconds)
     - add/drop partitions - external table (2 seconds, 750 milliseconds)
     - create/drop database - location without pre-created directory (500 milliseconds)
     - create/drop database - location with pre-created directory (407 milliseconds)
     - drop database containing tables - CASCADE (453 milliseconds)
     - drop an empty database - CASCADE (375 milliseconds)
     - drop database containing tables - RESTRICT (328 milliseconds)
     - drop an empty database - RESTRICT (391 milliseconds)
     - CREATE TABLE LIKE an external data source table (953 milliseconds)
     - CREATE TABLE LIKE an external Hive serde table (3 seconds, 782 milliseconds)
     - desc table for data source table - no user-defined schema (1 second, 150 milliseconds)
    ```
    
    ```
    MetastoreDataSourcesSuite
     - CTAS: persisted bucketed data source table (875 milliseconds)
    ```
    
    ```
    ShowCreateTableSuite:
     - simple external hive table (78 milliseconds)
    ```
    
    ```
    PartitionedTablePerfStatsSuite:
     - hive table: partitioned pruned table reports only selected files (1 second, 109 milliseconds)
    - datasource table: partitioned pruned table reports only selected files (860 milliseconds)
     - hive table: lazy partition pruning reads only necessary partition data (859 milliseconds)
     - datasource table: lazy partition pruning reads only necessary partition data (1 second, 219 milliseconds)
     - hive table: lazy partition pruning with file status caching enabled (875 milliseconds)
     - datasource table: lazy partition pruning with file status caching enabled (890 milliseconds)
     - hive table: file status caching respects refresh table and refreshByPath (922 milliseconds)
     - datasource table: file status caching respects refresh table and refreshByPath (640 milliseconds)
     - hive table: file status cache respects size limit (469 milliseconds)
     - datasource table: file status cache respects size limit (453 milliseconds)
     - datasource table: table setup does not scan filesystem (328 milliseconds)
     - hive table: table setup does not scan filesystem (313 milliseconds)
     - hive table: num hive client calls does not scale with partition count (5 seconds, 431 milliseconds)
     - datasource table: num hive client calls does not scale with partition count (4 seconds, 79 milliseconds)
     - hive table: files read and cached when filesource partition management is off (656 milliseconds)
     - datasource table: all partition data cached in memory when partition management is off (484 milliseconds)
     - SPARK-18700: table loaded only once even when resolved concurrently (2 seconds, 578 milliseconds)
    ```
    
    ```
    HiveSparkSubmitSuite:
     - temporary Hive UDF: define a UDF and use it (1 second, 745 milliseconds)
     - permanent Hive UDF: define a UDF and use it (406 milliseconds)
     - permanent Hive UDF: use a already defined permanent function (375 milliseconds)
     - SPARK-8368: includes jars passed in through --jars (391 milliseconds)
     - SPARK-8020: set sql conf in spark conf (156 milliseconds)
     - SPARK-8489: MissingRequirementError during reflection (187 milliseconds)
     - SPARK-9757 Persist Parquet relation with decimal column (157 milliseconds)
     - SPARK-11009 fix wrong result of Window function in cluster mode (156 milliseconds)
     - SPARK-14244 fix window partition size attribute binding failure (156 milliseconds)
     - set spark.sql.warehouse.dir (172 milliseconds)
     - set hive.metastore.warehouse.dir (156 milliseconds)
     - SPARK-16901: set javax.jdo.option.ConnectionURL (157 milliseconds)
     - SPARK-18360: default table path of tables in default database should depend on the location of default database (172 milliseconds)
    ```
    
    ```
    UtilsSuite:
     - resolveURIs with multiple paths (0 milliseconds)
    ```
    
    ```
    CheckpointSuite:
     - recovery with file input stream (4 seconds, 452 milliseconds)
    ```
    
    Note: after resolving the aborted tests, there is a test failure identified as below:
    
    ```
    OrcSourceSuite:
    - SPARK-18220: read Hive orc table with varchar column *** FAILED *** (4 seconds, 417 milliseconds)
      org.apache.spark.sql.execution.QueryExecutionException: FAILED: Execution Error, return code -101 from org.apache.hadoop.hive.ql.exec.mr.MapRedTask. org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z
      at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$runHive$1.apply(HiveClientImpl.scala:625)
    ```
    
    This does not look due to this problem so this PR does not fix it here.
    
    Author: hyukjinkwon <gurwls223@gmail.com>
    
    Closes #16451 from HyukjinKwon/all-path-resource-fixes.
    
    (cherry picked from commit 4e27578faa67c7a71a9b938aafbaf79bdbf36831)

commit 69306c2b96f552912b0871474d88dd192aa1cbf7
Author: Josh Rosen <joshrosen@databricks.com>
Date:   Wed Apr 27 11:23:26 2016 -0700

    [SPARK-14930][SPARK-13693] Fix race condition in CheckpointWriter.stop()
    
    CheckpointWriter.stop() is prone to a race condition: if one thread calls `stop()` right as a checkpoint write task begins to execute, that write task may become blocked when trying to access `fs`, the shared Hadoop FileSystem, since both the `fs` getter and `stop` method synchronize on the same lock. Here's a thread-dump excerpt which illustrates the problem:
    
    ```java
    "pool-31-thread-1" #156 prio=5 os_prio=31 tid=0x00007fea02cd2000 nid=0x5c0b waiting for monitor entry [0x000000013bc4c000]
       java.lang.Thread.State: BLOCKED (on object monitor)
        at org.apache.spark.streaming.CheckpointWriter.org$apache$spark$streaming$CheckpointWriter$$fs(Checkpoint.scala:302)
        - waiting to lock <0x00000007bf53ee78> (a org.apache.spark.streaming.CheckpointWriter)
        at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:224)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
    
    "pool-1-thread-1-ScalaTest-running-MapWithStateSuite" #11 prio=5 os_prio=31 tid=0x00007fe9ff879800 nid=0x5703 waiting on condition [0x000000012e54c000]
       java.lang.Thread.State: TIMED_WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00000007bf564568> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ThreadPoolExecutor.awaitTermination(ThreadPoolExecutor.java:1465)
        at org.apache.spark.streaming.CheckpointWriter.stop(Checkpoint.scala:291)
        - locked <0x00000007bf53ee78> (a org.apache.spark.streaming.CheckpointWriter)
        at org.apache.spark.streaming.scheduler.JobGenerator.stop(JobGenerator.scala:159)
        - locked <0x00000007bf53ea90> (a org.apache.spark.streaming.scheduler.JobGenerator)
        at org.apache.spark.streaming.scheduler.JobScheduler.stop(JobScheduler.scala:115)
        - locked <0x00000007bf53d3f0> (a org.apache.spark.streaming.scheduler.JobScheduler)
        at org.apache.spark.streaming.StreamingContext$$anonfun$stop$1.apply$mcV$sp(StreamingContext.scala:680)
        at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1219)
        at org.apache.spark.streaming.StreamingContext.stop(StreamingContext.scala:679)
        - locked <0x00000007bf516a70> (a org.apache.spark.streaming.StreamingContext)
        at org.apache.spark.streaming.StreamingContext.stop(StreamingContext.scala:644)
        - locked <0x00000007bf516a70> (a org.apache.spark.streaming.StreamingContext)
    [...]
    ```
    
    We can fix this problem by having `stop` and `fs` be synchronized on different locks: the synchronization on `stop` only needs to guard against multiple threads calling `stop` at the same time, whereas the synchronization on `fs` is only necessary for cross-thread visibility. There's only ever a single active checkpoint writer thread at a time, so we don't need to guard against concurrent access to `fs`. Thus, `fs` can simply become a `volatile` var, similar to `lastCheckpointTime`.
    
    This change should fix [SPARK-13693](https://issues.apache.org/jira/browse/SPARK-13693), a flaky `MapWithStateSuite` test suite which has recently been failing several times per day. It also results in a huge test speedup: prior to this patch, `MapWithStateSuite` took about 80 seconds to run, whereas it now runs in less than 10 seconds. For the `streaming` project's tests as a whole, they now run in ~220 seconds vs. ~354 before.
    
    /cc zsxwing and tdas for review.
    
    Author: Josh Rosen <joshrosen@databricks.com>
    
    Closes #12712 from JoshRosen/fix-checkpoint-writer-race.
    
    (cherry picked from commit 450136ec0dab16a12e514c842f9062a6979ee9aa)

commit 5ad4b20f5804197355163c2ce495b58f3d1075fb
Author: Shixiong Zhu <shixiong@databricks.com>
Date:   Sat Mar 5 15:26:27 2016 -0800

    [SPARK-13693][STREAMING][TESTS] Stop StreamingContext before deleting checkpoint dir
    
    ## What changes were proposed in this pull request?
    
    Stop StreamingContext before deleting checkpoint dir to avoid the race condition that deleting the checkpoint dir and writing checkpoint happen at the same time.
    
    The flaky test log is here: https://amplab.cs.berkeley.edu/jenkins/job/spark-master-test-sbt-hadoop-2.7/256/testReport/junit/org.apache.spark.streaming/MapWithStateSuite/_It_is_not_a_test_/
    
    ## How was this patch tested?
    
    unit tests
    
    Author: Shixiong Zhu <shixiong@databricks.com>
    
    Closes #11531 from zsxwing/SPARK-13693.
    
    (cherry picked from commit 8290004d94760c22d6d3ca8dda3003ac8644422f)

commit e7e4f8ca5603316729da81c29ce203c4f088732d
Author: Imran Rashid <irashid@cloudera.com>
Date:   Mon Mar 20 10:57:28 2017 -0500

    [SPARK-12297][SQL] Hive compatibility for Parquet Timestamps
    
    This change allows timestamps in parquet-based hive table to behave as a
    "floating time", without a timezone, as timestamps are for other file
    formats. If the storage timezone is the same as the session timezone,
    this conversion is a no-op. When data is read from a hive table, the
    table property is always respected. This allows spark to not change
    behavior when reading old data, but read newly written data correctly
    (whatever the source of the data is).
    
    Spark inherited the original behavior from Hive, but Hive is also
    updating behavior to use the same scheme in HIVE-12767 / HIVE-16231.
    
    The default for Spark remains unchanged; created tables do not include
    the new table property.
    
    This will only apply to hive tables; nothing is added to parquet
    metadata to indicate the timezone, so data that is read or written
    directly from parquet files will never have any conversions applied.
    
    Note that CDH-35305 is the backport of SPARK-12297 to cdh 5.12.
    However, the code has diverged so much that its not even a cherry-pick +
    conflict resolution, its a re-implementation.  The good news is, the
    main logic is similar and not very complicated.  The main difference
    from upstreeam is interaction w/ hive for metadata operations (eg.
    create table).
    
    Cloudera ID: CDH-35305

commit 50b343b7ea364a5d7100364b425fef7ded9200d5
Author: Mark Grover <mark@apache.org>
Date:   Wed Apr 26 17:06:21 2017 -0700

    [SPARK-20435][CORE] More thorough redaction of sensitive information
    
    This change does a more thorough redaction of sensitive information from logs and UI
    Add unit tests that ensure that no regressions happen that leak sensitive information to the logs.
    
    The motivation for this change was appearance of password like so in `SparkListenerEnvironmentUpdate` in event logs under some JVM configurations:
    `"sun.java.command":"org.apache.spark.deploy.SparkSubmit ... --conf spark.executorEnv.HADOOP_CREDSTORE_PASSWORD=secret_password ..."
    `
    Previously redaction logic was only checking if the key matched the secret regex pattern, it'd redact it's value. That worked for most cases. However, in the above case, the key (sun.java.command) doesn't tell much, so the value needs to be searched. This PR expands the check to check for values as well.
    
    New unit tests added that ensure that no sensitive information is present in the event logs or the yarn logs. Old unit test in UtilsSuite was modified because the test was asserting that a non-sensitive property's value won't be redacted. However, the non-sensitive value had the literal "secret" in it which was causing it to redact. Simply updating the non-sensitive property's value to another arbitrary value (that didn't have "secret" in it) fixed it.
    
    Author: Mark Grover <mark@apache.org>
    
    Closes #17725 from markgrover/spark-20435.
    
    (cherry picked from commit 66636ef0b046e5d1f340c3b8153d7213fa9d19c7)

commit f76a54d4eb044a7f2ecf2df253161f37368b7d44
Author: Marcelo Vanzin <vanzin@cloudera.com>
Date:   Fri Apr 7 13:22:38 2017 -0700

    CDH-52469. Fix dependencies to avoid re-packaging Hadoop classes.
    
    It seems maven pulls some dependencies of test jars in compile scope,
    and because they weren't declared in the root pom, they get included
    in the final assembly; so we ended up with duplicate Hadoop and HDFS
    classes.
    
    Instead of declaring these in the root pom, I chose a more surgical
    spot and just excluded all Hadoop dependencies from the lineage
    module when packaging the assembly. And also added a test to make
    sure a few classes are not in the assembly when building the CDH
    packaging.

commit 1e8be94d251eb101437e5fbf1fbbb04aadaec6d0
Author: jinxing <jinxing@meituan.com>
Date:   Sat Feb 18 10:49:40 2017 -0400

    [SPARK-19263] DAGScheduler should avoid sending conflicting task set.
    
    In current `DAGScheduler handleTaskCompletion` code, when event.reason is `Success`, it will first do `stage.pendingPartitions -= task.partitionId`, which maybe a bug when `FetchFailed` happens.
    
    **Think about below**
    
    1.  Stage 0 runs and generates shuffle output data.
    2. Stage 1 reads the output from stage 0 and generates more shuffle data. It has two tasks: ShuffleMapTask1 and ShuffleMapTask2, and these tasks are launched on executorA.
    3. ShuffleMapTask1 fails to fetch blocks locally and sends a FetchFailed to the driver. The driver marks executorA as lost and updates failedEpoch;
    4. The driver resubmits stage 0 so the missing output can be re-generated, and then once it completes, resubmits stage 1 with ShuffleMapTask1x and ShuffleMapTask2x.
    5. ShuffleMapTask2 (from the original attempt of stage 1) successfully finishes on executorA and sends Success back to driver. This causes DAGScheduler::handleTaskCompletion to remove partition 2 from stage.pendingPartitions (line 1149), but it does not add the partition to the set of output locations (line 1192), because the task’s epoch is less than the failure epoch for the executor (because of the earlier failure on executor A)
    6. ShuffleMapTask1x successfully finishes on executorB, causing the driver to remove partition 1 from stage.pendingPartitions. Combined with the previous step, this means that there are no more pending partitions for the stage, so the DAGScheduler marks the stage as finished (line 1196). However, the shuffle stage is not available (line 1215) because the completion for ShuffleMapTask2 was ignored because of its epoch, so the DAGScheduler resubmits the stage.
    7. ShuffleMapTask2x is still running, so when TaskSchedulerImpl::submitTasks is called for the re-submitted stage, it throws an error, because there’s an existing active task set
    
    **In this fix**
    
    If a task completion is from a previous stage attempt and the epoch is too low
    (i.e., it was from a failed executor), don't remove the corresponding partition
    from pendingPartitions.
    
    Author: jinxing <jinxing@meituan.com>
    Author: jinxing <jinxing6042@126.com>
    
    Closes #16620 from jinxing64/SPARK-19263.
    
    (cherry picked from commit 729ce3703257aa34c00c5c8253e6971faf6a0c8d)
    (cherry picked from commit d354ef5fa19c76d095cedc52af8950cb0ee7fc79)
    (cherry picked from commit 553ca6811e3b6e0a48f65276115acb4f8b3524eb)

commit f6e602d1fc720c68bdd021f669502404fc2c1037
Author: Kay Ousterhout <kayousterhout@gmail.com>
Date:   Fri Feb 10 22:34:57 2017 -0800

    [SPARK-19537] Move pendingPartitions to ShuffleMapStage.
    
    The pendingPartitions instance variable should be moved to ShuffleMapStage,
    because it is only used by ShuffleMapStages. This change is purely refactoring
    and does not change functionality.
    
    I fixed this in an attempt to clarify some of the discussion around #16620, which I was having trouble reasoning about.  I stole the helpful comment Imran wrote for pendingPartitions and used it here.
    
    cc squito markhamstra jinxing64
    
    Author: Kay Ousterhout <kayousterhout@gmail.com>
    
    Closes #16876 from kayousterhout/SPARK-19537.
    
    (cherry picked from commit 0fbecc736df95bf757cb497c108ae3dbc5893829)
    (cherry picked from commit a4801f9cfd302c9f8b342b80c5b52ff4e6480c21)
    (cherry picked from commit 0c3ae984916210065b5b2a159691a61dee71b97f)

commit de2c50b2ca6815b1f19fdb959452f85a35f00066
Author: Wenchen Fan <wenchen@databricks.com>
Date:   Thu Jan 12 22:52:34 2017 -0800

    CDH-51055. [SPARK-19178][SQL][Backport-to-1.6] convert string of large numbers to int should return null
    
    When we convert a string to integral, we will convert that string to `decimal(20, 0)` first, so that we can turn a string with decimal format to truncated integral, e.g. `CAST('1.2' AS int)` will return `1`.
    
    However, this brings problems when we convert a string with large numbers to integral, e.g. `CAST('1234567890123' AS int)` will return `1912276171`, while Hive returns null as we expected.
    
    This is a long standing bug(seems it was there the first day Spark SQL was created), this PR fixes this bug by adding the native support to convert `UTF8String` to integral.
    
    new regression tests
    
    Author: Wenchen Fan <wenchen@databricks.com>
    
    Closes #16550 from cloud-fan/string-to-int.
    
    Backport related comments:
    The cherry-pick from upstream wasn't clean. Conflicts came form two files: UTF8String.scala (replaced in Spark 2) and TypeCoercion ( renamed from HiveTypeCoercion in spark 2). To resolve the conflicts, I appled directly the changes proposed in SPARK-19178 to the related files in Spark 1.6.
    
    (cherry picked from commit 6b34e745bb8bdcf5a8bb78359fa39bbe8c6563cc)
    Signed-off-by: Wenchen Fan <wenchen@databricks.com>
    (cherry picked from commit f56819f9bacb2c3e13f148dbd86589b7248352bb)
    (cherry picked from commit 39a80403a5e20bb8b64eae60dfe91a86d1d06a77)

commit c24eefdb5b7304c96654978eacaf22b5eb4144b0
Author: Salil Surendran <salilsurendran@cloudera.com>
Date:   Mon Mar 13 00:33:00 2017 -0700

    CDH-51486. Set an error code if aggregated functions are found in query
    
    If query plan contains aggregated functions such as avg, sum, max etc.  set an error code in lineage output
    
    Below code is an example where the aggregated function avg is used.
    
    var df2 = hc.sql("select * from sample_07")
    df2 = df2.groupBy("code","description").avg("salary")
    df2 = df2.withColumnRenamed(df2.columns(2),"avgsal")
    df2.write.saveAsTable("group_agg2")

commit b07d359a43b846bbc1a9af04a3810b64aabdb900
Author: Salil Surendran <salilsurendran@cloudera.com>
Date:   Fri Mar 10 09:03:23 2017 -0800

    CDH-51296. Insert Queries should report lineage
    
    Insert queries such as
    sqlContext.sql("insert into orders select id, name from customers") should report lineage

commit fc31fcfb9086dfc1e81e1108212c3ea5e2079f00
Author: Salil Surendran <salilsurendran@cloudera.com>
Date:   Fri Mar 10 06:55:03 2017 -0800

    CDH-51351. CTAS Queries should report lineage
    
    CTAS queries such as sqlContext.sql("create table sample_1 as select * from sample_07") should report lineage

commit 901e719b0864c15e4bbe1196d5e11f8e0b0e5709
Author: Salil Surendran <salilsurendran@cloudera.com>
Date:   Fri Mar 3 18:21:29 2017 -0800

    CDH-51067. Output yarnApplicationId in end marker for lineage

commit 4e0ef9ee20937e964ad6b5de56db4b95703894b0
Author: Salil Surendran <salilsurendran@cloudera.com>
Date:   Tue Feb 21 14:05:12 2017 -0800

    CDH-50366. Lineage should output lineage data even when there are no inputs
    
    If a dataframe is created out of a sequence of numbers rather than from a hive table or parquet file and written to a table or file then lineage data should contain the output data. Joins with existing tables of such dataframes should only give that table as input.

commit e15407ff933e0796888904d5987aeba5bbd49a51
Author: Mark Grover <mark@apache.org>
Date:   Mon Mar 6 13:23:55 2017 -0800

    CLOUDERA-BUILD. CDH-50984. hiveMetastoreLocation is coming as a blank string in lineage output

commit 0232372ec281d928ebe59baacdafeb02afcb7518
Author: Mark Grover <mark@apache.org>
Date:   Mon Mar 6 10:40:20 2017 -0800

    CDH-50815. [SPARK-19720][CORE] Redact sensitive information from SparkSubmit console
    
    This change redacts senstive information (based on `spark.redaction.regex` property)
    from the Spark Submit console logs. Such sensitive information is already being
    redacted from event logs and yarn logs, etc.
    
    Testing was done manually to make sure that the console logs were not printing any
    sensitive information.
    
    Here's some output from the console:
    
    ```
    Spark properties used, including those specified through
     --conf and those from the properties file /etc/spark2/conf/spark-defaults.conf:
      (spark.yarn.appMasterEnv.HADOOP_CREDSTORE_PASSWORD,*********(redacted))
      (spark.authenticate,false)
      (spark.executorEnv.HADOOP_CREDSTORE_PASSWORD,*********(redacted))
    ```
    
    ```
    System properties:
    (spark.yarn.appMasterEnv.HADOOP_CREDSTORE_PASSWORD,*********(redacted))
    (spark.authenticate,false)
    (spark.executorEnv.HADOOP_CREDSTORE_PASSWORD,*********(redacted))
    ```
    There is a risk if new print statements were added to the console down the road, sensitive information may still get leaked, since there is no test that asserts on the console log output. I considered it out of the scope of this JIRA to write an integration test to make sure new leaks don't happen in the future.
    
    Running unit tests to make sure nothing else is broken by this change.
    
    Author: Mark Grover <mark@apache.org>
    
    Closes #17047 from markgrover/master_redaction.
    
    (cherry picked from commit 5ae3516bfb7716f1793eb76b4fdc720b31829d07)

commit 7bf865da938ed30122e9ee1257d2d078541516a1
Author: José Hiram Soltren <jose@cloudera.com>
Date:   Fri Mar 3 13:09:26 2017 -0600

    [CDH-49875] Make local metrics test fail less often
    
    org.apache.spark.scheduler.SparkListenerSuite.local metrics will fail
    when executors finish too quickly. This causes a check to fail.
    
    https://github.com/apache/spark/pull/16586 proposed a fix to
    this by increasing the run time of the local metrics test
    with more parallelism. Instead of cherry-picking the entire
    change, manually port that one proposed fix.
    
    (cherry picked from commit 9e2d735fc9c2bb90b67f9664d71e243d4e06ad76)

commit 8676566c34d96347a28355ad2399c07c805660a3
Author: Salil Surendran <salilsurendran@cloudera.com>
Date:   Mon Feb 27 19:33:53 2017 -0800

    CDH-50511. QueryExecutionListener api should be consistent with upstream Spark
    
    This commit removes the extraParams parameter from the onSuccess and onFailure methods of the QueryExecutionListener. This map is now embedded in the QueryExecution object.

commit 0a542ed55331486b20724ef09dd0fedeba90b7d3
Author: Davies Liu <davies@databricks.com>
Date:   Fri Dec 9 15:44:22 2016 -0800

    [SPARK-4105] retry the fetch or stage if shuffle block is corrupt
    
    There is an outstanding issue that existed for a long time: Sometimes the shuffle blocks are corrupt and can't be decompressed. We recently hit this in three different workloads, sometimes we can reproduce it by every try, sometimes can't. I also found that when the corruption happened, the beginning and end of the blocks are correct, the corruption happen in the middle. There was one case that the string of block id is corrupt by one character. It seems that it's very likely the corruption is introduced by some weird machine/hardware, also the checksum (16 bits) in TCP is not strong enough to identify all the corruption.
    
    Unfortunately, Spark does not have checksum for shuffle blocks or broadcast, the job will fail if any corruption happen in the shuffle block from disk, or broadcast blocks during network. This PR try to detect the corruption after fetching shuffle blocks by decompressing them, because most of the compression already have checksum in them. It will retry the block, or failed with FetchFailure, so the previous stage could be retried on different (still random) machines.
    
    Checksum for broadcast will be added by another PR.
    
    Added unit tests
    
    Author: Davies Liu <davies@databricks.com>
    
    Closes #15923 from davies/detect_corrupt.
    
    (cherry picked from commit cf33a86285629abe72c1acf235b8bfa6057220a8)
    
    CDH Note: To backport the patch to the CDH version that is based on the 1.6 line, some changes
    from SPARK-13921 were included (notedly some helper classes and methods), and some minor changes
    made to account for the shuffle encryption code in the CDH branch.
    
    Cloudera ID: CDH-48790

commit 22f1eb261cee97267a5fcec6f68ce8aca8d168bb
Author: Marcelo Vanzin <vanzin@cloudera.com>
Date:   Tue Feb 28 11:35:39 2017 -0800

    CLOUDERA-BUILD. CDH-50972. Don't package jackson-annotations in Spark assemblies.

commit dce4bbf2d2dd577281d0e7f72d5d61e27d1d2961
Author: Jenkins <dev-kitchen@cloudera.com>
Date:   Mon Feb 27 16:11:48 2017 -0800

    Updating Maven version to 5.12.0-SNAPSHOT

commit e2864c8156e5ed0ddb9360f8b762607996d12992
Author: Salil Surendran <salilsurendran@cloudera.com>
Date:   Mon Feb 27 00:35:57 2017 -0800

    CDH-50687. Fix broken unit test
    
    DiscoverySuite initializes TestHiveSingleton before the initial test run thereby creating a SparkContext which leads to errors when the test run starts and tries to create another duplicate SparkContext.
    To ensure that all tests are using the same SparkContext make sure that FileQueryAnalysisSuite is also extended from TestHiveSingleton rather than SharedSQLContext.

commit 66bbde5f8896d2eaac8637d53544c54e4a3f8250
Author: Salil Surendran <salilsurendran@cloudera.com>
Date:   Sun Feb 26 13:44:26 2017 -0800

    CDH-50687. Fix broken unit test

commit ebfd1dd28133b4249de75d5864bf2a0d86885a9b
Author: Imran Rashid <irashid@cloudera.com>
Date:   Wed Feb 22 21:54:56 2017 -0800

    [SPARK-16554] Fix backport

commit 1c365d5685597175301aa37b59bafe1c885dc6fe
Author: Marcelo Vanzin <vanzin@cloudera.com>
Date:   Wed Feb 22 17:38:47 2017 -0800

    CLOUDERA-BUILD. Fix sbt build.

commit 0e71a3aea33cab535bec6828c87c14f19518fde2
Author: José Hiram Soltren <jose@cloudera.com>
Date:   Thu Feb 9 12:49:31 2017 -0600

    [SPARK-16554][CORE] Automatically Kill Executors and Nodes when they are Blacklisted
    
    In SPARK-8425, we introduced a mechanism for blacklisting executors and nodes (hosts). After a certain number of failures, these resources would be "blacklisted" and no further work would be assigned to them for some period of time.
    
    In some scenarios, it is better to fail fast, and to simply kill these unreliable resources. This changes proposes to do so by having the BlacklistTracker kill unreliable resources when they would otherwise be "blacklisted".
    
    In order to be thread safe, this code depends on the CoarseGrainedSchedulerBackend sending a message to the driver backend in order to do the actual killing. This also helps to prevent a race which would permit work to begin on a resource (executor or node), between the time the resource is marked for killing and the time at which it is finally killed.
    
    ./dev/run-tests
    Ran https://github.com/jsoltren/jose-utils/blob/master/blacklist/test-blacklist.sh, and checked logs to see executors and nodes being killed.
    
    Testing can likely be improved here; suggestions welcome.
    
    Author: José Hiram Soltren <jose@cloudera.com>
    
    Closes #16650 from jsoltren/SPARK-16554-submit.
    
    (cherry picked from commit 6287c94f08200d548df5cc0a401b73b84f9968c4)

commit f555863b5729fc0cd303a9d07d54dc22c6ec1ab3
Author: Imran Rashid <irashid@cloudera.com>
Date:   Wed Feb 22 16:52:07 2017 -0800

    [SPARK-16654] Fix backport.

commit a1a84d84ed148a16db25283a596ab91091f7470d
Author: José Hiram Soltren <jose@cloudera.com>
Date:   Thu Jan 19 09:08:18 2017 -0600

    [SPARK-16654][CORE] Add UI coverage for Application Level Blacklisting
    
    Builds on top of work in SPARK-8425 to update Application Level Blacklisting in the scheduler.
    
    Adds a UI to these patches by:
    - defining new listener events for blacklisting and unblacklisting, nodes and executors;
    - sending said events at the relevant points in BlacklistTracker;
    - adding JSON (de)serialization code for these events;
    - augmenting the Executors UI page to show which, and how many, executors are blacklisted;
    - adding a unit test to make sure events are being fired;
    - adding HistoryServerSuite coverage to verify that the SHS reads these events correctly.
    - updates the Executor UI to show Blacklisted/Active/Dead as a tri-state in Executors Status
    
    Updates .rat-excludes to pass tests.
    
    username squito
    
    ./dev/run-tests
    testOnly org.apache.spark.util.JsonProtocolSuite
    testOnly org.apache.spark.scheduler.BlacklistTrackerSuite
    testOnly org.apache.spark.deploy.history.HistoryServerSuite
    https://github.com/jsoltren/jose-utils/blob/master/blacklist/test-blacklist.sh
    ![blacklist-20161219](https://cloud.githubusercontent.com/assets/1208477/21335321/9eda320a-c623-11e6-8b8c-9c912a73c276.jpg)
    
    Author: José Hiram Soltren <jose@cloudera.com>
    
    Closes #16346 from jsoltren/SPARK-16654-submit.
    
    Backport Notes: This had some conflicts in the ExecutorsPage (which was
    refactored to be more js based) and the event log parsing & expectations
    (the json itself changed somewhat in spark 2.0).
    
    (cherry picked from commit 640f942337e1ce87075195998bd051e19c4b50b9)

commit c83474651154372054a2de72723db365072f8a20
Author: Salil Surendran <salilsurendran@cloudera.com>
Date:   Sun Feb 12 22:36:48 2017 -0800

    CLOUDERA-BUILD. CDH-50079. Lineage should correctly handle hive tables when a parquet file is registered as a temp table and joined with it
    
    If a parquet file is registered as a temp table and joined with a hive table then lineage should output metadata related to the hive table rather that it's underlying hdfs file

commit dfe9a2b4e8327ffc584a16d861dbf62eeca01b26
Author: Salil Surendran <salilsurendran@cloudera.com>
Date:   Fri Feb 10 14:52:17 2017 -0800

    CLOUDERA-BUILD. CDH-49619. OutputMetadata in spark lineage json should contain fully qualified database name
    
    The fully qualified table name is not available in the QueryExecution object for output metadata. Navigator needs a fully qualified table name. This commit aims to expose the current database via the sql catalog. It has the partial changes from the PR for SPARK-13571. This PR was never accepted into Spark upstream since the work was done by another larger piece of work associated with changing interfaces.

commit 5c60bd5eaa04e543528c0b18022a297d3635ab4a
Author: Salil Surendran <salilsurendran@cloudera.com>
Date:   Wed Feb 8 22:10:47 2017 -0800

    CLOUDERA-BUILD. CDH-46754. Generate lineage data for Spark SQL applications
    
    Addition of the lineage project which will house the lineage code responsible for parsing and outputting the lineage data to the navigator json file.
    Lineage of Spark applications using Spark SQL queries will be outputted to the lineage log directory. For each spark sql query that has inputs or outputs for eg. from hive, parquet file, json file etc., the tables or files from where the data was read and to where it was written, the columns read, data source type of the inputs/outputs will be written out as lineage data.

commit b9d65e85a5bc4c5a00b4130a7d1d4d6cc356f0c5
Author: Salil Surendran <salilsurendran@cloudera.com>
Date:   Wed Feb 8 21:52:30 2017 -0800

    CLOUDERA-BUILD. CDH-46754. Adding output table to LogicalRelation
    
    Changes to support the output table in case data is being persisted as done in the PR for SPARK-12724. We don't need the exact changes in the PR but only the change related to the output table which is the 3rd parameter in the constructor definition of the class LogicalRelation. This is necessary to have the output data in the QueryExecution object for analysis. So you will see changes like "case LogicalRelation(r, )" changed to "case LogicalRelation(r, , \_)" with the 3rd parameter being the output table name.

commit d9b28ed73448d335333da1eb9f3878bdc34168eb
Author: Salil Surendran <salilsurendran@cloudera.com>
Date:   Tue Feb 7 17:46:14 2017 -0800

    CLOUDERA-BUILD. CDH-46754. Call QueryExecutionListener callback methods for DataFrameWriter methods
    
    Calls the QueryExecutionListener callback methods onSuccess() and onFailure() methods for DataFrameWriter output methods.  It fixes an issue where the callback methods were being called for several of the DataFrame methods like take, head, first, collect etc. but didn't get called for any of the DataFrameWriter methods like saveAsTable, save etc. Added a new property "spark.sql.queryExecutionListeners" that can be used to specify instances of QueryExecutionListeners that should be attached to the SparkSession when the spark application starts up.

commit 0803ceadf093546a056c0c3213f51027e9381319
Author: Marcelo Vanzin <vanzin@cloudera.com>
Date:   Wed Feb 15 13:31:25 2017 -0800

    [SPARK-19554][UI,YARN] Allow SHS URL to be used for tracking in YARN RM.
    
    Allow an application to use the History Server URL as the tracking
    URL in the YARN RM, so there's still a link to the web UI somewhere
    in YARN even if the driver's UI is disabled. This is useful, for
    example, if an admin wants to disable the driver UI by default for
    applications, since it's harder to secure it (since it involves non
    trivial ssl certificate and auth management that admins may not want
    to expose to user apps).
    
    This needs to be opt-in, because of the way the YARN proxy works, so
    a new configuration was added to enable the option.
    
    The YARN RM will proxy requests to live AMs instead of redirecting
    the client, so pages in the SHS UI will not render correctly since
    they'll reference invalid paths in the RM UI. The proxy base support
    in the SHS cannot be used since that would prevent direct access to
    the SHS.
    
    So, to solve this problem, for the feature to work end-to-end, a new
    YARN-specific filter was added that detects whether the requests come
    from the proxy and redirects the client appropriatly. The SHS admin has
    to add this filter manually if they want the feature to work.
    
    Tested with new unit test, and by running with the documented configuration
    set in a test cluster. Also verified the driver UI is used when it's
    enabled.
    
    (cherry picked from commit 4661d30b988bf773ab45a15b143efb2908d33743)
    
    Cloudera ID: CDH-50173

commit a0b575581cf0b37adf3b7214f3ea1c75a8c19c4b
Author: Marcelo Vanzin <vanzin@cloudera.com>
Date:   Wed Feb 22 13:19:31 2017 -0800

    [SPARK-19652][UI] Do auth checks for REST API access.
    
    The REST API has a security filter that performs auth checks
    based on the UI root's security manager. That works fine when
    the UI root is the app's UI, but not when it's the history server.
    
    In the SHS case, all users would be allowed to see all applications
    through the REST API, even if the UI itself wouldn't be available
    to them.
    
    This change adds auth checks for each app access through the API
    too, so that only authorized users can see the app's data.
    
    The change also modifies the existing security filter to use
    `HttpServletRequest.getRemoteUser()`, which is used in other
    places. That is not necessarily the same as the principal's
    name; for example, when using Hadoop's SPNEGO auth filter,
    the remote user strips the realm information, which then matches
    the user name registered as the owner of the application.
    
    I also renamed the UIRootFromServletContext trait to a more generic
    name since I'm using it to store more context information now.
    
    Tested manually with an authentication filter enabled.
    
    Author: Marcelo Vanzin <vanzin@cloudera.com>
    
    Closes #17019 from vanzin/SPARK-19652_2.1.
    
    (cherry picked from commit 21afc4534f90e063330ad31033aa178b37ef8340)
    
    Cloudera ID: CDH-50507

commit 0995e73697bc0dec2bb36d04b867ee28c4064b8b
Author: Marcelo Vanzin <vanzin@cloudera.com>
Date:   Thu Feb 9 22:06:46 2017 +0900

    [SPARK-17874][CORE] Add SSL port configuration.
    
    Make the SSL port configuration explicit, instead of deriving it
    from the non-SSL port, but retain the existing functionality in
    case anyone depends on it.
    
    The change starts the HTTPS and HTTP connectors separately, so
    that it's possible to use independent ports for each. For that to
    work, the initialization of the server needs to be shuffled around
    a bit. The change also makes it so the initialization of both
    connectors is similar, and end up using the same Scheduler - previously
    only the HTTP connector would use the correct one.
    
    Also fixed some outdated documentation about a couple of services
    that were removed long ago.
    
    Tested with unit tests and by running spark-shell with SSL configs.
    
    Author: Marcelo Vanzin <vanzin@cloudera.com>
    
    Closes #16625 from vanzin/SPARK-17874.
    
    (cherry picked from commit 3fc8e8caf81d0049daf9b776ad4059b0df81630f)
    
    Cloudera ID: CDH-49082

commit a61d18b4a012a10c9f2579af49cd805831cdcc98
Author: Marcelo Vanzin <vanzin@cloudera.com>
Date:   Mon Feb 6 14:11:09 2017 -0800

    CDH-49608. Encrypt blocks stored in memory.
    
    The underlying issue is that when fetching a remote block, the code does
    not know where the block was stored in the remote block manager. So, with
    the current 1.6 code, if the block was in memory, the reader would get
    unencrypted data, but if it was on disk, the reader would see encrypted
    data.
    
    That means that the reader cannot know how to read that data. So, instead,
    do what Spark 2.0 does and always encrypt bytes stored in the memory store,
    so that fetching them will always return encrypted data, no matter which
    code path is used. This adds some overhead, but avoids having to change the
    RPC protocols to include information about whether the block being read is
    encrypted or not, or changing the local read path of the block manager, which
    would be even more expensive (i.e. would make it complicated to use memory
    mapped files).
    
    To properly support that a few places had to be changed. Namely:
    - adding a block to the block manager encrypts the data
    - when the bm replicates the block, it sends over encrypted data, so the remote
      manager doesn't need to encrypt it again
    - evicting a block from the memory store to the disk store does not decrypt
      the data, the data is just dumped into a file
    - the broadcast code needed slight corrections to handle encryption
    
    The new "ByteBufferOutputStream" class was just backported from Spark 2.0 (it
    was added for a different change unrelated to this feature, but is useful
    here).
    
    One thing that deviates from Spark 2.0 is that WAL files written by streaming
    apps are not encrypted, and a small change had to be made to the WAL code to
    account for that.

commit 922a114e91e9ce08774c4e7a37feb83e945d217b
Author: Marcelo Vanzin <vanzin@cloudera.com>
Date:   Fri Jan 27 10:16:09 2017 -0800

    [SPARK-19220][UI] Make redirection to HTTPS apply to all URIs.
    
    The redirect handler was installed only for the root of the server;
    any other context ended up being served directly through the HTTP
    port. Since every sub page (e.g. application UIs in the history
    server) is a separate servlet context, this meant that everything
    but the root was accessible via HTTP still.
    
    The change adds separate names to each connector, and binds contexts
    to specific connectors so that content is only served through the
    HTTPS connector when it's enabled. In that case, the only thing that
    binds to the HTTP connector is the redirect handler.
    
    Tested with new unit tests and by checking a live history server.
    
    (cherry picked from commit 93d588756b4d507fbb26890495dc90b91209b89b)
    
    Cloudera ID: CDH-49081

commit 5346fdb6cf33121fb07b855873f2e3ac204852ff
Author: Mingjie Tang <mtang@hortonworks.com>
Date:   Fri Jan 6 17:22:59 2017 -0800

    [SPARK-18372][SQL][BRANCH-1.6] Staging directory fail to be removed
    
    ## What changes were proposed in this pull request?
    
    This fix is related to be bug: https://issues.apache.org/jira/browse/SPARK-18372 .
    The insertIntoHiveTable would generate a .staging directory, but this directory  fail to be removed in the end.
    
    This is backport from spark 2.0.x code, and is related to PR #12770
    
    ## How was this patch tested?
    manual tests
    
    Author: Mingjie Tang <mtanghortonworks.com>
    
    Author: Mingjie Tang <mtang@hortonworks.com>
    Author: Mingjie Tang <mtang@HW12398.local>
    
    Closes #15819 from merlintang/branch-1.6.
    
    (cherry picked from commit 2303887ce659b2ee90bfb412dc17629668894b03)

commit 597903d8686e7b4ee485871530e8020fccd97e02
Author: Marcelo Vanzin <vanzin@cloudera.com>
Date:   Wed Jan 25 14:22:58 2017 -0800

    [SPARK-18750][YARN] Follow up: move test to correct directory in 2.1 branch.
    
    Author: Marcelo Vanzin <vanzin@cloudera.com>
    
    Closes #16704 from vanzin/SPARK-18750_2.1.
    
    (cherry picked from commit 97d3353ef16a6e6edc93d8177b08442a03e19eee)
    Signed-off-by: Marcelo Vanzin <vanzin@cloudera.com>
    (cherry picked from commit 00a48075ac2e0d6dbdc7b2632d8702af4f30aa97)
    
    Cloudera ID: CDH-49317

commit 11ab924e7f5cf0a808ac738a7f7b34ed7ee75ba6
Author: Marcelo Vanzin <vanzin@cloudera.com>
Date:   Wed Jan 25 08:18:41 2017 -0600

    [SPARK-18750][YARN] Avoid using "mapValues" when allocating containers.
    
    That method is prone to stack overflows when the input map is really
    large; instead, use plain "map". Also includes a unit test that was
    tested and caused stack overflows without the fix.
    
    Author: Marcelo Vanzin <vanzin@cloudera.com>
    
    Closes #16667 from vanzin/SPARK-18750.
    
    (cherry picked from commit 76db394f2baedc2c7b7a52c05314a64ec9068263)
    Signed-off-by: Marcelo Vanzin <vanzin@cloudera.com>
    (cherry picked from commit 2d9e8d5e90ae7365f38923822df7f521200dc7bc)
    
    Cloudera ID: CDH-49317

commit 9e4fb206e73131281469da87c0657a2c9e85f30a
Author: Marcelo Vanzin <vanzin@cloudera.com>
Date:   Thu Jan 19 12:41:38 2017 -0800

    CDH-49085. Encrypt cached data stored on disk.
    
    Spark 1.6 has a slightly different block manager than 2.0, and cached
    data is not automatically encrypted here when evicted to disk. This
    applied to both deserialized and also serialized data (in 2.0, serialized
    data in memory is also encrypted, making eviction easy).
    
    This change makes the changes so that paths that evict blocks to disk do
    proper encryption, and the read paths do decryption only when necessary.
    
    The disk-related storage level tests in BlockManager suite were updated
    to run with encryption both on and off, with some extra checks added for
    the encrypted case.

commit e8e19e348bbf560b51c8aed0eea332851d31214a
Author: Marcelo Vanzin <vanzin@cloudera.com>
Date:   Tue Jan 24 09:39:02 2017 -0800

    CDH-49397. Ensure context class loader is set when initializing Hive.
    
    This makes sure classes are loaded by the correct class loader when
    all Hive internal state is being initialized.

commit 2f2a9d0666747b60a24fa53be25ad45d8a0c52ac
Author: Marcelo Vanzin <vanzin@cloudera.com>
Date:   Thu Jan 19 18:41:13 2017 -0800

    CLOUDERA-BUILD. CDH-49226. Executor memory should be in bytes, not MB.
    
    Parse the user-defined string as bytes instead of MB, which is what
    the "MemoryParam()" call used to do before it was replaced as part
    of CDH-48784.
    
    Also added a unit test to avoid breaking this yet again.

commit 8bc255fd912ce7a67083dfadbf91aab0c1b84664
Author: jerryshao <sshao@hortonworks.com>
Date:   Fri Jan 6 10:07:54 2017 -0600

    [SPARK-19033][CORE] Add admin acls for history server
    
    Current HistoryServer's ACLs is derived from application event-log, which means the newly changed ACLs cannot be applied to the old data, this will become a problem where newly added admin cannot access the old application history UI, only the new application can be affected.
    
    So here propose to add admin ACLs for history server, any configured user/group could have the view access to all the applications, while the view ACLs derived from application run-time still take effect.
    
    Unit test added.
    
    Cloudera ID: CDH-49083
    
    Author: jerryshao <sshao@hortonworks.com>
    
    Closes #16470 from jerryshao/SPARK-19033.
    
    (cherry picked from commit 4a4c3dc9ca10e52f7981b225ec44e97247986905)

commit 85c52dd8614e842859a0522f552a63bc23b2984b
Author: Marcelo Vanzin <vanzin@cloudera.com>
Date:   Mon Jan 9 16:37:42 2017 -0800

    CLOUDERA-BUILD. CDH-46717. Add versions to jar's manifest.
    
    This restores functionality that is available in Apache's root pom
    but is missing from CDH's pom, and which some downstream
    dependencies like Oozie want.
    
    (cherry picked from commit 6c6a16a4291dfb15917b490c23c1dbca56d7f9ec)

commit c5b7149ab01adf0e722c2106d36d0ee5580380b3
Author: Marcelo Vanzin <vanzin@cloudera.com>
Date:   Thu Jan 5 11:37:26 2017 -0800

    CLOUDERA-BUILD. CDH-48784. Respect app's SparkConf in YARN cluster mode.
    
    The YARN allocator in cluster mode was ignoring executor core and memory
    configs set by the user code in SparkConf, choosing instead to respect
    only the launch-side configuration. If there was no launch-side config,
    the defaults were used instead.
    
    This change fixes things to make YARN cluster also respect the app's
    SparkConf, and gives preference to it over values set in the launcher
    side. This matches the behavior of client mode.
    
    This was fixed in 2.0 as part of SPARK-12343, but the full diff is
    too risky for backporting to 1.6, so this change is a customized
    version that only applies to executor cores and memory.

commit ec4ad81377fae8986d1c44d37f106d77dff1e398
Author: huangzhaowei <carlmartinmax@gmail.com>
Date:   Fri Feb 26 07:32:07 2016 -0600

    [SPARK-12523][YARN] Support long-running of the Spark On HBase and hive meta store.
    
    Obtain the hive metastore and hbase token as well as hdfs token in DelegationToeknRenewer to supoort long-running application of spark on hbase or thriftserver.
    
    Author: huangzhaowei <carlmartinmax@gmail.com>
    
    Closes #10645 from SaintBacchus/SPARK-12523.
    
    (cherry picked from commit 5c3912e5c90ce659146c3056430d100604378b71)

commit e1bc155f785d62d19c184fc717377880d1cdd000
Author: Steve Loughran <stevel@hortonworks.com>
Date:   Wed Dec 9 10:25:38 2015 -0800

    [SPARK-12241][YARN] Improve failure reporting in Yarn client obtainTokenForHBase()
    
    This lines up the HBase token logic with that done for Hive in SPARK-11265: reflection with only CFNE being swallowed.
    
    There is a test, one which doesn't try to put HBase on the yarn/test class and really do the reflection (the way the hive introspection does). If people do want that then it could be added with careful POM work
    
    +also: cut an incorrect comment from the Hive test case before copying it, and a couple of imports that may have been related to the hive test in the past.
    
    Author: Steve Loughran <stevel@hortonworks.com>
    
    Closes #10227 from steveloughran/stevel/patches/SPARK-12241-obtainTokenForHBase.
    
    (cherry picked from commit 442a7715a590ba2ea2446c73b1f914a16ae0ed4b)

commit d2b152dd0c90f9011eea39607e1f584cb96f30f4
Author: Marcelo Vanzin <vanzin@cloudera.com>
Date:   Mon Dec 5 10:46:29 2016 -0800

    CDH-47767. Always set "isSrcLocal" to false in HiveShim.
    
    The only time "isSrcLocal" should be true is in statements like
    "LOAD DATA LOCAL ...", which are not handled by Spark. So in the
    calls made by HiveShim, isSrcLocal should always be false.

commit a8ca6e6e6cd84765ace91f850871d0263845e0c5
Author: Marcelo Vanzin <vanzin@cloudera.com>
Date:   Wed Nov 30 14:10:32 2016 -0800

    [SPARK-18546][CORE] Fix merging shuffle spills when using encryption.
    
    The problem exists because it's not possible to just concatenate encrypted
    partition data from different spill files; currently each partition would
    have its own initial vector to set up encryption, and the final merged file
    should contain a single initial vector for each merged partiton, otherwise
    iterating over each record becomes really hard.
    
    To fix that, UnsafeShuffleWriter now decrypts the partitions when merging,
    so that the merged file contains a single initial vector at the start of
    the partition data.
    
    Because it's not possible to do that using the fast transferTo path, when
    encryption is enabled UnsafeShuffleWriter will revert back to using file
    streams when merging. It may be possible to use a hybrid approach when
    using encryption, using an intermediate direct buffer when reading from
    files and encrypting the data, but that's better left for a separate patch.
    
    As part of the change I made DiskBlockObjectWriter take a SerializerManager
    instead of a "wrap stream" closure, since that makes it easier to test the
    code without having to mock SerializerManager functionality.
    
    Tested with newly added unit tests (UnsafeShuffleWriterSuite for the write
    side and ExternalAppendOnlyMapSuite for integration), and by running some
    apps that failed without the fix.
    
    Author: Marcelo Vanzin <vanzin@cloudera.com>
    
    Closes #15982 from vanzin/SPARK-18546.
    
    (cherry picked from commit 93e9d880bf8a144112d74a6897af4e36fcfa5807)

commit 11182b694875431826aee56c52e33f196272a709
Author: Marcelo Vanzin <vanzin@cloudera.com>
Date:   Mon Nov 28 21:10:57 2016 -0800

    [SPARK-18547][CORE] Propagate I/O encryption key when executors register.
    
    This change modifies the method used to propagate encryption keys used during
    shuffle. Instead of relying on YARN's UserGroupInformation credential propagation,
    this change explicitly distributes the key using the messages exchanged between
    driver and executor during registration. When RPC encryption is enabled, this means
    key propagation is also secure.
    
    This allows shuffle encryption to work in non-YARN mode, which means that it's
    easier to write unit tests for areas of the code that are affected by the feature.
    
    The key is stored in the SecurityManager; because there are many instances of
    that class used in the code, the key is only guaranteed to exist in the instance
    managed by the SparkEnv. This path was chosen to avoid storing the key in the
    SparkConf, which would risk having the key being written to disk as part of the
    configuration (as, for example, is done when starting YARN applications).
    
    Tested by new and existing unit tests (which were moved from the YARN module to
    core), and by running apps with shuffle encryption enabled.
    
    Author: Marcelo Vanzin <vanzin@cloudera.com>
    
    Closes #15981 from vanzin/SPARK-18547.
    
    (cherry picked from commit 8b325b17ecdf013b7a6edcb7ee3773546bd914df)

commit fdf1459a380666851db8cc5368442656751b3734
Author: Jenkins <dev-kitchen@cloudera.com>
Date:   Mon Nov 28 16:24:44 2016 -0800

    Updating Maven version to 5.11.0-SNAPSHOT

commit 928a10b9b5d37b860d922f94dd61392db59c44f5
Author: Imran Rashid <irashid@cloudera.com>
Date:   Wed Nov 23 17:16:21 2016 -0600

    CLOUDERA-PREVIEW.  SPARK-8425.  App executor and node blacklisting.
    
    This is a cloudera preview of SPARK-8425.  This expands executor and
    node blacklisting to include blacklisting for an entire application.

commit 433781cd0530e32be976152f094a6ca3e0de70be
Author: Imran Rashid <irashid@cloudera.com>
Date:   Wed Oct 26 06:43:42 2016 -0700

    [SPARK-18117][CORE] Add test for TaskSetBlacklist
    
    This adds tests to verify the interaction between TaskSetBlacklist and
    TaskSchedulerImpl.  TaskSetBlacklist was introduced by SPARK-17675 but
    it neglected to add these tests.
    
    This change does not fix any bugs -- it is just for increasing test
    coverage.

commit 31e430e59574d7fb0baba1d09a956d31f20f293e
Author: Imran Rashid <irashid@cloudera.com>
Date:   Wed Oct 12 16:43:03 2016 -0500

    [SPARK-17675][CORE] Expand Blacklist for TaskSets
    
    This is a step along the way to SPARK-8425.
    
    To enable incremental review, the first step proposed here is to expand the blacklisting within tasksets. In particular, this will enable blacklisting for
    * (task, executor) pairs (this already exists via an undocumented config)
    * (task, node)
    * (taskset, executor)
    * (taskset, node)
    
    Adding (task, node) is critical to making spark fault-tolerant of one-bad disk in a cluster, without requiring careful tuning of "spark.task.maxFailures". The other additions are also important to avoid many misleading task failures and long scheduling delays when there is one bad node on a large cluster.
    
    Note that some of the code changes here aren't really required for just this -- they put pieces in place for SPARK-8425 even though they are not used yet (eg. the `BlacklistTracker` helper is a little out of place, `TaskSetBlacklist` holds onto a little more info than it needs to for just this change, and `ExecutorFailuresInTaskSet` is more complex than it needs to be).
    
    Added unit tests, run tests via jenkins.
    
    Author: Imran Rashid <irashid@cloudera.com>
    Author: mwws <wei.mao@intel.com>
    
    Closes #15249 from squito/taskset_blacklist_only.
    
    (cherry picked from commit 9ce7d3e542e786c62f047c13f3001e178f76e06a)

commit 4440f2a6e3c0c102feeec9a64eeecebc1b6b75a5
Author: Imran Rashid <irashid@cloudera.com>
Date:   Thu Sep 29 15:36:40 2016 -0400

    [SPARK-17648][CORE] TaskScheduler really needs offers to be an IndexedSeq
    
    The Seq[WorkerOffer] is accessed by index, so it really should be an
    IndexedSeq, otherwise an O(n) operation becomes O(n^2).  In practice
    this hasn't been an issue b/c where these offers are generated, the call
    to `.toSeq` just happens to create an IndexedSeq anyway.I got bitten by
    this in performance tests I was doing, and its better for the types to be
    more precise so eg. a change in Scala doesn't destroy performance.
    
    Unit tests via jenkins.
    
    Author: Imran Rashid <irashid@cloudera.com>
    
    Closes #15221 from squito/SPARK-17648.
    
    (cherry picked from commit 7f779e7439127efa0e3611f7745e1c8423845198)

commit 5337d313505573c924fb3c1186d986d8cfb41a24
Author: Imran Rashid <irashid@cloudera.com>
Date:   Wed Sep 21 17:49:36 2016 -0400

    [SPARK-17623][CORE] Clarify type of TaskEndReason with a failed task.
    
    In TaskResultGetter, enqueueFailedTask currently deserializes the result
    as a TaskEndReason. But the type is actually more specific, its a
    TaskFailedReason. This just leads to more blind casting later on – it
    would be more clear if the msg was cast to the right type immediately,
    so method parameter types could be tightened.
    
    Existing unit tests via jenkins.  Note that the code was already performing a blind-cast to a TaskFailedReason before in any case, just in a different spot, so there shouldn't be any behavior change.
    
    Author: Imran Rashid <irashid@cloudera.com>
    
    Closes #15181 from squito/SPARK-17623.
    
    (cherry picked from commit 9fcf1c51d518847eda7f5ea71337cfa7def3c45c)

commit 0cb2842165d307bd4ba569504e13650d5dfe1439
Author: Imran Rashid <irashid@cloudera.com>
Date:   Wed Nov 23 13:50:51 2016 -0600

    Revert "CLOUDERA-BUILD. CDH-27213. Preview of SPARK-8425."
    
    The original preview commit has been superseded by changes upstream.
    There are no known issues with the original approach, but we update to
    be closer to the upstream version.
    
    This reverts commit be681557e65a2489db37f4e9a5f1defb4c84528c.

commit 98ace9e1a6931fc0bfc54775afb205e67e560c74
Author: Mark Grover <mark@apache.org>
Date:   Wed Nov 23 17:01:28 2016 -0800

    CDH-46684, CDH-46209: Preview of SPARK-18535
    
    [SPARK-18535][UI][YARN] Redact sensitive information from Spark logs and UI
    
    This patch adds a new property called spark.secret.redactionPattern that
    allows users to specify a scala regex to decide which Spark configuration
    properties and environment variables in driver and executor environments
    contain sensitive information. When this regex matches the property or
    environment variable name, its value is redacted from the environment UI and
    various logs like YARN and event logs.
    
    This change uses this property to redact information from event logs and YARN
    logs. It also, updates the UI code to adhere to this property instead of
    hardcoding the logic to decipher which properties are sensitive.
    
    Here's an image of the UI post-redaction:
    image
    
    Here's the text in the YARN logs, post-redaction:
    HADOOP_CREDSTORE_PASSWORD -> *********(redacted)
    
    Here's the text in the event logs, post-redaction:
    ...,"spark.executorEnv.HADOOP_CREDSTORE_PASSWORD":"*********(redacted)","spark.yarn.appMasterEnv.HADOOP_CREDSTORE_PASSWORD":"*********(redacted)",...
    
    How was this patch tested?
    
    Unit tests are added to ensure that redaction works.
    A YARN job reading data off of S3 with confidential information
    (hadoop credential provider password) being provided in the environment
    variables of driver and executor. And, afterwards, logs were grepped to make
    sure that no mention of secret password was present. It was also ensure that
    the job was able to read the data off of S3 correctly, thereby ensuring that
    the sensitive information was being trickled down to the right places to read
    the data.
    The event logs were checked to make sure no mention of secret password was
    present.
    UI environment tab was checked to make sure there was no secret information
    being displayed.

commit ac6b4b1580be1de820d0202e40b5697b6a966bd3
Author: Josh Rosen <joshrosen@databricks.com>
Date:   Tue Aug 30 13:15:21 2016 -0700

    [SPARK-17304] Fix perf. issue caused by TaskSetManager.abortIfCompletelyBlacklisted
    
    This patch addresses a minor scheduler performance issue that was introduced in #13603. If you run
    
    ```
    sc.parallelize(1 to 100000, 100000).map(identity).count()
    ```
    
    then most of the time ends up being spent in `TaskSetManager.abortIfCompletelyBlacklisted()`:
    
    ![image](https://cloud.githubusercontent.com/assets/50748/18071032/428732b0-6e07-11e6-88b2-c9423cd61f53.png)
    
    When processing resource offers, the scheduler uses a nested loop which considers every task set at multiple locality levels:
    
    ```scala
       for (taskSet <- sortedTaskSets; maxLocality <- taskSet.myLocalityLevels) {
          do {
            launchedTask = resourceOfferSingleTaskSet(
                taskSet, maxLocality, shuffledOffers, availableCpus, tasks)
          } while (launchedTask)
        }
    ```
    
    In order to prevent jobs with globally blacklisted tasks from hanging, #13603 added a `taskSet.abortIfCompletelyBlacklisted` call inside of  `resourceOfferSingleTaskSet`; if a call to `resourceOfferSingleTaskSet` fails to schedule any tasks, then `abortIfCompletelyBlacklisted` checks whether the tasks are completely blacklisted in order to figure out whether they will ever be schedulable. The problem with this placement of the call is that the last call to `resourceOfferSingleTaskSet` in the `while` loop will return `false`, implying that  `resourceOfferSingleTaskSet` will call `abortIfCompletelyBlacklisted`, so almost every call to `resourceOffers` will trigger the `abortIfCompletelyBlacklisted` check for every task set.
    
    Instead, I think that this call should be moved out of the innermost loop and should be called _at most_ once per task set in case none of the task set's tasks can be scheduled at any locality level.
    
    Before this patch's changes, the microbenchmark example that I posted above took 35 seconds to run, but it now only takes 15 seconds after this change.
    
    /cc squito and kayousterhout for review.
    
    Author: Josh Rosen <joshrosen@databricks.com>
    
    Closes #14871 from JoshRosen/bail-early-if-no-cpus.
    
    (cherry picked from commit fb20084313470593d8507a43fcb2cde2a4c854d9)

commit 358438fb8698e6efb75cb0913474e2c0de9ea772
Author: Davies Liu <davies@databricks.com>
Date:   Wed Oct 19 22:55:30 2016 -0700

    [SPARK-16078][SQL] Backport: from_utc_timestamp/to_utc_timestamp should not depends on local timezone
    
    ## What changes were proposed in this pull request?
    
    Back-port of https://github.com/apache/spark/pull/13784 to `branch-1.6`
    
    ## How was this patch tested?
    
    Existing tests.
    
    Author: Davies Liu <davies@databricks.com>
    
    Closes #15554 from srowen/SPARK-16078.
    
    (cherry picked from commit 82e98f1265f98b49893e04590989b623169d66d9)

commit 477ac454769a21d7da888dbc9aee3932f7e7ce73
Author: prigarg <prigarg@adobe.com>
Date:   Fri Oct 14 11:28:16 2016 -0700

    [SPARK-17884][SQL] To resolve Null pointer exception when casting from  empty string to interval type
    
    ## What changes were proposed in this pull request?
    This change adds a check in castToInterval method of Cast expression , such that if converted value is null , then isNull variable should be set to true.
    
    Earlier, the expression Cast(Literal(), CalendarIntervalType) was throwing NullPointerException because of the above mentioned reason.
    
    ## How was this patch tested?
    Added test case in CastSuite.scala
    
    jira entry for detail: https://issues.apache.org/jira/browse/SPARK-17884
    
    Author: prigarg <prigarg@adobe.com>
    
    Closes #15479 from priyankagargnitk/cast_empty_string_bug.
    
    (cherry picked from commit 745c5e70fb4be0cdd3006e88dbf3ba42d729e421)

commit 804cf400fb12511b33895a81900e40feefda9811
Author: Shixiong Zhu <shixiong@databricks.com>
Date:   Thu Oct 13 00:33:00 2016 -0700

    [SPARK-17850][CORE] Add a flag to ignore corrupt files (branch 1.6)
    
    ## What changes were proposed in this pull request?
    
    This is the patch for 1.6. It only adds Spark conf `spark.files.ignoreCorruptFiles` because SQL just uses HadoopRDD directly in 1.6. `spark.files.ignoreCorruptFiles` is `true` by default.
    
    ## How was this patch tested?
    
    The added test.
    
    Author: Shixiong Zhu <shixiong@databricks.com>
    
    Closes #15454 from zsxwing/SPARK-17850-1.6.
    
    (cherry picked from commit 585c5657f9452b7a1f4f6c9c0a9d933ebb4ed7b0)

commit 937de7daa1d19f1b333dcf24880a1feea6f696d2
Author: Burak Yavuz <brkyvz@gmail.com>
Date:   Thu Oct 6 13:47:49 2016 -0700

    [SPARK-15062][SQL] Backport fix list type infer serializer issue
    
    This backports https://github.com/apache/spark/commit/733cbaa3c0ff617a630a9d6937699db37ad2943b
    to Branch 1.6. It's a pretty simple patch, and would be nice to have for Spark 1.6.3.
    
    Unit tests
    
    Author: Burak Yavuz <brkyvz@gmail.com>
    
    Closes #15380 from brkyvz/bp-SPARK-15062.
    
    Signed-off-by: Michael Armbrust <michael@databricks.com>
    (cherry picked from commit d3890deb7bb49dd5016c823a0de0429f73877d70)

commit 800c9d9890de6b6f272075f1730d5f2cca8caafc
Author: Bjarne Fruergaard <bwahlgreen@gmail.com>
Date:   Sat Oct 1 19:28:51 2016 -0700

    [SPARK-17721][MLLIB][BACKPORT] Fix for multiplying transposed SparseMatrix with SparseVector
    
    Backport PR of changes relevant to mllib only, but otherwise identical to #15296
    
    jkbradley
    
    Author: Bjarne Fruergaard <bwahlgreen@gmail.com>
    
    Closes #15311 from bwahlgreen/bugfix-spark-17721-1.6.
    
    (cherry picked from commit 376545e4d38cd41b4a3233819d63bb81f5c83283)

commit 868375bf4f09c252a234473a1fb0e64eafb70a9d
Author: Josh Rosen <joshrosen@databricks.com>
Date:   Tue Sep 27 10:57:15 2016 -0700

    [SPARK-17618] Fix invalid comparisons between UnsafeRow and other row formats
    
    ## What changes were proposed in this pull request?
    
    This patch addresses a correctness bug in Spark 1.6.x in where `coalesce()` declares that it can process `UnsafeRows` but mis-declares that it always outputs safe rows. If UnsafeRow and other Row types are compared for equality then we will get spurious `false` comparisons, leading to wrong answers in operators which perform whole-row comparison (such as `distinct()` or `except()`). An example of a query impacted by this bug is given in the [JIRA ticket](https://issues.apache.org/jira/browse/SPARK-17618).
    
    The problem is that the validity of our row format conversion rules depends on operators which handle `unsafeRows` (signalled by overriding `canProcessUnsafeRows`) correctly reporting their output row format (which is done by overriding `outputsUnsafeRows`). In #9024, we overrode `canProcessUnsafeRows` but forgot to override `outputsUnsafeRows`, leading to the incorrect `equals()` comparison.
    
    Our interface design is flawed because correctness depends on operators correctly overriding multiple methods this problem could have been prevented by a design which coupled row format methods / metadata into a single method / class so that all three methods had to be overridden at the same time.
    
    This patch addresses this issue by adding missing `outputsUnsafeRows` overrides. In order to ensure that bugs in this logic are uncovered sooner, I have modified `UnsafeRow.equals()` to throw an `IllegalArgumentException` if it is called with an object that is not an `UnsafeRow`.
    
    ## How was this patch tested?
    
    I believe that the stronger misuse-checking in `UnsafeRow.equals()` is sufficient to detect and prevent this class of bug.
    
    Author: Josh Rosen <joshrosen@databricks.com>
    
    Closes #15185 from JoshRosen/SPARK-17618.
    
    (cherry picked from commit e2ce0caed9cc2380cc24d61a0685a4534f21a7f3)

commit a59ee23cb1e472b9c00acea6ffdcb691748e1792
Author: Shixiong Zhu <shixiong@databricks.com>
Date:   Mon Sep 26 11:03:05 2016 -0700

    [SPARK-17649][CORE] Log how many Spark events got dropped in AsynchronousListenerBus (branch 1.6)
    
    ## What changes were proposed in this pull request?
    
    Backport #15220 to 1.6.
    
    ## How was this patch tested?
    
    Jenkins
    
    Author: Shixiong Zhu <shixiong@databricks.com>
    
    Closes #15226 from zsxwing/SPARK-17649-branch-1.6.
    
    (cherry picked from commit 7aded55e7329d331728ce2ec24d8ec915204d775)

commit 0521ff1bd736c2fb703cbebaec284d8183448cfa
Author: Josh Rosen <joshrosen@databricks.com>
Date:   Thu Sep 22 11:05:35 2016 -0700

    [SPARK-17485] Prevent failed remote reads of cached blocks from failing entire job (branch-1.6 backport)
    
    This patch is a branch-1.6 backport of #15037:
    
    In Spark's `RDD.getOrCompute` we first try to read a local copy of a cached RDD block, then a remote copy, and only fall back to recomputing the block if no cached copy (local or remote) can be read. This logic works correctly in the case where no remote copies of the block exist, but if there _are_ remote copies and reads of those copies fail (due to network issues or internal Spark bugs) then the BlockManager will throw a `BlockFetchException` that will fail the task (and which could possibly fail the whole job if the read failures keep occurring).
    
    In the cases of TorrentBroadcast and task result fetching we really do want to fail the entire job in case no remote blocks can be fetched, but this logic is inappropriate for reads of cached RDD blocks because those can/should be recomputed in case cached blocks are unavailable.
    
    Therefore, I think that the `BlockManager.getRemoteBytes()` method should never throw on remote fetch errors and, instead, should handle failures by returning `None`.
    
    Block manager changes should be covered by modified tests in `BlockManagerSuite`: the old tests expected exceptions to be thrown on failed remote reads, while the modified tests now expect `None` to be returned from the `getRemote*` method.
    
    I also manually inspected all usages of `BlockManager.getRemoteValues()`, `getRemoteBytes()`, and `get()` to verify that they correctly pattern-match on the result and handle `None`. Note that these `None` branches are already exercised because the old `getRemoteBytes` returned `None` when no remote locations for the block could be found (which could occur if an executor died and its block manager de-registered with the master).
    
    Author: Josh Rosen <joshrosen@databricks.com>
    
    Closes #15186 from JoshRosen/SPARK-17485-branch-1.6-backport.
    
    (cherry picked from commit 94524cef4cf367a0e73ebe0e919cc21f25f1043f)

commit 3c291924a32d220f67f369c5a37d477f3d6684eb
Author: Josh Rosen <joshrosen@databricks.com>
Date:   Wed Sep 21 11:38:10 2016 -0700

    [SPARK-17418] Prevent kinesis-asl-assembly artifacts from being published
    
    This patch updates the `kinesis-asl-assembly` build to prevent that module from being published as part of Maven releases and snapshot builds.
    
    The `kinesis-asl-assembly` includes classes from the Kinesis Client Library (KCL) and Kinesis Producer Library (KPL), both of which are licensed under the Amazon Software License and are therefore prohibited from being distributed in Apache releases.
    
    Author: Josh Rosen <joshrosen@databricks.com>
    
    Closes #15167 from JoshRosen/stop-publishing-kinesis-assembly.
    
    (cherry picked from commit ce0a222f56ffaf85273d2935b3e6d02aa9f6fa48)

commit db9e5b0ef7e96a1af3ac6a87f0f37cd1e01c7126
Author: Sean Zhong <seanzhong@databricks.com>
Date:   Wed Sep 21 16:53:34 2016 +0800

    [SPARK-17617][SQL] Remainder(%) expression.eval returns incorrect result on double value
    
    ## What changes were proposed in this pull request?
    
    Remainder(%) expression's `eval()` returns incorrect result when the dividend is a big double. The reason is that Remainder converts the double dividend to decimal to do "%", and that lose precision.
    
    This bug only affects the `eval()` that is used by constant folding, the codegen path is not impacted.
    
    ### Before change
    ```
    scala> -5083676433652386516D % 10
    res2: Double = -6.0
    
    scala> spark.sql("select -5083676433652386516D % 10 as a").show
    +---+
    |  a|
    +---+
    |0.0|
    +---+
    ```
    
    ### After change
    ```
    scala> spark.sql("select -5083676433652386516D % 10 as a").show
    +----+
    |   a|
    +----+
    |-6.0|
    +----+
    ```
    
    ## How was this patch tested?
    
    Unit test.
    
    Author: Sean Zhong <seanzhong@databricks.com>
    
    Closes #15171 from clockfly/SPARK-17617.
    
    (cherry picked from commit 3977223a3268aaf6913a325ee459139a4a302b1c)
    Signed-off-by: Wenchen Fan <wenchen@databricks.com>
    (cherry picked from commit 8f88412c31dc840c15df9822638645381c82a2fe)

commit f7779de1159d4f7a83442b4bed5f623037ed42ad
Author: Josh Rosen <joshrosen@databricks.com>
Date:   Thu Sep 15 11:22:58 2016 -0700

    [SPARK-17547] Ensure temp shuffle data file is cleaned up after error
    
    SPARK-8029 (#9610) modified shuffle writers to first stage their data to a temporary file in the same directory as the final destination file and then to atomically rename this temporary file at the end of the write job. However, this change introduced the potential for the temporary output file to be leaked if an exception occurs during the write because the shuffle writers' existing error cleanup code doesn't handle deletion of the temp file.
    
    This patch avoids this potential cause of disk-space leaks by adding `finally` blocks to ensure that temp files are always deleted if they haven't been renamed.
    
    Author: Josh Rosen <joshrosen@databricks.com>
    
    Closes #15104 from JoshRosen/cleanup-tmp-data-file-in-shuffle-writer.
    
    (cherry picked from commit 5b8f7377d54f83b93ef2bfc2a01ca65fae6d3032)
    Signed-off-by: Josh Rosen <joshrosen@databricks.com>
    (cherry picked from commit 8646b84fb8ed319e3a998f93de4821c723f7d419)

commit eb875cf655497f8ad7072005a3bd545362ffd0d8
Author: Xing SHI <shi-kou@indetail.co.jp>
Date:   Wed Sep 14 13:46:46 2016 -0700

    [SPARK-17465][SPARK CORE] Inappropriate memory management in `org.apache.spark.storage.MemoryStore` may lead to memory leak
    
    ## What changes were proposed in this pull request?
    
    The expression like `if (memoryMap(taskAttemptId) == 0) memoryMap.remove(taskAttemptId)` in method `releaseUnrollMemoryForThisTask` and `releasePendingUnrollMemoryForThisTask` should be called after release memory operation, whatever `memoryToRelease` is > 0 or not.
    
    If the memory of a task has been set to 0 when calling a `releaseUnrollMemoryForThisTask` or a `releasePendingUnrollMemoryForThisTask` method, the key in the memory map corresponding to that task will never be removed from the hash map.
    
    See the details in [SPARK-17465](https://issues.apache.org/jira/browse/SPARK-17465).
    
    Author: Xing SHI <shi-kou@indetail.co.jp>
    
    Closes #15022 from saturday-shi/SPARK-17465.
    
    (cherry picked from commit a447cd88897bc3d76eee0e8757e6545019704f30)

commit 5431cd46f6907e05d340d03f92ab781968c65010
Author: Burak Yavuz <brkyvz@gmail.com>
Date:   Tue Sep 13 16:15:44 2016 -0700

    [SPARK-17531][BACKPORT] Don't initialize Hive Listeners for the Execution Client
    
    If a user provides listeners inside the Hive Conf, the configuration for these listeners are passed to the Hive Execution Client as well. This may cause issues for two reasons:
    1. The Execution Client will actually generate garbage
    2. The listener class needs to be both in the Spark Classpath and Hive Classpath
    
    This PR empties the listener configurations in HiveUtils.newTemporaryConfiguration so that the execution client will not contain the listener confs, but the metadata client will.
    
    Unit tests
    
    Author: Burak Yavuz <brkyvz@gmail.com>
    
    Closes #15087 from brkyvz/overwrite-hive-listeners.
    
    (cherry picked from commit bf3f6d2f10872567e083467da7951a7111975065)

commit 1a86bf3392e2176f8afb5568ab145c8cebbffe49
Author: Yin Huai <yhuai@databricks.com>
Date:   Wed Sep 7 21:55:08 2016 +0800

    [SPARK-17245][SQL][BRANCH-1.6] Do not rely on Hive's session state to retrieve HiveConf
    
    ## What changes were proposed in this pull request?
    Right now, we rely on Hive's `SessionState.get()` to retrieve the HiveConf used by ClientWrapper. However, this conf is actually the HiveConf set with the `state`. There is a small chance that we are trying to use the Hive client in a new thread while the global client has not been created yet. In this case, `SessionState.get()` will return a `null`, which causes a NPE when we call `SessionState.get(). getConf `. Since the conf that we want is actually the conf we set to `state`. I am changing the code to just call `state.getConf` (this is also what Spark 2.0 does).
    
    ## How was this patch tested?
    I have not figured out a good way to reproduce this.
    
    Author: Yin Huai <yhuai@databricks.com>
    
    Closes #14816 from yhuai/SPARK-17245.
    
    (cherry picked from commit 047bc3f13ae193aa49571b6e417f2b5001698bbd)

commit d98d3beebbe791f9398e6541829ca2a98df5ad59
Author: Shixiong Zhu <shixiong@databricks.com>
Date:   Tue Sep 6 16:49:06 2016 -0700

    [SPARK-17316][CORE] Fix the 'ask' type parameter in 'removeExecutor'
    
    ## What changes were proposed in this pull request?
    
    Fix the 'ask' type parameter in 'removeExecutor' to eliminate a lot of error logs `Cannot cast java.lang.Boolean to scala.runtime.Nothing$`
    
    ## How was this patch tested?
    
    Jenkins
    
    Author: Shixiong Zhu <shixiong@databricks.com>
    
    Closes #14983 from zsxwing/SPARK-17316-3.
    
    (cherry picked from commit 175b4344112b376cbbbd05265125ed0e1b87d507)
    Signed-off-by: Shixiong Zhu <shixiong@databricks.com>
    (cherry picked from commit 3f797dd87e11e1aa44a063d6faa7e423e291dc2d)

commit 76fbae278cd2e90717133bb1798910ce7beed384
Author: Sean Zhong <seanzhong@databricks.com>
Date:   Tue Sep 6 20:07:44 2016 +0800

    [SPARK-17356][SQL][1.6] Fix out of memory issue when generating JSON for TreeNode
    
    This is a backport of PR https://github.com/apache/spark/pull/14915 to branch 1.6.
    
    ## What changes were proposed in this pull request?
    
    class `org.apache.spark.sql.types.Metadata` is widely used in mllib to store some ml attributes. `Metadata` is commonly stored in `Alias` expression.
    
    ```
    case class Alias(child: Expression, name: String)(
        val exprId: ExprId = NamedExpression.newExprId,
        val qualifier: Option[String] = None,
        val explicitMetadata: Option[Metadata] = None,
        override val isGenerated: java.lang.Boolean = false)
    ```
    
    The `Metadata` can take a big memory footprint since the number of attributes is big ( in scale of million). When `toJSON` is called on `Alias` expression, the `Metadata` will also be converted to a big JSON string.
    If a plan contains many such kind of `Alias` expressions, it may trigger out of memory error when `toJSON` is called, since converting all `Metadata` references to JSON will take huge memory.
    
    With this PR, we will skip scanning Metadata when doing JSON conversion. For a reproducer of the OOM, and analysis, please look at jira https://issues.apache.org/jira/browse/SPARK-17356.
    
    ## How was this patch tested?
    
    Existing tests.
    
    Author: Sean Zhong <seanzhong@databricks.com>
    
    Closes #14973 from clockfly/json_oom_1.6.
    
    (cherry picked from commit e6480a670dd091f02bbe6eb27fca4f78d14dd252)

commit 8a859aee80b06d6f8e3e9a9038932cb1baa164b6
Author: Dongjoon Hyun <dongjoon@apache.org>
Date:   Tue Sep 6 19:36:12 2016 +0800

    [SPARK-11301][SQL] Fix case sensitivity for filter on partitioned col…
    
    ## What changes were proposed in this pull request?
    
    `DataSourceStrategy` does not consider `SQLConf` in `Context` and always match column names. For instance, `HiveContext` uses case insensitive configuration, but it's ignored in `DataSourceStrategy`. This issue was originally registered at SPARK-11301 against 1.6.0 and seemed to be fixed at that time, but Apache Spark 1.6.2 still handles **partitioned column name** in a case-sensitive way always. This is incorrect like the following.
    
    ```scala
    scala> sql("CREATE TABLE t(a int) PARTITIONED BY (b string) STORED AS PARQUET")
    scala> sql("INSERT INTO TABLE t PARTITION(b='P') SELECT * FROM (SELECT 1) t")
    scala> sql("INSERT INTO TABLE t PARTITION(b='Q') SELECT * FROM (SELECT 2) t")
    scala> sql("SELECT * FROM T WHERE B='P'").show
    +---+---+
    |  a|  b|
    +---+---+
    |  1|  P|
    |  2|  Q|
    +---+---+
    ```
    
    The result is the same with `set spark.sql.caseSensitive=false`. Here is the result in [Databricks CE](https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/6660119172909095/3421754458488607/5162191866050912/latest.html) .
    
    This PR reads the configuration and handle the column name comparison accordingly.
    
    ## How was this patch tested?
    
    Pass the Jenkins test with a modified test.
    
    Author: Dongjoon Hyun <dongjoon@apache.org>
    
    Closes #14970 from dongjoon-hyun/SPARK-11301.
    
    (cherry picked from commit 958039a14e93bb4bab6074ab11d3b168fd2e023e)

commit 8a428af15780d974f4e41151c25e0f243789f634
Author: Sun Rui <sunrui2016@gmail.com>
Date:   Tue May 3 09:29:49 2016 -0700

    [SPARK-15091][SPARKR] Fix warnings and a failure in SparkR test cases with testthat version 1.0.1
    
    Fix warnings and a failure in SparkR test cases with testthat version 1.0.1
    
    SparkR unit test cases.
    
    Author: Sun Rui <sunrui2016@gmail.com>
    
    Closes #12867 from sun-rui/SPARK-15091.
    
    (cherry picked from commit 8b6491fc0b49b4e363887ae4b452ba69fe0290d5)
    Signed-off-by: Shivaram Venkataraman <shivaram@cs.berkeley.edu>
    (cherry picked from commit 21be94b160555fccb390c0c48a401b319d3d45ca)

commit 16fdfaabccc56ace2e622133db5e00806485eb3e
Author: Shixiong Zhu <shixiong@databricks.com>
Date:   Wed Aug 31 10:56:02 2016 -0700

    [SPARK-17316][CORE] Make CoarseGrainedSchedulerBackend.removeExecutor non-blocking
    
    ## What changes were proposed in this pull request?
    
    StandaloneSchedulerBackend.executorRemoved is a blocking call right now. It may cause some deadlock since it's called inside StandaloneAppClient.ClientEndpoint.
    
    This PR just changed CoarseGrainedSchedulerBackend.removeExecutor to be non-blocking. It's safe since the only two usages (StandaloneSchedulerBackend and YarnSchedulerEndpoint) don't need the return value).
    
    ## How was this patch tested?
    
    Jenkins unit tests.
    
    Author: Shixiong Zhu <shixiong@databricks.com>
    
    Closes #14882 from zsxwing/SPARK-17316.
    
    (cherry picked from commit b84a92c248e571af1d81586948d5c84b41e18d07)

commit 9de1d3a3733a238d65f7846aac3ea5488efe2b73
Author: Xin Ren <iamshrek@126.com>
Date:   Wed Aug 17 16:31:42 2016 -0700

    [SPARK-17038][STREAMING] fix metrics retrieval source of 'lastReceivedBatch'
    
    https://issues.apache.org/jira/browse/SPARK-17038
    
    ## What changes were proposed in this pull request?
    
    StreamingSource's lastReceivedBatch_submissionTime, lastReceivedBatch_processingTimeStart, and lastReceivedBatch_processingTimeEnd all use data from lastCompletedBatch instead of lastReceivedBatch.
    
    In particular, this makes it impossible to match lastReceivedBatch_records with a batchID/submission time.
    
    This is apparent when looking at StreamingSource.scala, lines 89-94.
    
    ## How was this patch tested?
    
    Manually running unit tests on local laptop
    
    Author: Xin Ren <iamshrek@126.com>
    
    Closes #14681 from keypointt/SPARK-17038.
    
    (cherry picked from commit e6bef7d52f0e19ec771fb0f3e96c7ddbd1a6a19b)
    Signed-off-by: Shixiong Zhu <shixiong@databricks.com>
    (cherry picked from commit 412b0e8969215411b97efd3d0984dc6cac5d31e0)

commit 32db70a287c763049bcbe0246fcaed1e337e8b27
Author: Wenchen Fan <wenchen@databricks.com>
Date:   Wed Aug 17 09:31:22 2016 -0700

    [SPARK-17102][SQL] bypass UserDefinedGenerator for json format check
    
    We use reflection to convert `TreeNode` to json string, and currently don't support arbitrary object. `UserDefinedGenerator` takes a function object, so we should skip json format test for it, or the tests can be flacky, e.g. `DataFrameSuite.simple explode`, this test always fail with scala 2.10(branch 1.6 builds with scala 2.10 by default), but pass with scala 2.11(master branch builds with scala 2.11 by default).
    
    N/A
    
    Author: Wenchen Fan <wenchen@databricks.com>
    
    Closes #14679 from cloud-fan/json.
    
    (cherry picked from commit 928ca1c6d12b23d84f9b6205e22d2e756311f072)
    Signed-off-by: Yin Huai <yhuai@databricks.com>
    (cherry picked from commit 60de30faf29b77b9488495fbcd57f46e3d9248ab)

commit 3da8eb9b39c2be3cd7df651f0d3e7e81ec28edbe
Author: Yin Huai <yhuai@databricks.com>
Date:   Tue Aug 16 13:42:58 2016 -0700

    [SPARK-16656][SQL][BRANCH-1.6] Try to make CreateTableAsSelectSuite more stable
    
    ## What changes were proposed in this pull request?
    
    This PR backports #14289 to branch 1.6
    
    https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/62593/testReport/junit/org.apache.spark.sql.sources/CreateTableAsSelectSuite/create_a_table__drop_it_and_create_another_one_with_the_same_name/ shows that `create a table, drop it and create another one with the same name` failed. But other runs were good. Seems it is a flaky test. This PR tries to make this test more stable.
    
    Author: Yin Huai <yhuai@databricks.com>
    
    Closes #14668 from yhuai/SPARK-16656-branch1.6.
    
    (cherry picked from commit 5c34029b856efdf80cd139b73bcdb9197fe43e2f)

commit 20c0668c1114731049b5b25e2f375d2d4da51c18
Author: Sean Owen <sowen@cloudera.com>
Date:   Sun Aug 14 12:18:30 2016 +0100

    Revert "[SPARK-17027][ML] Avoid integer overflow in PolynomialExpansion.getPolySize"
    
    This reverts commit b54a586af4b8ca7e8b97311bf5e75e00797de899.
    
    (cherry picked from commit 4d64c7fd170bca2b6dca74c4adb03b369b5e81fc)

commit 25c9c5ba6150ad0553448a1a8ec2a7667de6018e
Author: zero323 <zero323@users.noreply.github.com>
Date:   Sun Aug 14 11:59:24 2016 +0100

    [SPARK-17027][ML] Avoid integer overflow in PolynomialExpansion.getPolySize
    
    Replaces custom choose function with o.a.commons.math3.CombinatoricsUtils.binomialCoefficient
    
    Spark unit tests
    
    Author: zero323 <zero323@users.noreply.github.com>
    
    Closes #14614 from zero323/SPARK-17027.
    
    (cherry picked from commit 0ebf7c1bff736cf54ec47957d71394d5b75b47a7)
    Signed-off-by: Sean Owen <sowen@cloudera.com>
    (cherry picked from commit b54a586af4b8ca7e8b97311bf5e75e00797de899)

commit adbfd8c9f2e1e343638e88782a5a6c81df36960f
Author: Sean Owen <sowen@cloudera.com>
Date:   Sat Aug 13 16:40:49 2016 +0100

    Change check for particular missing file message to accommodate the message that would occur, it seems, only in Hadoop 1.x (and therefore in Spark 1.x)
    
    (cherry picked from commit 8a2b8fcbb05381a5e7de9c1adb85ee167d2fc97e)

commit eb9cb7a2de9bcddc42e21e9128fbdbffff9514ef
Author: Yin Huai <yhuai@databricks.com>
Date:   Fri Aug 12 10:29:05 2016 -0700

    [SPARK-17003][BUILD][BRANCH-1.6] release-build.sh is missing hive-thriftserver for scala 2.11
    
    ## What changes were proposed in this pull request?
    hive-thriftserver works with Scala 2.11 (https://issues.apache.org/jira/browse/SPARK-8013). So, let's publish scala 2.11 artifacts with the flag of `-Phive-thfitserver`. I am also fixing the doc.
    
    Author: Yin Huai <yhuai@databricks.com>
    
    Closes #14586 from yhuai/SPARK-16453-branch-1.6.
    
    (cherry picked from commit 909231d7aec591af2fcf0ffaf0612a8c034bcd7a)

commit 3615306f633b73d5f90606e69f3a54e3f6cd1c2e
Author: Sean Owen <sowen@cloudera.com>
Date:   Thu Aug 11 16:59:54 2016 +0100

    Revert "[SPARK-16831][PYTHON] Fixed bug in CrossValidator.avgMetrics"
    
    This reverts commit 92ee6fbf5d5096245d9f1a84cd3a8e66062dd945.
    
    (cherry picked from commit b3ecff640f5ecaf07edcdc6f284644440f788201)

commit 334cbf1e883401c6b7194f9223a5082fc9c2fa06
Author: Josh Rosen <joshrosen@databricks.com>
Date:   Tue Aug 9 11:21:45 2016 -0700

    [SPARK-16956] Make ApplicationState.MAX_NUM_RETRY configurable
    
    ## What changes were proposed in this pull request?
    
    This patch introduces a new configuration, `spark.deploy.maxExecutorRetries`, to let users configure an obscure behavior in the standalone master where the master will kill Spark applications which have experienced too many back-to-back executor failures. The current setting is a hardcoded constant (10); this patch replaces that with a new cluster-wide configuration.
    
    **Background:** This application-killing was added in 6b5980da796e0204a7735a31fb454f312bc9daac (from September 2012) and I believe that it was designed to prevent a faulty application whose executors could never launch from DOS'ing the Spark cluster via an infinite series of executor launch attempts. In a subsequent patch (#1360), this feature was refined to prevent applications which have running executors from being killed by this code path.
    
    **Motivation for making this configurable:** Previously, if a Spark Standalone application experienced more than `ApplicationState.MAX_NUM_RETRY` executor failures and was left with no executors running then the Spark master would kill that application, but this behavior is problematic in environments where the Spark executors run on unstable infrastructure and can all simultaneously die. For instance, if your Spark driver runs on an on-demand EC2 instance while all workers run on ephemeral spot instances then it's possible for all executors to die at the same time while the driver stays alive. In this case, it may be desirable to keep the Spark application alive so that it can recover once new workers and executors are available. In order to accommodate this use-case, this patch modifies the Master to never kill faulty applications if `spark.deploy.maxExecutorRetries` is negative.
    
    I'd like to merge this patch into master, branch-2.0, and branch-1.6.
    
    ## How was this patch tested?
    
    I tested this manually using `spark-shell` and `local-cluster` mode. This is a tricky feature to unit test and historically this code has not changed very often, so I'd prefer to skip the additional effort of adding a testing framework and would rather rely on manual tests and review for now.
    
    Author: Josh Rosen <joshrosen@databricks.com>
    
    Closes #14544 from JoshRosen/add-setting-for-max-executor-failures.
    
    (cherry picked from commit b89b3a5c8e391fcaebe7ef3c77ef16bb9431d6ab)
    Signed-off-by: Josh Rosen <joshrosen@databricks.com>
    (cherry picked from commit ace458f0330f22463ecf7cbee7c0465e10fba8a8)

commit 0c1099d77879450a9ca76a7e4a83e5f69207ab6c
Author: Dongjoon Hyun <dongjoon@apache.org>
Date:   Sun Aug 7 20:51:54 2016 +0100

    [SPARK-16939][SQL] Fix build error by using `Tuple1` explicitly in StringFunctionsSuite
    
    ## What changes were proposed in this pull request?
    
    This PR aims to fix a build error on branch 1.6 at https://github.com/apache/spark/commit/8d8725208771a8815a60160a5a30dc6ea87a7e6a, but I think we had better have this consistently in master branch, too. It's because there exist other ongoing PR (https://github.com/apache/spark/pull/14525) about this.
    
    https://amplab.cs.berkeley.edu/jenkins/job/spark-branch-1.6-compile-maven-with-yarn-2.3/286/console
    
    ```scala
    [error] /home/jenkins/workspace/spark-branch-1.6-compile-maven-with-yarn-2.3/sql/core/src/test/scala/org/apache/spark/sql/StringFunctionsSuite.scala:82: value toDF is not a member of Seq[String]
    [error]     val df = Seq("aaaac").toDF("s")
    [error]                           ^
    ```
    
    ## How was this patch tested?
    
    After passing Jenkins, run compilation test on branch 1.6.
    ```
    build/mvn -DskipTests -Pyarn -Phadoop-2.3 -Pkinesis-asl -Phive -Phive-thriftserver install
    ```
    
    Author: Dongjoon Hyun <dongjoon@apache.org>
    
    Closes #14526 from dongjoon-hyun/SPARK-16939.
    
    (cherry picked from commit a16983c97b4c6539f97e5d26f163fed49872df2b)
    Signed-off-by: Sean Owen <sowen@cloudera.com>
    (cherry picked from commit a3b06ae0bbd1b7b0e73abdb9fefe9d0bddec0ea9)

commit 512a85fe9cfcc29e89edc9a535c4bf633bc33246
Author: Sean Owen <sowen@cloudera.com>
Date:   Sun Aug 7 12:20:07 2016 +0100

    [SPARK-16409][SQL] regexp_extract with optional groups causes NPE
    
    ## What changes were proposed in this pull request?
    
    regexp_extract actually returns null when it shouldn't when a regex matches but the requested optional group did not. This makes it return an empty string, as apparently designed.
    
    ## How was this patch tested?
    
    Additional unit test
    
    Author: Sean Owen <sowen@cloudera.com>
    
    Closes #14504 from srowen/SPARK-16409.
    
    (cherry picked from commit 8d8725208771a8815a60160a5a30dc6ea87a7e6a)
    Signed-off-by: Sean Owen <sowen@cloudera.com>
    (cherry picked from commit 1a5e762ffbc0a8b437448b9c119a008642f6d346)

commit d62133937cae643de5214ada9358fb3c5226f1e1
Author: Josh Rosen <joshrosen@databricks.com>
Date:   Sat Aug 6 19:29:19 2016 -0700

    [SPARK-16925] Master should call schedule() after all executor exit events, not only failures
    
    This patch fixes a bug in Spark's standalone Master which could cause applications to hang if tasks cause executors to exit with zero exit codes.
    
    As an example of the bug, run
    
    ```
    sc.parallelize(1 to 1, 1).foreachPartition { _ => System.exit(0) }
    ```
    
    on a standalone cluster which has a single Spark application. This will cause all executors to die but those executors won't be replaced unless another Spark application or worker joins or leaves the cluster (or if an executor exits with a non-zero exit code). This behavior is caused by a bug in how the Master handles the `ExecutorStateChanged` event: the current implementation calls `schedule()` only if the executor exited with a non-zero exit code, so a task which causes a JVM to unexpectedly exit "cleanly" will skip the `schedule()` call.
    
    This patch addresses this by modifying the `ExecutorStateChanged` to always unconditionally call `schedule()`. This should be safe because it should always be safe to call `schedule()`; adding extra `schedule()` calls can only affect performance and should not introduce correctness bugs.
    
    I added a regression test in `DistributedSuite`.
    
    Author: Josh Rosen <joshrosen@databricks.com>
    
    Closes #14510 from JoshRosen/SPARK-16925.
    
    (cherry picked from commit 4f5f9b670e1f1783f43feb22490613e72dcff852)
    Signed-off-by: Josh Rosen <joshrosen@databricks.com>
    (cherry picked from commit c1628866af85a8888dbcf793653f9ae59253a484)

commit f9cf6612801a5fd72f920506e9f63b19ab828432
Author: sharkd <sharkd.tu@gmail.com>
Date:   Wed Aug 3 19:20:34 2016 -0700

    [SPARK-16873][CORE] Fix SpillReader NPE when spillFile has no data
    
    ## What changes were proposed in this pull request?
    
    SpillReader NPE when spillFile has no data. See follow logs:
    
    16/07/31 20:54:04 INFO collection.ExternalSorter: spill memory to file:/data4/yarnenv/local/usercache/tesla/appcache/application_1465785263942_56138/blockmgr-db5f46c3-d7a4-4f93-8b77-565e469696fb/09/temp_shuffle_ec3ece08-4569-4197-893a-4a5dfcbbf9fa, fileSize:0.0 B
    16/07/31 20:54:04 WARN memory.TaskMemoryManager: leak 164.3 MB memory from org.apache.spark.util.collection.ExternalSorter3db4b52d
    16/07/31 20:54:04 ERROR executor.Executor: Managed memory leak detected; size = 190458101 bytes, TID = 2358516/07/31 20:54:04 ERROR executor.Executor: Exception in task 1013.0 in stage 18.0 (TID 23585)
    java.lang.NullPointerException
    	at org.apache.spark.util.collection.ExternalSorter$SpillReader.cleanup(ExternalSorter.scala:624)
    	at org.apache.spark.util.collection.ExternalSorter$SpillReader.nextBatchStream(ExternalSorter.scala:539)
    	at org.apache.spark.util.collection.ExternalSorter$SpillReader.<init>(ExternalSorter.scala:507)
    	at org.apache.spark.util.collection.ExternalSorter$SpillableIterator.spill(ExternalSorter.scala:816)
    	at org.apache.spark.util.collection.ExternalSorter.forceSpill(ExternalSorter.scala:251)
    	at org.apache.spark.util.collection.Spillable.spill(Spillable.scala:109)
    	at org.apache.spark.memory.TaskMemoryManager.acquireExecutionMemory(TaskMemoryManager.java:154)
    	at org.apache.spark.memory.TaskMemoryManager.allocatePage(TaskMemoryManager.java:249)
    	at org.apache.spark.memory.MemoryConsumer.allocatePage(MemoryConsumer.java:112)
    	at org.apache.spark.shuffle.sort.ShuffleExternalSorter.acquireNewPageIfNecessary(ShuffleExternalSorter.java:346)
    	at org.apache.spark.shuffle.sort.ShuffleExternalSorter.insertRecord(ShuffleExternalSorter.java:367)
    	at org.apache.spark.shuffle.sort.UnsafeShuffleWriter.insertRecordIntoSorter(UnsafeShuffleWriter.java:237)
    	at org.apache.spark.shuffle.sort.UnsafeShuffleWriter.write(UnsafeShuffleWriter.java:164)
    	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)
    	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
    	at org.apache.spark.scheduler.Task.run(Task.scala:89)
    	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
    	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    	at java.lang.Thread.run(Thread.java:744)
    16/07/31 20:54:30 INFO executor.Executor: Executor is trying to kill task 1090.1 in stage 18.0 (TID 23793)
    16/07/31 20:54:30 INFO executor.CoarseGrainedExecutorBackend: Driver commanded a shutdown
    
    ## How was this patch tested?
    
    Manual test.
    
    Author: sharkd <sharkd.tu@gmail.com>
    Author: sharkdtu <sharkdtu@tencent.com>
    
    Closes #14479 from sharkdtu/master.
    
    (cherry picked from commit 583d91a1957f4258a64184cc6b9007588791d332)
    Signed-off-by: Reynold Xin <rxin@databricks.com>
    (cherry picked from commit d2518acc1df44b1ecb8eed20404bcc1277f358a4)

commit 8f95c797809f960bdc20f0a920e3e4ead0247877
Author: =^_^= <maxmoroz@gmail.com>
Date:   Wed Aug 3 04:18:28 2016 -0700

    [SPARK-16831][PYTHON] Fixed bug in CrossValidator.avgMetrics
    
    avgMetrics was summed, not averaged, across folds
    
    Author: =^_^= <maxmoroz@gmail.com>
    
    Closes #14456 from pkch/pkch-patch-1.
    
    (cherry picked from commit 639df046a250873c26446a037cb832ab28cb5272)
    Signed-off-by: Sean Owen <sowen@cloudera.com>
    (cherry picked from commit 92ee6fbf5d5096245d9f1a84cd3a8e66062dd945)

commit 81d25728ec01ccbeb9db4a5e4814e3ec9015deed
Author: Maciej Brynski <maciej.brynski@adpilot.pl>
Date:   Tue Aug 2 16:07:35 2016 -0700

    [SPARK-15541] Casting ConcurrentHashMap to ConcurrentMap (branch-1.6)
    
    ## What changes were proposed in this pull request?
    
    Casting ConcurrentHashMap to ConcurrentMap allows to run code compiled with Java 8 on Java 7
    
    ## How was this patch tested?
    
    Compilation. Existing automatic tests
    
    Author: Maciej Brynski <maciej.brynski@adpilot.pl>
    
    Closes #14390 from maver1ck/spark-15541.
    
    (cherry picked from commit 797e758b16946aa5779cc302f943eafec34c0c39)

commit fcbf9bfc46b94027311a0b9c70970c27025133b9
Author: Maciej Brynski <maciej.brynski@adpilot.pl>
Date:   Tue Aug 2 08:07:08 2016 -0700

    [SPARK-15541] Casting ConcurrentHashMap to ConcurrentMap (master branch)
    
    Casting ConcurrentHashMap to ConcurrentMap allows to run code compiled with Java 8 on Java 7
    
    Compilation. Existing automatic tests
    
    Author: Maciej Brynski <maciej.brynski@adpilot.pl>
    
    Closes #14459 from maver1ck/spark-15541-master.
    
    (cherry picked from commit 511dede1118f20a7756f614acb6fc88af52c9de9)
    Signed-off-by: Sean Owen <sowen@cloudera.com>
    (cherry picked from commit 8a22275dea74cd79ecd59438fd88bebcae13c944)

commit 7ddf68e6fadee93eb76cfd45a910dab6e4a3f2c0
Author: Wesley Tang <tangmingjun@mininglamp.com>
Date:   Fri Jul 29 13:25:33 2016 -0700

    [SPARK-16664][SQL] Fix persist call on Data frames with more than 200…
    
    ## What changes were proposed in this pull request?
    
    Cherry-pick from d1d5069aa3744d46abd3889abab5f15e9067382a and fix the test case
    
    ## How was this patch tested?
    
    Test updated
    
    Author: Wesley Tang <tangmingjun@mininglamp.com>
    
    Closes #14404 from breakdawn/branch-1.6.
    
    (cherry picked from commit 1b2e6f636c2a59e21451e7d12e2dec32d048bc17)

commit 42586a8f32957b00a4e5fe1d181b02fac6db4422
Author: Sean Owen <sowen@cloudera.com>
Date:   Fri Jul 29 09:06:34 2016 -0700

    [SPARK-16751][HOTFIX] Also update hadoop-1 deps file to reflect derby 10.12.1.1 security fix
    
    ## What changes were proposed in this pull request?
    
    See https://github.com/apache/spark/pull/14379 ; I failed to note in back-porting to 1.6 that an additional Hadoop 1 deps file would need to be updated. This makes that change.
    
    ## How was this patch tested?
    
    Jenkins tests.
    
    Author: Sean Owen <sowen@cloudera.com>
    
    Closes #14403 from srowen/SPARK-16751.2.
    
    (cherry picked from commit 03913af30bc0f000dcdcff768ba1625952270ef4)

commit 36620eabf16a864abb9ad83d4f5f0668c14c85c9
Author: Sean Owen <sowen@cloudera.com>
Date:   Fri Jul 29 05:40:58 2016 -0700

    Revert "[SPARK-16664][SQL] Fix persist call on Data frames with more than 200…"
    
    This reverts commit 15abbf9d26fd80ae44d6aaee4b435ec4dc08aa95.
    
    (cherry picked from commit f445cce94e525fd8005948d69c40c81e31140b1c)

commit 858817e1bb620e8f3f9025cde1976aa626cb235c
Author: Adam Roberts <aroberts@uk.ibm.com>
Date:   Fri Jul 29 04:43:01 2016 -0700

    [SPARK-16751] Upgrade derby to 10.12.1.1
    
    Version of derby upgraded based on important security info at VersionEye. Test scope added so we don't include it in our final package anyway. NB: I think this should be backported to all previous releases as it is a security problem https://www.versioneye.com/java/org.apache.derby:derby/10.11.1.1
    
    The CVE number is 2015-1832. I also suggest we add a SECURITY tag for JIRAs
    
    Existing tests with the change making sure that we see no new failures. I checked derby 10.12.x and not derby 10.11.x is downloaded to our ~/.m2 folder.
    
    I then used dev/make-distribution.sh and checked the dist/jars folder for Spark 2.0: no derby jar is present.
    
    I don't know if this would also remove it from the assembly jar in our 1.x branches.
    
    Author: Adam Roberts <aroberts@uk.ibm.com>
    
    Closes #14379 from a-roberts/patch-4.
    
    (cherry picked from commit 04a2c072d94874f3f7ae9dd94c026e8826a75ccd)
    Signed-off-by: Sean Owen <sowen@cloudera.com>
    (cherry picked from commit b6f60756942486a9367b1e7b6bd89ac476576114)

commit a156377540a47680ac6d924217e13d4bea776569
Author: Wesley Tang <tangmingjun@mininglamp.com>
Date:   Fri Jul 29 04:26:05 2016 -0700

    [SPARK-16664][SQL] Fix persist call on Data frames with more than 200…
    
    f12f11e578169b47e3f8b18b299948c0670ba585 introduced this bug, missed foreach as map
    
    Test added
    
    Author: Wesley Tang <tangmingjun@mininglamp.com>
    
    Closes #14324 from breakdawn/master.
    
    (cherry picked from commit d1d5069aa3744d46abd3889abab5f15e9067382a)
    Signed-off-by: Sean Owen <sowen@cloudera.com>
    (cherry picked from commit 15abbf9d26fd80ae44d6aaee4b435ec4dc08aa95)

commit 4fc0c54ad77fb07420b7b1011d9af74abb00b682
Author: Yanbo Liang <ybliang8@gmail.com>
Date:   Wed Jul 27 11:24:28 2016 +0100

    [MINOR][ML] Fix some mistake in LinearRegression formula.
    
    ## What changes were proposed in this pull request?
    Fix some mistake in ```LinearRegression``` formula.
    
    ## How was this patch tested?
    Documents change, no tests.
    
    Author: Yanbo Liang <ybliang8@gmail.com>
    
    Closes #14369 from yanboliang/LiR-formula.
    
    (cherry picked from commit 3c3371bbd6361011b138cce88f6396a2aa4e2cb9)
    Signed-off-by: Sean Owen <sowen@cloudera.com>
    (cherry picked from commit 4ff9892f61b75def8e9082d954dfcebf7e91a8e3)

commit 6acf46d21d6ce482868dcbca96f4f8457fde6145
Author: Anthony Truchet <a.truchet@criteo.com>
Date:   Wed Jul 20 10:39:59 2016 +0100

    [SPARK-16440][MLLIB] Destroy broadcasted variables even on driver
    
    ## What changes were proposed in this pull request?
    Forgotten broadcasted variables were persisted into a previous #PR 14153). This PR turns those `unpersist()` into `destroy()` so that memory is freed even on the driver.
    
    ## How was this patch tested?
    Unit Tests in Word2VecSuite were run locally.
    
    This contribution is done on behalf of Criteo, according to the
    terms of the Apache license 2.0.
    
    Author: Anthony Truchet <a.truchet@criteo.com>
    
    Closes #14268 from AnthonyTruchet/SPARK-16440.
    
    (cherry picked from commit 0dc79ffd1cbb45e69a35e3f5334c9a13290037a0)
    Signed-off-by: Sean Owen <sowen@cloudera.com>
    (cherry picked from commit f6e0c17370cfaed766e2f3a4f8ae175e28420c08)

commit 29dfd499a4d7c34c85c3ff56f2bb57474aa5a792
Author: Yin Huai <yhuai@databricks.com>
Date:   Thu Jul 14 12:00:31 2016 -0700

    [SPARK-16313][SQL][BRANCH-1.6] Spark should not silently drop exceptions in file listing
    
    ## What changes were proposed in this pull request?
    Spark silently drops exceptions during file listing. This is a very bad behavior because it can mask legitimate errors and the resulting plan will silently have 0 rows. This patch changes it to not silently drop the errors.
    
    After making partition discovery not silently drop exceptions, HiveMetastoreCatalog can trigger partition discovery on empty tables, which cause FileNotFoundExceptions (these Exceptions were dropped by partition discovery silently). To address this issue, this PR introduces two **hacks** to workaround the issues. These two hacks try to avoid of triggering partition discovery on empty tables in HiveMetastoreCatalog.
    
    ## How was this patch tested?
    Manually tested.
    
    **Note: This is a backport of https://github.com/apache/spark/pull/13987**
    
    Author: Yin Huai <yhuai@databricks.com>
    
    Closes #14139 from yhuai/SPARK-16313-branch-1.6.
    
    (cherry picked from commit 6ea7d4bd393911d2d15b61e78df7473a7ea9b161)

commit bde57a20f2bef20daab2b10f0e281bdfdb46745a
Author: Sean Owen <sowen@cloudera.com>
Date:   Wed Jul 13 11:39:32 2016 +0100

    [SPARK-16440][MLLIB] Undeleted broadcast variables in Word2Vec causing OoM for long runs
    
    ## What changes were proposed in this pull request?
    
    Unpersist broadcasted vars in Word2Vec.fit for more timely / reliable resource cleanup
    
    ## How was this patch tested?
    
    Jenkins tests
    
    Author: Sean Owen <sowen@cloudera.com>
    
    Closes #14153 from srowen/SPARK-16440.
    
    (cherry picked from commit 51ade51a9fd64fc2fe651c505a286e6f29f59d40)
    Signed-off-by: Sean Owen <sowen@cloudera.com>
    (cherry picked from commit 4381e212140102b4bce756146c09e866c7b2d85c)

commit 87c8cde7eb648fb50b9503b5d17ba44cf9b094a1
Author: Alex Bozarth <ajbozart@us.ibm.com>
Date:   Wed Jul 13 10:45:06 2016 +0100

    [SPARK-16375][WEB UI] Fixed misassigned var: numCompletedTasks was assigned to numSkippedTasks
    
    ## What changes were proposed in this pull request?
    
    I fixed a misassigned var,  numCompletedTasks was assigned to numSkippedTasks in the convertJobData method
    
    ## How was this patch tested?
    
    dev/run-tests
    
    Author: Alex Bozarth <ajbozart@us.ibm.com>
    
    Closes #14141 from ajbozarth/spark16375.
    
    (cherry picked from commit f156136dae5df38f73a25cf3fb48f98f417ef059)
    Signed-off-by: Sean Owen <sowen@cloudera.com>
    (cherry picked from commit fb0933681db199af85543ccb5601b44a4af92321)

commit 6ac7630a0182010b8706dc2fa4b4873d8554f60a
Author: Reynold Xin <rxin@databricks.com>
Date:   Tue Jul 12 23:40:37 2016 -0700

    [HOTFIX] Fix build break.
    
    (cherry picked from commit 980db2bd491398ac4d6db3a4550f1a377b6bf577)

commit c7e06e3fb67e7e231f85ebcdd37f0a91a56f94f3
Author: Reynold Xin <rxin@databricks.com>
Date:   Tue Jul 12 10:07:23 2016 -0700

    [SPARK-16489][SQL] Guard against variable reuse mistakes in expression code generation
    
    In code generation, it is incorrect for expressions to reuse variable names across different instances of itself. As an example, SPARK-16488 reports a bug in which pmod expression reuses variable name "r".
    
    This patch updates ExpressionEvalHelper test harness to always project two instances of the same expression, which will help us catch variable reuse problems in expression unit tests. This patch also fixes the bug in crc32 expression.
    
    This is a test harness change, but I also created a new test suite for testing the test harness.
    
    Author: Reynold Xin <rxin@databricks.com>
    
    Closes #14146 from rxin/SPARK-16489.
    
    (cherry picked from commit c377e49e38a290e5c4fbc178278069788674dfb7)
    Signed-off-by: Reynold Xin <rxin@databricks.com>
    (cherry picked from commit 7c8a399a292de113ebec4235ebe21c9a8fb85256)

commit 89f186e911eb057a9f6dbc453614dd1076159881
Author: Sameer Agarwal <sameer@databricks.com>
Date:   Mon Jul 11 20:26:01 2016 -0700

    [SPARK-16488] Fix codegen variable namespace collision in pmod and partitionBy
    
    This patch fixes a variable namespace collision bug in pmod and partitionBy
    
    Regression test for one possible occurrence. A more general fix in `ExpressionEvalHelper.checkEvaluation` will be in a subsequent PR.
    
    Author: Sameer Agarwal <sameer@databricks.com>
    
    Closes #14144 from sameeragarwal/codegen-bug.
    
    (cherry picked from commit 9cc74f95edb6e4f56151966139cd0dc24e377949)
    Signed-off-by: Reynold Xin <rxin@databricks.com>
    (cherry picked from commit 689261465ad1dd443ebf764ad837243418b986ef)
    Signed-off-by: Reynold Xin <rxin@databricks.com>
    (cherry picked from commit d1c992fea3e5999a3494c398f5040d6102f30aff)

commit d152a5f021b2664be78b77ff6be9f5dff86e61cb
Author: Eric Liang <ekl@databricks.com>
Date:   Tue Jul 12 23:09:02 2016 -0700

    [SPARK-16514][SQL] Fix various regex codegen bugs
    
    ## What changes were proposed in this pull request?
    
    RegexExtract and RegexReplace currently crash on non-nullable input due use of a hard-coded local variable name (e.g. compiles fail with `java.lang.Exception: failed to compile: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 85, Column 26: Redefinition of local variable "m" `).
    
    This changes those variables to use fresh names, and also in a few other places.
    
    ## How was this patch tested?
    
    Unit tests. rxin
    
    Author: Eric Liang <ekl@databricks.com>
    
    Closes #14168 from ericl/sc-3906.
    
    (cherry picked from commit 1c58fa905b6543d366d00b2e5394dfd633987f6d)
    Signed-off-by: Reynold Xin <rxin@databricks.com>
    (cherry picked from commit 9808735e0ce91c68df4c1ce82c44543995d44aed)

commit a93236df6b51a39b088467d5778318d98d0a5720
Author: Marcelo Vanzin <vanzin@cloudera.com>
Date:   Tue Jul 5 16:55:22 2016 -0700

    [SPARK-16385][CORE] Catch correct exception when calling method via reflection.
    
    Using "Method.invoke" causes an exception to be thrown, not an error, so
    Utils.waitForProcess() was always throwing an exception when run on Java 7.
    
    Author: Marcelo Vanzin <vanzin@cloudera.com>
    
    Closes #14056 from vanzin/SPARK-16385.
    
    (cherry picked from commit 59f9c1bd1adfea7069e769fb68351c228c37c8fc)
    Signed-off-by: Sean Owen <sowen@cloudera.com>
    (cherry picked from commit 702178d1f1f02aaa6efa2de84f23d11be5a8e681)

commit 50a6246a6032aaaee33e7088c2cde494e65afd10
Author: Shixiong Zhu <shixiong@databricks.com>
Date:   Thu Jul 7 10:34:50 2016 -0700

    Revert "[SPARK-16372][MLLIB] Retag RDD to tallSkinnyQR of RowMatrix"
    
    This reverts commit 45dda92214191310a56333a2085e2343eba170cd.
    
    (cherry picked from commit bb92788f96426e57555ba5771e256c6425e0e75e)

commit 93b7e74522195e335ce32f4ac5c57f28ea86638f
Author: Xusen Yin <yinxusen@gmail.com>
Date:   Thu Jul 7 11:28:04 2016 +0100

    [SPARK-16372][MLLIB] Retag RDD to tallSkinnyQR of RowMatrix
    
    ## What changes were proposed in this pull request?
    
    The following Java code because of type erasing:
    
    ```Java
    JavaRDD<Vector> rows = jsc.parallelize(...);
    RowMatrix mat = new RowMatrix(rows.rdd());
    QRDecomposition<RowMatrix, Matrix> result = mat.tallSkinnyQR(true);
    ```
    
    We should use retag to restore the type to prevent the following exception:
    
    ```Java
    java.lang.ClassCastException: [Ljava.lang.Object; cannot be cast to [Lorg.apache.spark.mllib.linalg.Vector;
    ```
    
    ## How was this patch tested?
    
    Java unit test
    
    Author: Xusen Yin <yinxusen@gmail.com>
    
    Closes #14051 from yinxusen/SPARK-16372.
    
    (cherry picked from commit 4c6f00d09c016dfc1d2de6e694dff219c9027fa0)
    Signed-off-by: Sean Owen <sowen@cloudera.com>
    (cherry picked from commit 45dda92214191310a56333a2085e2343eba170cd)

commit eec40b77bad18b1b239444107e419e6cf0a90252
Author: jerryshao <sshao@hortonworks.com>
Date:   Wed Jul 6 14:49:21 2016 +0100

    [MINOR][CORE][1.6-BACKPORT] Fix display wrong free memory size in the log
    
    ## What changes were proposed in this pull request?
    
    Free memory size displayed in the log is wrong (used memory), fix to make it correct. Backported to 1.6.
    
    ## How was this patch tested?
    
    N/A
    
    Author: jerryshao <sshao@hortonworks.com>
    
    Closes #14043 from jerryshao/memory-log-fix-1.6-backport.
    
    (cherry picked from commit 2588776ad3d91e39300d61c8a12dc0803c28e866)

commit 6031b6406129e401286d5ca976a497058371d84f
Author: Sean Owen <sowen@cloudera.com>
Date:   Wed Jul 6 12:27:17 2016 +0100

    [MINOR][BUILD] Download Maven 3.3.9 instead of 3.3.3 because the latter is no longer published on Apache mirrors
    
    ## What changes were proposed in this pull request?
    
    Download Maven 3.3.9 instead of 3.3.3 because the latter is no longer published on Apache mirrors
    
    ## How was this patch tested?
    
    Jenkins
    
    Author: Sean Owen <sowen@cloudera.com>
    
    Closes #14066 from srowen/Maven339Branch16.
    
    (cherry picked from commit 76781950fd500ace0f939951fc7a94a58aca87c4)

commit c5b65a2134e73bc83031e38d5da66097c6be5062
Author: Michael Allman <michael@videoamp.com>
Date:   Mon Jul 4 21:16:17 2016 +0100

    [SPARK-16353][BUILD][DOC] Missing javadoc options for java unidoc
    
    Link to Jira issue: https://issues.apache.org/jira/browse/SPARK-16353
    
    ## What changes were proposed in this pull request?
    
    The javadoc options for the java unidoc generation are ignored when generating the java unidoc. For example, the generated `index.html` has the wrong HTML page title. This can be seen at http://spark.apache.org/docs/latest/api/java/index.html.
    
    I changed the relevant setting scope from `doc` to `(JavaUnidoc, unidoc)`.
    
    ## How was this patch tested?
    
    I ran `docs/jekyll build` and verified that the java unidoc `index.html` has the correct HTML page title.
    
    Author: Michael Allman <michael@videoamp.com>
    
    Closes #14031 from mallman/spark-16353.
    
    (cherry picked from commit 7dbffcdd6dc76b8e8d6a9cd6eeb24323a6b740c3)
    Signed-off-by: Sean Owen <sowen@cloudera.com>
    (cherry picked from commit 4fcb88843f5591619f85867e0407a49ed2a9756c)

commit ec937037adc6761114ee944abc3d24765079d498
Author: gatorsmile <gatorsmile@gmail.com>
Date:   Tue Jul 5 00:40:08 2016 +0800

    [SPARK-16329][SQL][BACKPORT-1.6] Star Expansion over Table Containing No Column #14040
    
    #### What changes were proposed in this pull request?
    Star expansion over a table containing zero column does not work since 1.6. However, it works in Spark 1.5.1. This PR is to fix the issue in the master branch.
    
    For example,
    ```scala
    val rddNoCols = sqlContext.sparkContext.parallelize(1 to 10).map(_ => Row.empty)
    val dfNoCols = sqlContext.createDataFrame(rddNoCols, StructType(Seq.empty))
    dfNoCols.registerTempTable("temp_table_no_cols")
    sqlContext.sql("select * from temp_table_no_cols").show
    ```
    
    Without the fix, users will get the following the exception:
    ```
    java.lang.IllegalArgumentException: requirement failed
            at scala.Predef$.require(Predef.scala:221)
            at org.apache.spark.sql.catalyst.analysis.UnresolvedStar.expand(unresolved.scala:199)
    ```
    
    #### How was this patch tested?
    Tests are added
    
    Author: gatorsmile <gatorsmile@gmail.com>
    
    Closes #14042 from gatorsmile/starExpansionEmpty.
    
    (cherry picked from commit c25aa8fca64a1a83e909a8f9baddb7b2a3fdaec5)

commit 73190dc30ecff285cf9fc1ce14ff1cf8d7e852c8
Author: MechCoder <mks542@nyu.edu>
Date:   Fri Jul 1 09:27:34 2016 +0100

    [SPARK-15761][MLLIB][PYSPARK] Load ipython when default python is Python3
    
    ## What changes were proposed in this pull request?
    
    I would like to use IPython with Python 3.5. It is annoying when it fails with IPython requires Python 2.7+; please install python2.7 or set PYSPARK_PYTHON when I have a version greater than 2.7
    
    ## How was this patch tested
    It now works with IPython and Python3
    
    Author: MechCoder <mks542@nyu.edu>
    
    Closes #13503 from MechCoder/spark-15761.
    
    (cherry picked from commit 66283ee0b25de2a5daaa21d50a05a7fadec1de77)
    Signed-off-by: Sean Owen <sowen@cloudera.com>
    (cherry picked from commit 1026aba16554f6c5b5a6a3fdc2b9bdb7911a9fcc)

commit 9103430e5d8d819b0fcff90f95fee6ae8cf188d4
Author: Sean Owen <sowen@cloudera.com>
Date:   Fri Jul 1 09:22:27 2016 +0100

    [SPARK-16182][CORE] Utils.scala -- terminateProcess() should call Process.destroyForcibly() if and only if Process.destroy() fails
    
    ## What changes were proposed in this pull request?
    
    Utils.terminateProcess should `destroy()` first and only fall back to `destroyForcibly()` if it fails. It's kind of bad that we're force-killing executors -- and only in Java 8. See JIRA for an example of the impact: no shutdown
    
    While here: `Utils.waitForProcess` should use the Java 8 method if available instead of a custom implementation.
    
    ## How was this patch tested?
    
    Existing tests, which cover the force-kill case, and Amplab tests, which will cover both Java 7 and Java 8 eventually. However I tested locally on Java 8 and the PR builder will try Java 7 here.
    
    Author: Sean Owen <sowen@cloudera.com>
    
    Closes #13973 from srowen/SPARK-16182.
    
    (cherry picked from commit 2075bf8ef6035fd7606bcf20dc2cd7d7b9cda446)
    Signed-off-by: Sean Owen <sowen@cloudera.com>
    (cherry picked from commit 83f86044879b3c6bbfb0f3075cba552070b064cf)

commit 64c2cbe8d0a0d3f43df5119dc71c0cbd1226b8eb
Author: Brian Uri <brian.uri@novetta.com>
Date:   Thu Jun 30 07:52:28 2016 +0100

    [SPARK-16257][BUILD] Update spark_ec2.py to support Spark 1.6.2 and 1.6.3.
    
    ## What changes were proposed in this pull request?
    
    - Adds 1.6.2 and 1.6.3 as supported Spark versions within the bundled spark-ec2 script.
    - Makes the default Spark version 1.6.3 to keep in sync with the upcoming release.
    - Does not touch the newer spark-ec2 scripts in the separate amplabs repository.
    
    ## How was this patch tested?
    
    - Manual script execution:
    
    export AWS_SECRET_ACCESS_KEY=_snip_
    export AWS_ACCESS_KEY_ID=_snip_
    $SPARK_HOME/ec2/spark-ec2 \
        --key-pair=_snip_ \
        --identity-file=_snip_ \
        --region=us-east-1 \
        --vpc-id=_snip_ \
        --slaves=1 \
        --instance-type=t1.micro \
        --spark-version=1.6.2 \
        --hadoop-major-version=yarn \
        launch test-cluster
    
    - Result: Successful creation of a 1.6.2-based Spark cluster.
    
    This contribution is my original work and I license the work to the project under the project's open source license.
    
    Author: Brian Uri <brian.uri@novetta.com>
    
    Closes #13947 from briuri/branch-1.6-bug-spark-16257.
    
    (cherry picked from commit ccc7fa357099e0f621cfc02448ba20d3f6fabc14)

commit 586a0ed8d224567b13d4fb5ec7474908a8504e7f
Author: hyukjinkwon <gurwls223@gmail.com>
Date:   Wed Jun 29 13:11:56 2016 -0700

    [SPARK-16044][SQL] Backport input_file_name() for data source based on NewHadoopRDD to branch 1.6
    
    ## What changes were proposed in this pull request?
    
    This PR backports https://github.com/apache/spark/pull/13759.
    
    (`SqlNewHadoopRDDState` was renamed to `InputFileNameHolder` and `spark` API does not exist in branch 1.6)
    
    ## How was this patch tested?
    
    Unit tests in `ColumnExpressionSuite`.
    
    Author: hyukjinkwon <gurwls223@gmail.com>
    
    Closes #13806 from HyukjinKwon/backport-SPARK-16044.
    
    (cherry picked from commit 1ac830aca089e9f0b9b0bf367236ffc1184eae7e)

commit a7b9f30ece74110d6a924b0581567999e64f3570
Author: Tom Magrino <tmagrino@fb.com>
Date:   Tue Jun 28 13:36:41 2016 -0700

    [SPARK-16148][SCHEDULER] Allow for underscores in TaskLocation in the Executor ID
    
    ## What changes were proposed in this pull request?
    
    Previously, the TaskLocation implementation would not allow for executor ids which include underscores.  This tweaks the string split used to get the hostname and executor id, allowing for underscores in the executor id.
    
    This addresses the JIRA found here: https://issues.apache.org/jira/browse/SPARK-16148
    
    This is moved over from a previous PR against branch-1.6: https://github.com/apache/spark/pull/13857
    
    ## How was this patch tested?
    
    Ran existing unit tests for core and streaming.  Manually ran a simple streaming job with an executor whose id contained underscores and confirmed that the job ran successfully.
    
    This is my original work and I license the work to the project under the project's open source license.
    
    Author: Tom Magrino <tmagrino@fb.com>
    
    Closes #13858 from tmagrino/fixtasklocation.
    
    (cherry picked from commit ae14f362355b131fcb3e3633da7bb14bdd2b6893)
    Signed-off-by: Shixiong Zhu <shixiong@databricks.com>
    (cherry picked from commit 0cb06c993f487d00fdb528af9ac5720ab80bfa8f)

commit 4fd9e047599e92d5abb12f6274ace1251ad93399
Author: hyukjinkwon <gurwls223@gmail.com>
Date:   Mon Jun 27 17:40:37 2016 -0700

    [SPARK-13023][PROJECT INFRA][FOLLOWUP][BRANCH-1.6] Unable to check `root` module ending up failure of Python tests
    
    ## What changes were proposed in this pull request?
    
    This PR fixes incorrect checking for `root` module (meaning all tests).
    
    I realised that https://github.com/apache/spark/pull/13806 is being failed due to this one.
    
    The PR corrects two files in `sql` and `core`. Since it seems fixing `core` module triggers all tests by `root` value from `determine_modules_for_files`.
    So, `changed_modules` becomes as below:
    
    ```
    ['root', 'sql']
    ```
    
    and `module.dependent_modules` becaomes as below:
    
    ```
    ['pyspark-mllib', 'pyspark-ml', 'hive-thriftserver', 'sparkr', 'mllib', 'examples', 'pyspark-sql']
    ```
    
    Now, `modules_to_test` does not include `root` and this checking is skipped but then both `changed_modules` and `modules_to_test` are being merged after that. So, this includes `root` module to test.
    
    This ends up with failing with the message below (e.g. https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/60990/consoleFull):
    
    ```
    Error: unrecognized module 'root'. Supported modules: pyspark-core, pyspark-sql, pyspark-streaming, pyspark-ml, pyspark-mllib
    ```
    
    ## How was this patch tested?
    
    N/A
    
    Author: hyukjinkwon <gurwls223@gmail.com>
    
    Closes #13845 from HyukjinKwon/fix-build-1.6.
    
    (cherry picked from commit 4a67541db24f47f8101a32cb83153bca2dabc759)

commit e014a05ee2dbb4098888c2dfa7000a2f8625faea
Author: 杨浩 <yanghaogn@163.com>
Date:   Mon Jun 27 08:31:52 2016 +0100

    [SPARK-16214][EXAMPLES] fix the denominator of SparkPi
    
    ## What changes were proposed in this pull request?
    
    reduce the denominator of SparkPi by 1
    
    ## How was this patch tested?
    
      integration tests
    
    Author: 杨浩 <yanghaogn@163.com>
    
    Closes #13910 from yanghaogn/patch-1.
    
    (cherry picked from commit b452026324da20f76f7d8b78e5ba1c007712e585)
    Signed-off-by: Sean Owen <sowen@cloudera.com>
    (cherry picked from commit 22a496d2a12e24f97977d324c38f5aa6ff260588)

commit 08d073680679d6f53495223d3ec35b3ee79783f3
Author: Sean Owen <sowen@cloudera.com>
Date:   Sat Jun 25 12:14:14 2016 +0100

    [SPARK-16193][TESTS] Address flaky ExternalAppendOnlyMapSuite spilling tests
    
    ## What changes were proposed in this pull request?
    
    Make spill tests wait until job has completed before returning the number of stages that spilled
    
    ## How was this patch tested?
    
    Existing Jenkins tests.
    
    Author: Sean Owen <sowen@cloudera.com>
    
    Closes #13896 from srowen/SPARK-16193.
    
    (cherry picked from commit e87741589a24821b5fe73e5d9ee2164247998580)
    Signed-off-by: Sean Owen <sowen@cloudera.com>
    (cherry picked from commit 60e095b9bea3caa3e9d1e768d116f911a048d8ec)

commit 96558b2ac32660dc35aadfe4fb4048ffebb7e7a1
Author: José Antonio <joseanmunoz@gmail.com>
Date:   Sat Jun 25 09:11:25 2016 +0100

    [MLLIB] org.apache.spark.mllib.util.SVMDataGenerator generates ArrayIndexOutOfBoundsException. I have found the bug and tested the solution.
    
    ## What changes were proposed in this pull request?
    
    Just adjust the size of an array in line 58 so it does not cause an ArrayOutOfBoundsException in line 66.
    
    ## How was this patch tested?
    
    Manual tests. I have recompiled the entire project with the fix, it has been built successfully and I have run the code, also with good results.
    
    line 66: val yD = blas.ddot(trueWeights.length, x, 1, trueWeights, 1) + rnd.nextGaussian() * 0.1
    crashes because trueWeights has length "nfeatures + 1" while "x" has length "features", and they should have the same length.
    
    To fix this just make trueWeights be the same length as x.
    
    I have recompiled the project with the change and it is working now:
    [spark-1.6.1]$ spark-submit --master local[*] --class org.apache.spark.mllib.util.SVMDataGenerator mllib/target/spark-mllib_2.11-1.6.1.jar local /home/user/test
    
    And it generates the data successfully now in the specified folder.
    
    Author: José Antonio <joseanmunoz@gmail.com>
    
    Closes #13895 from j4munoz/patch-2.
    
    (cherry picked from commit a3c7b4187bad00dad87df7e3b5929a44d29568ed)
    Signed-off-by: Sean Owen <sowen@cloudera.com>
    (cherry picked from commit 24d59fb64770fb8951794df9ee6398329838359a)

commit 861982911f509d053d9a2281ddcc0724851c129c
Author: Dongjoon Hyun <dongjoon@apache.org>
Date:   Fri Jun 24 22:30:52 2016 -0700

    [SPARK-16173] [SQL] Can't join describe() of DataFrame in Scala 2.10
    
    ## What changes were proposed in this pull request?
    
    This PR fixes `DataFrame.describe()` by forcing materialization to make the `Seq` serializable. Currently, `describe()` of `DataFrame` throws `Task not serializable` Spark exceptions when joining in Scala 2.10.
    
    ## How was this patch tested?
    
    Manual. (After building with Scala 2.10, test on bin/spark-shell and bin/pyspark.)
    
    Author: Dongjoon Hyun <dongjoon@apache.org>
    
    Closes #13902 from dongjoon-hyun/SPARK-16173-branch-1.6.
    
    (cherry picked from commit b7acc1b71c5d4b163a7451e8c6430afe920a04e0)

commit 925fde5b5aaa383c05606f0275f8755de55a17a3
Author: Davies Liu <davies@databricks.com>
Date:   Fri Jun 24 14:35:34 2016 -0700

    [SPARK-16077] [PYSPARK] catch the exception from pickle.whichmodule()
    
    ## What changes were proposed in this pull request?
    
    In the case that we don't know which module a object came from, will call pickle.whichmodule() to go throught all the loaded modules to find the object, which could fail because some modules, for example, six, see https://bitbucket.org/gutworth/six/issues/63/importing-six-breaks-pickling
    
    We should ignore the exception here, use `__main__` as the module name (it means we can't find the module).
    
    ## How was this patch tested?
    
    Manual tested. Can't have a unit test for this.
    
    Author: Davies Liu <davies@databricks.com>
    
    Closes #13788 from davies/whichmodule.
    
    (cherry picked from commit d48935400ca47275f677b527c636976af09332c8)
    Signed-off-by: Davies Liu <davies.liu@gmail.com>
    (cherry picked from commit d7223bb9fdc54edcc1a45cead9a71b5bac49b2ab)

commit c8500467c19b5d731ceda20b6faca26d3d93fe73
Author: Shixiong Zhu <shixiong@databricks.com>
Date:   Tue May 10 13:26:53 2016 -0700

    [SPARK-6005][TESTS] Fix flaky test: o.a.s.streaming.kafka.DirectKafkaStreamSuite.offset recovery
    
    ## What changes were proposed in this pull request?
    
    Because this test extracts data from `DStream.generatedRDDs` before stopping, it may get data before checkpointing. Then after recovering from the checkpoint, `recoveredOffsetRanges` may contain something not in `offsetRangesBeforeStop`, which will fail the test. Adding `Thread.sleep(1000)` before `ssc.stop()` will reproduce this failure.
    
    This PR just moves the logic of `offsetRangesBeforeStop` (also renamed to `offsetRangesAfterStop`) after `ssc.stop()` to fix the flaky test.
    
    ## How was this patch tested?
    
    Jenkins unit tests.
    
    Author: Shixiong Zhu <shixiong@databricks.com>
    
    Closes #12903 from zsxwing/SPARK-6005.
    
    (cherry picked from commit 9533f5390a3ad7ab96a7bea01cdb6aed89503a51)
    Signed-off-by: Sean Owen <sowen@cloudera.com>
    (cherry picked from commit 4fdac3c271eccc5db69c45788af15e955752a163)

commit a18d4ee6a44dad54a800046ff7db232e5f90b936
Author: Pete Robbins <robbinspg@gmail.com>
Date:   Thu Jun 2 10:14:51 2016 -0700

    [SPARK-15606][CORE] Use non-blocking removeExecutor call to avoid deadlocks
    
    ## What changes were proposed in this pull request?
    Set minimum number of dispatcher threads to 3 to avoid deadlocks on machines with only 2 cores
    
    ## How was this patch tested?
    
    Spark test builds
    
    Author: Pete Robbins <robbinspg@gmail.com>
    
    Closes #13355 from robbinspg/SPARK-13906.
    
    (cherry picked from commit d98fb19c18f0122f335e5d810a2f8ff752b98d86)

commit 1f81f9d8907a95848bad18fd8c1de566da33ed43
Author: Davies Liu <davies.liu@gmail.com>
Date:   Mon Jun 20 20:50:30 2016 -0700

    [SPARK-16086] [SQL] fix Python UDF without arguments (for 1.6)
    
    ## What changes were proposed in this pull request?
    
    Fix the bug for Python UDF that does not have any arguments.
    
    ## How was this patch tested?
    
    Added regression tests.
    
    Author: Davies Liu <davies.liu@gmail.com>
    
    Closes #13793 from davies/fix_no_arguments.
    
    (cherry picked from commit abe36c53d126bb580e408a45245fd8e81806869c)

commit 7bfbab5f0f5f8df6a8e327b30840ec23bdadcfed
Author: Davies Liu <davies@databricks.com>
Date:   Sun Jun 19 00:34:52 2016 -0700

    [SPARK-15613] [SQL] Fix incorrect days to millis conversion due to Daylight Saving Time
    
    Internally, we use Int to represent a date (the days since 1970-01-01), when we convert that into unix timestamp (milli-seconds since epoch in UTC), we get the offset of a timezone using local millis (the milli-seconds since 1970-01-01 in a timezone), but TimeZone.getOffset() expect unix timestamp, the result could be off by one hour (in Daylight Saving Time (DST) or not).
    
    This PR change to use best effort approximate of posix timestamp to lookup the offset. In the event of changing of DST, Some time is not defined (for example, 2016-03-13 02:00:00 PST), or could lead to multiple valid result in UTC (for example, 2016-11-06 01:00:00), this best effort approximate should be enough in practice.
    
    Added regression tests.
    
    Author: Davies Liu <davies@databricks.com>
    
    Closes #13652 from davies/fix_timezone.
    
    (cherry picked from commit db86e7fd263ca4e24cf8faad95fca3189bab2fb0)

commit b251839fa9ae0e318ebc450d89d13376a210f357
Author: Davies Liu <davies.liu@gmail.com>
Date:   Sun Jun 19 09:30:59 2016 -0700

    Revert "[SPARK-15613] [SQL] Fix incorrect days to millis conversion due to Daylight Saving Time"
    
    This reverts commit 41efd2091781b31118c6d37be59e4f0f4ae2bf66.
    
    (cherry picked from commit 3d569d9ea9357d6161b0c75ce2e6f045c3447458)

commit 0b416cf8b3c83378526ce563b108ded39934940f
Author: Davies Liu <davies@databricks.com>
Date:   Sun Jun 19 00:34:52 2016 -0700

    [SPARK-15613] [SQL] Fix incorrect days to millis conversion due to Daylight Saving Time
    
    ## What changes were proposed in this pull request?
    
    Internally, we use Int to represent a date (the days since 1970-01-01), when we convert that into unix timestamp (milli-seconds since epoch in UTC), we get the offset of a timezone using local millis (the milli-seconds since 1970-01-01 in a timezone), but TimeZone.getOffset() expect unix timestamp, the result could be off by one hour (in Daylight Saving Time (DST) or not).
    
    This PR change to use best effort approximate of posix timestamp to lookup the offset. In the event of changing of DST, Some time is not defined (for example, 2016-03-13 02:00:00 PST), or could lead to multiple valid result in UTC (for example, 2016-11-06 01:00:00), this best effort approximate should be enough in practice.
    
    ## How was this patch tested?
    
    Added regression tests.
    
    Author: Davies Liu <davies@databricks.com>
    
    Closes #13652 from davies/fix_timezone.
    
    (cherry picked from commit 001a58960311b07fe80e2f01e473f4987948d06e)
    Signed-off-by: Davies Liu <davies.liu@gmail.com>
    (cherry picked from commit 41efd2091781b31118c6d37be59e4f0f4ae2bf66)

commit 03ddb6c48613408e9be89288f57083f4056650fe
Author: andreapasqua <andrea@radius.com>
Date:   Fri Jun 17 22:41:05 2016 -0700

    [SPARK-16035][PYSPARK] Fix SparseVector parser assertion for end parenthesis
    
    ## What changes were proposed in this pull request?
    The check on the end parenthesis of the expression to parse was using the wrong variable. I corrected that.
    ## How was this patch tested?
    Manual test
    
    Author: andreapasqua <andrea@radius.com>
    
    Closes #13750 from andreapasqua/sparse-vector-parser-assertion-fix.
    
    (cherry picked from commit 4c64e88d5ba4c36cbdbc903376492f0f43401e4e)
    Signed-off-by: Xiangrui Meng <meng@databricks.com>
    (cherry picked from commit 3f1d730e90ea270fab7d244651d1266cfd7fa893)

commit 185eaa28fc2834d173799223614ff9437726b85d
Author: hyukjinkwon <gurwls223@gmail.com>
Date:   Fri Jun 17 21:04:24 2016 -0700

    [SPARK-15892][ML] Backport correctly merging AFTAggregators to branch 1.6
    
    ## What changes were proposed in this pull request?
    
    This PR backports https://github.com/apache/spark/pull/13619.
    
    The original test added in branch-2.0 was failed in branch-1.6.
    
    This seems because the behaviour was changed in https://github.com/apache/spark/commit/101663f1ae222a919fc40510aa4f2bad22d1be6f. This was failure while calculating Euler's number which ends up with a infinity regardless of this path.
    
    So, I brought the dataset from `AFTSurvivalRegressionExample` to make sure this is working and then wrote the test.
    
    I ran the test before/after creating empty partitions. `model.scale` becomes `1.0` with empty partitions and becames `1.547` without them.
    
    After this patch, this becomes always `1.547`.
    
    ## How was this patch tested?
    
    Unit test in `AFTSurvivalRegressionSuite`.
    
    Author: hyukjinkwon <gurwls223@gmail.com>
    
    Closes #13725 from HyukjinKwon/SPARK-15892-1-6.
    
    (cherry picked from commit fd053892c4f456c2ab6d4d8993704e3cc5013f13)

commit 1e00d1855dbbbe0f10ef70684d802680c7d1e623
Author: Shixiong Zhu <shixiong@databricks.com>
Date:   Fri Jun 17 13:33:31 2016 -0700

    Revert "[SPARK-15395][CORE] Use getHostString to create RpcAddress (backport for 1.6)"
    
    This reverts commit 7ad82b663092615b02bef3991fb1a21af77d2358. See SPARK-16017.
    
    (cherry picked from commit e530823dd5ddc27436a01f00899679bef2affe5e)

commit 6e32d4b9f67a7d2e9c7cf5e2dfd5d383bb166d71
Author: Josh Rosen <joshrosen@databricks.com>
Date:   Thu Jun 16 14:18:58 2016 -0700

    [SPARK-15975] Fix improper Popen retcode code handling in dev/run-tests
    
    In the `dev/run-tests.py` script we check a `Popen.retcode` for success using `retcode > 0`, but this is subtlety wrong because Popen's return code will be negative if the child process was terminated by a signal: https://docs.python.org/2/library/subprocess.html#subprocess.Popen.returncode
    
    In order to properly handle signals, we should change this to check `retcode != 0` instead.
    
    Author: Josh Rosen <joshrosen@databricks.com>
    
    Closes #13692 from JoshRosen/dev-run-tests-return-code-handling.
    
    (cherry picked from commit acef843f67e770f0a2709fb3fbd1a53c200b2bc5)
    Signed-off-by: Andrew Or <andrew@databricks.com>
    (cherry picked from commit 0a8ada5064bec22116363f93ed476352776b49e4)

commit 4e31061e6aee448765a8df6a2c41352ed72fcb0d
Author: Takuya UESHIN <ueshin@happy-camper.st>
Date:   Wed Jun 15 10:05:19 2016 -0700

    [SPARK-15915][SQL] Logical plans should use subqueries eliminated plan when override sameResult.
    
    ## What changes were proposed in this pull request?
    
    This pr is a backport of #13638 for `branch-1.6`.
    
    ## How was this patch tested?
    
    Added the same test as #13638 modified for `branch-1.6`.
    
    Author: Takuya UESHIN <ueshin@happy-camper.st>
    
    Closes #13668 from ueshin/issues/SPARK-15915_1.6.
    
    (cherry picked from commit cffc0800b1a07ca450b0727401a44a8169324e6c)

commit 03f7cfb2f0f2d00cebb0770e8099894a4f847ba1
Author: Joseph K. Bradley <joseph.kurata.bradley@gmail.com>
Date:   Tue Jun 14 14:08:33 2016 -0700

    Revert "[SPARK-15892][ML] Incorrectly merged AFTAggregator with zero total count"
    
    This reverts commit be3c41b2633215ff6f20885c04f288aab25a1712.
    
    (cherry picked from commit 2f3e327c4cbf163d8536c4451b4829ec7d1886a9)

commit f6caddddfe2f99aab93bbee73b2825c571efc82f
Author: hyukjinkwon <gurwls223@gmail.com>
Date:   Sun Jun 12 14:26:53 2016 -0700

    [SPARK-15892][ML] Incorrectly merged AFTAggregator with zero total count
    
    ## What changes were proposed in this pull request?
    
    Currently, `AFTAggregator` is not being merged correctly. For example, if there is any single empty partition in the data, this creates an `AFTAggregator` with zero total count which causes the exception below:
    
    ```
    IllegalArgumentException: u'requirement failed: The number of instances should be greater than 0.0, but got 0.'
    ```
    
    Please see [AFTSurvivalRegression.scala#L573-L575](https://github.com/apache/spark/blob/6ecedf39b44c9acd58cdddf1a31cf11e8e24428c/mllib/src/main/scala/org/apache/spark/ml/regression/AFTSurvivalRegression.scala#L573-L575) as well.
    
    Just to be clear, the python example `aft_survival_regression.py` seems using 5 rows. So, if there exist partitions more than 5, it throws the exception above since it contains empty partitions which results in an incorrectly merged `AFTAggregator`.
    
    Executing `bin/spark-submit examples/src/main/python/ml/aft_survival_regression.py` on a machine with CPUs more than 5 is being failed because it creates tasks with some empty partitions with defualt  configurations (AFAIK, it sets the parallelism level to the number of CPU cores).
    
    ## How was this patch tested?
    
    An unit test in `AFTSurvivalRegressionSuite.scala` and manually tested by `bin/spark-submit examples/src/main/python/ml/aft_survival_regression.py`.
    
    Author: hyukjinkwon <gurwls223@gmail.com>
    Author: Hyukjin Kwon <gurwls223@gmail.com>
    
    Closes #13619 from HyukjinKwon/SPARK-15892.
    
    (cherry picked from commit e3554605b36bdce63ac180cc66dbdee5c1528ec7)
    Signed-off-by: Joseph K. Bradley <joseph@databricks.com>
    (cherry picked from commit be3c41b2633215ff6f20885c04f288aab25a1712)

commit 950175fcd74bcdb9421b0f2609ad9a11197c83ba
Author: Mortada Mehyar <mortada.mehyar@gmail.com>
Date:   Fri Jun 10 00:23:34 2016 -0700

    [DOCUMENTATION] fixed groupby aggregation example for pyspark
    
    ## What changes were proposed in this pull request?
    
    fixing documentation for the groupby/agg example in python
    
    ## How was this patch tested?
    
    the existing example in the documentation dose not contain valid syntax (missing parenthesis) and is not using `Column` in the expression for `agg()`
    
    after the fix here's how I tested it:
    
    ```
    In [1]: from pyspark.sql import Row
    
    In [2]: import pyspark.sql.functions as func
    
    In [3]: %cpaste
    Pasting code; enter '--' alone on the line to stop or use Ctrl-D.
    :records = [{'age': 19, 'department': 1, 'expense': 100},
    : {'age': 20, 'department': 1, 'expense': 200},
    : {'age': 21, 'department': 2, 'expense': 300},
    : {'age': 22, 'department': 2, 'expense': 300},
    : {'age': 23, 'department': 3, 'expense': 300}]
    :--
    
    In [4]: df = sqlContext.createDataFrame([Row(**d) for d in records])
    
    In [5]: df.groupBy("department").agg(df["department"], func.max("age"), func.sum("expense")).show()
    
    +----------+----------+--------+------------+
    |department|department|max(age)|sum(expense)|
    +----------+----------+--------+------------+
    |         1|         1|      20|         300|
    |         2|         2|      22|         600|
    |         3|         3|      23|         300|
    +----------+----------+--------+------------+
    
    Author: Mortada Mehyar <mortada.mehyar@gmail.com>
    
    Closes #13587 from mortada/groupby_agg_doc_fix.
    
    (cherry picked from commit 675a73715d3c8adb9d9a9dce5f76a2db5106790c)
    Signed-off-by: Reynold Xin <rxin@databricks.com>
    (cherry picked from commit 393f4ba1516af47388e72310aee8dbbea9652134)

commit 20f122f3bd5e2b42033d9db086fbbea4c57bb774
Author: Josh Rosen <joshrosen@databricks.com>
Date:   Thu Jun 9 11:04:08 2016 -0700

    [SPARK-15827][BUILD] Publish Spark's forked sbt-pom-reader to Maven Central
    
    Spark's SBT build currently uses a fork of the sbt-pom-reader plugin but depends on that fork via a SBT subproject which is cloned from https://github.com/scrapcodes/sbt-pom-reader/tree/ignore_artifact_id. This unnecessarily slows down the initial build on fresh machines and is also risky because it risks a build breakage in case that GitHub repository ever changes or is deleted.
    
    In order to address these issues, I have published a pre-built binary of our forked sbt-pom-reader plugin to Maven Central under the `org.spark-project` namespace and have updated Spark's build to use that artifact. This published artifact was built from https://github.com/JoshRosen/sbt-pom-reader/tree/v1.0.0-spark, which contains the contents of ScrapCodes's branch plus an additional patch to configure the build for artifact publication.
    
    /cc srowen ScrapCodes for review.
    
    Author: Josh Rosen <joshrosen@databricks.com>
    
    Closes #13564 from JoshRosen/use-published-fork-of-pom-reader.
    
    (cherry picked from commit f74b77713e17960dddb7459eabfdc19f08f4024b)
    Signed-off-by: Josh Rosen <joshrosen@databricks.com>
    (cherry picked from commit 739d992f041b995fbf44b93cf47bced3d3811ad9)

commit 19f6ca81d4263d35f66e1be60428defbe0e1b9e0
Author: Josh Rosen <joshrosen@databricks.com>
Date:   Thu Jun 9 00:51:24 2016 -0700

    [SPARK-12712] Fix failure in ./dev/test-dependencies when run against empty .m2 cache
    
    This patch fixes a bug in `./dev/test-dependencies.sh` which caused spurious failures when the script was run on a machine with an empty `.m2` cache. The problem was that extra log output from the dependency download was conflicting with the grep / regex used to identify the classpath in the Maven output. This patch fixes this issue by adjusting the regex pattern.
    
    Tested manually with the following reproduction of the bug:
    
    ```
    rm -rf ~/.m2/repository/org/apache/commons/
    ./dev/test-dependencies.sh
    ```
    
    Author: Josh Rosen <joshrosen@databricks.com>
    
    Closes #13568 from JoshRosen/SPARK-12712.
    
    (cherry picked from commit 921fa40b14082bfd1094fa49fb3b0c46a79c1aaa)
    Signed-off-by: Josh Rosen <joshrosen@databricks.com>
    (cherry picked from commit bb917fc659ec62718214f2f2fceb03a90515ac3e)

commit 1fffb33666944c86066ce4a0995197fcefbaedec
Author: Jason Lee <cjlee@us.ibm.com>
Date:   Fri Jan 15 12:04:05 2016 +0000

    [SPARK-12655][GRAPHX] GraphX does not unpersist RDDs
    
    Some VertexRDD and EdgeRDD are created during the intermediate step of g.connectedComponents() but unnecessarily left cached after the method is done. The fix is to unpersist these RDDs once they are no longer in use.
    
    A test case is added to confirm the fix for the reported bug.
    
    Author: Jason Lee <cjlee@us.ibm.com>
    
    Closes #10713 from jasoncl/SPARK-12655.
    
    (cherry picked from commit d0a5c32bd05841f411a342a80c5da9f73f30d69a)
    Signed-off-by: Sean Owen <sowen@cloudera.com>
    (cherry picked from commit 5830828efbf863df510a2b5b17d76214863ff48f)

commit 29885e082d0c758767f7d45a9675573973b363dd
Author: Brett Randall <javabrett@gmail.com>
Date:   Sun Jun 5 15:31:56 2016 +0100

    [SPARK-15723] Fixed local-timezone-brittle test where short-timezone form "EST" is …
    
    ## What changes were proposed in this pull request?
    
    Stop using the abbreviated and ambiguous timezone "EST" in a test, since it is machine-local default timezone dependent, and fails in different timezones.  Fixed [SPARK-15723](https://issues.apache.org/jira/browse/SPARK-15723).
    
    ## How was this patch tested?
    
    Note that to reproduce this problem in any locale/timezone, you can modify the scalatest-maven-plugin argLine to add a timezone:
    
        <argLine>-ea -Xmx3g -XX:MaxPermSize=${MaxPermGen} -XX:ReservedCodeCacheSize=${CodeCacheSize} -Duser.timezone="Australia/Sydney"</argLine>
    
    and run
    
        $ mvn test -DwildcardSuites=org.apache.spark.status.api.v1.SimpleDateParamSuite -Dtest=none. Equally this will fix it in an effected timezone:
    
        <argLine>-ea -Xmx3g -XX:MaxPermSize=${MaxPermGen} -XX:ReservedCodeCacheSize=${CodeCacheSize} -Duser.timezone="America/New_York"</argLine>
    
    To test the fix, apply the above change to `pom.xml` to set test TZ to `Australia/Sydney`, and confirm the test now passes.
    
    Author: Brett Randall <javabrett@gmail.com>
    
    Closes #13462 from javabrett/SPARK-15723-SimpleDateParamSuite.
    
    (cherry picked from commit 4e767d0f9042bfea6074c2637438859699ec4dc3)
    Signed-off-by: Sean Owen <sowen@cloudera.com>
    (cherry picked from commit 6a9f19dd57dadb80bccc328cf1d099bed04f7f18)

commit 49308bca33cd7eed789f4e3ca56e676dca67f9cd
Author: Josh Rosen <joshrosen@databricks.com>
Date:   Thu Jun 2 17:47:31 2016 -0700

    [SPARK-15736][CORE][BRANCH-1.6] Gracefully handle loss of DiskStore files
    
    If an RDD partition is cached on disk and the DiskStore file is lost, then reads of that cached partition will fail and the missing partition is supposed to be recomputed by a new task attempt. In the current BlockManager implementation, however, the missing file does not trigger any metadata updates / does not invalidate the cache, so subsequent task attempts will be scheduled on the same executor and the doomed read will be repeatedly retried, leading to repeated task failures and eventually a total job failure.
    
    In order to fix this problem, the executor with the missing file needs to properly mark the corresponding block as missing so that it stops advertising itself as a cache location for that block.
    
    This patch fixes this bug and adds an end-to-end regression test (in `FailureSuite`) and a set of unit tests (`in BlockManagerSuite`).
    
    This is a branch-1.6 backport of #13473.
    
    Author: Josh Rosen <joshrosen@databricks.com>
    
    Closes #13479 from JoshRosen/handle-missing-cache-files-branch-1.6.
    
    (cherry picked from commit 4259a28588a4dceb55d7bf1bf9327065dd751863)

commit 446b9e364d510464618d218d7ad7190afd8874af
Author: Kevin McHale <kevin@premise.com>
Date:   Thu Jun 2 11:17:33 2016 -0500

    [SPARK-14204][SQL] register driverClass rather than user-specified class
    
    This pull request fixes an issue in which cluster-mode executors fail to properly register a JDBC driver when the driver is provided in a jar by the user, but the driver class name is derived from a JDBC URL (rather than specified by the user).  The consequence of this is that all JDBC accesses under the described circumstances fail with an `IllegalStateException`. I reported the issue here: https://issues.apache.org/jira/browse/SPARK-14204
    
    My proposed solution is to have the executors register the JDBC driver class under all circumstances, not only when the driver is specified by the user.
    
    This patch was tested manually.  I built an assembly jar, deployed it to a cluster, and confirmed that the problem was fixed.
    
    Author: Kevin McHale <kevin@premise.com>
    
    Closes #12000 from mchalek/jdbc-driver-registration.
    
    (cherry picked from commit 0a13e4c0712fb83525eb5acbf55aabc4c9891ff7)

commit 77ee66004ffb18f365b00e6a412daa1e895e76b0
Author: Tejas Patil <tejasp@fb.com>
Date:   Tue May 31 19:52:22 2016 -0500

    [SPARK-15601][CORE] CircularBuffer's toString() to print only the contents written if buffer isn't full
    
    1. The class allocated 4x space than needed as it was using `Int` to store the `Byte` values
    
    2. If CircularBuffer isn't full, currently toString() will print some garbage chars along with the content written as is tries to print the entire array allocated for the buffer. The fix is to keep track of buffer getting full and don't print the tail of the buffer if it isn't full (suggestion by sameeragarwal over https://github.com/apache/spark/pull/12194#discussion_r64495331)
    
    3. Simplified `toString()`
    
    Added new test case
    
    Author: Tejas Patil <tejasp@fb.com>
    
    Closes #13351 from tejasapatil/circular_buffer.
    
    (cherry picked from commit ac38bdc756c25632069e7887a657250fe2fd6d82)
    Signed-off-by: Sean Owen <sowen@cloudera.com>
    (cherry picked from commit 714f4d78a09c7cd0a71a3867418d5262b6a14527)

commit aad41323573bb7df77f8b51fd68fda6dbf96444d
Author: Takeshi YAMAMURO <linguin.m.s@gmail.com>
Date:   Tue May 31 07:25:16 2016 -0500

    [SPARK-15528][SQL] Fix race condition in NumberConverter
    
    ## What changes were proposed in this pull request?
    A local variable in NumberConverter is wrongly shared between threads.
    This pr fixes the race condition.
    
    ## How was this patch tested?
    Manually checked.
    
    Author: Takeshi YAMAMURO <linguin.m.s@gmail.com>
    
    Closes #13391 from maropu/SPARK-15528.
    
    (cherry picked from commit 95db8a44f3e2d79913cbe0d29297796b4c3b0d1b)
    Signed-off-by: Sean Owen <sowen@cloudera.com>
    (cherry picked from commit ea84b3373a69d7150f8480a52f1413e7eca339b9)

commit a1fdd6c7f314fdf2300fd14430464214c5e444d9
Author: Shixiong Zhu <shixiong@databricks.com>
Date:   Fri May 20 09:56:50 2016 -0700

    [SPARK-15395][CORE] Use getHostString to create RpcAddress (backport for 1.6)
    
    ## What changes were proposed in this pull request?
    
    Backport #13185 to branch 1.6.
    
    ## How was this patch tested?
    
    Jenkins unit tests.
    
    Author: Shixiong Zhu <shixiong@databricks.com>
    
    Closes #13196 from zsxwing/host-string-1.6.
    
    (cherry picked from commit 7ad82b663092615b02bef3991fb1a21af77d2358)

commit 93345e88bfeaa73822f182af15ffe22ceed5bbe8
Author: Oleg Danilov <oleg.danilov@wandisco.com>
Date:   Thu May 19 22:23:28 2016 -0700

    [SPARK-14261][SQL] Memory leak in Spark Thrift Server
    
    Fixed memory leak (HiveConf in the CommandProcessorFactory)
    
    Author: Oleg Danilov <oleg.danilov@wandisco.com>
    
    Closes #12932 from dosoft/SPARK-14261.
    
    (cherry picked from commit e384c7fbb94cef3c18e8fa8d06159b76b88b5167)
    Signed-off-by: Reynold Xin <rxin@databricks.com>
    (cherry picked from commit 7200e6b544ded6c0a63175b20847958f2105c685)

commit 6b7f86f441cfa32a326d9a3975d669bfa36020e8
Author: Andrew Or <andrew@databricks.com>
Date:   Wed May 11 17:25:57 2016 -0700

    [SPARK-15260] Atomically resize memory pools (branch 1.6)
    
    ## What changes were proposed in this pull request?
    
    (This is the branch-1.6 version of #13039)
    
    When we acquire execution memory, we do a lot of things between shrinking the storage memory pool and enlarging the execution memory pool. In particular, we call memoryStore.evictBlocksToFreeSpace, which may do a lot of I/O and can throw exceptions. If an exception is thrown, the pool sizes on that executor will be in a bad state.
    
    This patch minimizes the things we do between the two calls to make the resizing more atomic.
    
    ## How was this patch tested?
    
    Jenkins.
    
    Author: Andrew Or <andrew@databricks.com>
    
    Closes #13058 from andrewor14/safer-pool-1.6.
    
    (cherry picked from commit fd2da7b91e33e8fc994c4a6a0524831807f1324f)

commit f976687a83d5ca66d9fa7f0898bd406d9c145236
Author: Andrew Or <andrew@databricks.com>
Date:   Wed May 11 13:36:58 2016 -0700

    [SPARK-15262] Synchronize block manager / scheduler executor state
    
    ## What changes were proposed in this pull request?
    
    If an executor is still alive even after the scheduler has removed its metadata, we may receive a heartbeat from that executor and tell its block manager to reregister itself. If that happens, the block manager master will know about the executor, but the scheduler will not.
    
    That is a dangerous situation, because when the executor does get disconnected later, the scheduler will not ask the block manager to also remove metadata for that executor. Later, when we try to clean up an RDD or a broadcast variable, we may try to send a message to that executor, triggering an exception.
    
    ## How was this patch tested?
    
    Jenkins.
    
    Author: Andrew Or <andrew@databricks.com>
    
    Closes #13055 from andrewor14/block-manager-remove.
    
    (cherry picked from commit 40a949aae9c3040019a52482d091912a85b0f4d4)
    Signed-off-by: Shixiong Zhu <shixiong@databricks.com>
    (cherry picked from commit e2a43d0070b7204b1c6ed1c9292f1d215e0df30d)

commit 7d5909f9ef0d6024ebb80d3f88637fb25583e8d8
Author: Shixiong Zhu <shixiong@databricks.com>
Date:   Mon Feb 29 11:52:11 2016 -0800

    [SPARK-13522][CORE] Fix the exit log place for heartbeat
    
    ## What changes were proposed in this pull request?
    
    Just fixed the log place introduced by #11401
    
    ## How was this patch tested?
    
    unit tests.
    
    Author: Shixiong Zhu <shixiong@databricks.com>
    
    Closes #11432 from zsxwing/SPARK-13522-follow-up.
    
    (cherry picked from commit ced71d353a0908abcf5b83503661bef97ae0953d)

commit 74a34e1ee8f5da57228ac4049d6b0c6a7963718f
Author: Shixiong Zhu <shixiong@databricks.com>
Date:   Mon Feb 29 11:02:45 2016 -0800

    [SPARK-13522][CORE] Executor should kill itself when it's unable to heartbeat to driver more than N times
    
    ## What changes were proposed in this pull request?
    
    Sometimes, network disconnection event won't be triggered for other potential race conditions that we may not have thought of, then the executor will keep sending heartbeats to driver and won't exit.
    
    This PR adds a new configuration `spark.executor.heartbeat.maxFailures` to kill Executor when it's unable to heartbeat to the driver more than `spark.executor.heartbeat.maxFailures` times.
    
    ## How was this patch tested?
    
    unit tests
    
    Author: Shixiong Zhu <shixiong@databricks.com>
    
    Closes #11401 from zsxwing/SPARK-13522.
    
    (cherry picked from commit 86bf93e65481b8fe5d7532ca6d4cd29cafc9e9dd)

commit d796b030707f34ec67e206eac2d9da85b4e3a635
Author: Shixiong Zhu <shixiong@databricks.com>
Date:   Fri Feb 26 15:11:57 2016 -0800

    [SPARK-13519][CORE] Driver should tell Executor to stop itself when cleaning executor's state
    
    ## What changes were proposed in this pull request?
    
    When the driver removes an executor's state, the connection between the driver and the executor may be still alive so that the executor cannot exit automatically (E.g., Master will send RemoveExecutor when a work is lost but the executor is still alive), so the driver should try to tell the executor to stop itself. Otherwise, we will leak an executor.
    
    This PR modified the driver to send `StopExecutor` to the executor when it's removed.
    
    ## How was this patch tested?
    
    manual test: increase the worker heartbeat interval to force it's always timeout and the leak executors are gone.
    
    Author: Shixiong Zhu <shixiong@databricks.com>
    
    Closes #11399 from zsxwing/SPARK-13519.
    
    (cherry picked from commit c433c0afd4c3f96ef24686a1f28262af81b67723)

commit 077297a5abe4e68e05aad700bea99f763af13426
Author: xin Wu <xinwu@us.ibm.com>
Date:   Wed May 11 16:30:45 2016 +0800

    [SPARK-14495][SQL][1.6] fix resolution failure of having clause with distinct aggregate function
    
    #### Symptom:
    In the latest **branch 1.6**, when a `DISTINCT` aggregation function is used in the `HAVING` clause, Analyzer throws `AnalysisException` with a message like following:
    ```
    resolved attribute(s) gid#558,id#559 missing from date#554,id#555 in operator !Expand [List(date#554, null, 0, if ((gid#558 = 1)) id#559 else null),List(date#554, id#555, 1, null)], [date#554,id#561,gid#560,if ((gid = 1)) id else null#562];
    ```
    #### Root cause:
    The problem is that the distinct aggregate in having condition are resolved by the rule `DistinctAggregationRewriter` twice, which messes up the resulted `EXPAND` operator.
    
    In a `ResolveAggregateFunctions` rule, when resolving ```Filter(havingCondition, _: Aggregate)```, the `havingCondition` is resolved as an `Aggregate` in a nested loop of analyzer rule execution (by invoking `RuleExecutor.execute`). At this nested level of analysis, the rule `DistinctAggregationRewriter` rewrites this distinct aggregate clause to an expanded two-layer aggregation, where the `aggregateExpresssions` of the final `Aggregate` contains the resolved `gid` and the aggregate expression attributes (In the above case, they are  `gid#558, id#559`).
    
    After completion of the nested analyzer rule execution, the resulted `aggregateExpressions` in the `havingCondition` is pushed down into the underlying `Aggregate` operator. The `DistinctAggregationRewriter` rule is executed again. The `projections` field of `EXPAND` operator is populated with the `aggregateExpressions` of the `havingCondition` mentioned above. However, the attributes (In the above case, they are `gid#558, id#559`) in the projection list of `EXPAND` operator can not be found in the underlying relation.
    
    #### Solution:
    This PR retrofits part of [#11579](https://github.com/apache/spark/pull/11579) that moves the `DistinctAggregationRewriter` to the beginning of Optimizer, so that it guarantees that the rewrite only happens after all the aggregate functions are resolved first. Thus, it avoids resolution failure.
    
    #### How is the PR change tested
    New [test cases ](https://github.com/xwu0226/spark/blob/f73428f94746d6d074baf6702589545bdbd11cad/sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/AggregationQuerySuite.scala#L927-L988) are added to drive `DistinctAggregationRewriter` rewrites for multi-distinct aggregations , involving having clause.
    
    A following up PR will be submitted to add these test cases to master(2.0) branch.
    
    Author: xin Wu <xinwu@us.ibm.com>
    
    Closes #12974 from xwu0226/SPARK-14495_review.
    
    (cherry picked from commit d1654864a60503a5e495a1261f55ceb89f916984)

commit 60e812c22da88ac219666f588b6b760415596ad7
Author: Josh Rosen <joshrosen@databricks.com>
Date:   Tue May 10 08:21:32 2016 +0900

    [SPARK-15209] Fix display of job descriptions with single quotes in web UI timeline
    
    ## What changes were proposed in this pull request?
    
    This patch fixes an escaping bug in the Web UI's event timeline that caused Javascript errors when displaying timeline entries whose descriptions include single quotes.
    
    The original bug can be reproduced by running
    
    ```scala
    sc.setJobDescription("double quote: \" ")
    sc.parallelize(1 to 10).count()
    
    sc.setJobDescription("single quote: ' ")
    sc.parallelize(1 to 10).count()
    ```
    
    and then browsing to the driver UI. Previously, this resulted in an "Uncaught SyntaxError" because the single quote from the description was not escaped and ended up closing a Javascript string literal too early.
    
    The fix implemented here is to change the relevant Javascript to define its string literals using double-quotes. Our escaping logic already properly escapes double quotes in the description, so this is safe to do.
    
    ## How was this patch tested?
    
    Tested manually in `spark-shell` using the following cases:
    
    ```scala
    sc.setJobDescription("double quote: \" ")
    sc.parallelize(1 to 10).count()
    
    sc.setJobDescription("single quote: ' ")
    sc.parallelize(1 to 10).count()
    
    sc.setJobDescription("ampersand: &")
    sc.parallelize(1 to 10).count()
    
    sc.setJobDescription("newline: \n text after newline ")
    sc.parallelize(1 to 10).count()
    
    sc.setJobDescription("carriage return: \r text after return ")
    sc.parallelize(1 to 10).count()
    ```
    
    /cc sarutak for review.
    
    Author: Josh Rosen <joshrosen@databricks.com>
    
    Closes #12995 from JoshRosen/SPARK-15209.
    
    (cherry picked from commit 3323d0f931ddd11f41abca11425b5e43a6538667)
    Signed-off-by: Kousuke Saruta <sarutak@oss.nttdata.co.jp>
    (cherry picked from commit 1678bff7f4d3bbcfd80df2ee6ea4213498b29fa3)

commit ab0ce21deceecd8fab928614d2aa57ee6fd26867
Author: Philipp Hoffmann <mail@philipphoffmann.de>
Date:   Mon May 9 11:02:13 2016 -0700

    [SPARK-15223][DOCS] fix wrongly named config reference
    
    ## What changes were proposed in this pull request?
    
    The configuration setting `spark.executor.logs.rolling.size.maxBytes` was changed to `spark.executor.logs.rolling.maxSize` in 1.4 or so.
    
    This commit fixes a remaining reference to the old name in the documentation.
    
    Also the description for `spark.executor.logs.rolling.maxSize` was edited to clearly state that the unit for the size is bytes.
    
    ## How was this patch tested?
    
    no tests
    
    Author: Philipp Hoffmann <mail@philipphoffmann.de>
    
    Closes #13001 from philipphoffmann/patch-3.
    
    (cherry picked from commit 518af0796384cd68927b90de8cb33b5a765c2dd0)

commit 33d7b2c8a5f0ee23021bfb429334db143295c110
Author: cenyuhai <cenyuhai@didichuxing.com>
Date:   Fri May 6 13:50:49 2016 -0700

    [SPARK-13566][CORE] Avoid deadlock between BlockManager and Executor Thread
    
    Temp patch for branch 1.6， avoid deadlock between BlockManager and Executor Thread.
    
    Author: cenyuhai <cenyuhai@didichuxing.com>
    
    Closes #11546 from cenyuhai/SPARK-13566.
    
    (cherry picked from commit ab006523b840b1d2dbf3f5ff0a238558e7665a1e)

commit a0635c76391f9058804c9e20228ae47a8055e50f
Author: Sean Owen <sowen@cloudera.com>
Date:   Tue May 3 13:13:35 2016 +0100

    [SPARK-14897][CORE] Upgrade Jetty to latest version of 8
    
    Update Jetty 8.1 to the latest 2016/02 release, from a 2013/10 release, for security and bug fixes. This does not resolve the JIRA necessarily, as it's still worth considering an update to 9.3.
    
    Jenkins tests
    
    Author: Sean Owen <sowen@cloudera.com>
    
    Closes #12842 from srowen/SPARK-14897.
    
    (cherry picked from commit 57ac7c182465e1653e74a8ad6c826b2cf56a0ad8)
    Signed-off-by: Sean Owen <sowen@cloudera.com>
    (cherry picked from commit 2db19a3af4404b2fa8c48dec261ce073c4ef31f3)

commit 5d692fd12b170834cd00c6d31ff4040f241563b0
Author: Reynold Xin <rxin@databricks.com>
Date:   Fri Apr 29 22:26:12 2016 -0700

    [SPARK-14757] [SQL] Fix nullability bug in EqualNullSafe codegen
    
    This patch fixes a null handling bug in EqualNullSafe's code generation.
    
    Updated unit test so they would fail without the fix.
    
    Closes #12628.
    
    Author: Reynold Xin <rxin@databricks.com>
    Author: Arash Nabili <arash@levyx.com>
    
    Closes #12799 from rxin/equalnullsafe.
    
    (cherry picked from commit 10d513feb4b657d62d576b39e97ad87a988a9e2a)

commit 1d0242632ed61e839e50df5da3ba0893fb9b90df
Author: Gregory Hart <greg.hart@thinkbiganalytics.com>
Date:   Thu Apr 28 11:21:43 2016 -0700

    [SPARK-14965][SQL] Indicate an exception is thrown for a missing struct field
    
    ## What changes were proposed in this pull request?
    
    Fix to ScalaDoc for StructType.
    
    ## How was this patch tested?
    
    Built locally.
    
    Author: Gregory Hart <greg.hart@thinkbiganalytics.com>
    
    Closes #12758 from freastro/hotfix/SPARK-14965.
    
    (cherry picked from commit 12c360c057f09d13a31c458ad277640b5f6de394)
    Signed-off-by: Reynold Xin <rxin@databricks.com>
    (cherry picked from commit 8ac0ce6dd742edc50ceebb65946c24bfa4b4f301)

commit bb9d0b6958c55208ec78eee04e83faed495d7a63
Author: Yin Huai <yhuai@databricks.com>
Date:   Wed Apr 27 16:33:30 2016 -0700

    [SPARK-13023][PROJECT INFRA][BRANCH-1.6] Fix handling of root module in modules_to_test()
    
    This is a 1.6 branch backport of SPARK-13023 based on JoshRosen's https://github.com/apache/spark/commit/41f0c85f9be264103c066935e743f59caf0fe268.
    
    There's a minor bug in how we handle the `root` module in the `modules_to_test()` function in `dev/run-tests.py`: since `root` now depends on `build` (since every test needs to run on any build test), we now need to check for the presence of root in `modules_to_test` instead of `changed_modules`.
    
    Author: Yin Huai <yhuai@databricks.com>
    
    Closes #12743 from yhuai/1.6build.
    
    (cherry picked from commit f4af6a8b3ce5cea4dc4096e43001c7d60fce8cdb)

commit 3e829e56e93ca97168c62d6ba85aa611cc8ced17
Author: Joseph K. Bradley <joseph@databricks.com>
Date:   Wed Apr 27 16:11:12 2016 -0700

    [SPARK-14671][ML] Pipeline setStages should handle subclasses of PipelineStage
    
    Pipeline.setStages failed for some code examples which worked in 1.5 but fail in 1.6.  This tends to occur when using a mix of transformers from ml.feature. It is because Java Arrays are non-covariant and the addition of MLWritable to some transformers means the stages0/1 arrays above are not of type Array[PipelineStage].  This PR modifies the following to accept subclasses of PipelineStage:
    * Pipeline.setStages()
    * Params.w()
    
    Unit test which fails to compile before this fix.
    
    Author: Joseph K. Bradley <joseph@databricks.com>
    
    Closes #12430 from jkbradley/pipeline-setstages.
    
    (cherry picked from commit f5ebb18c45ffdee2756a80f64239cb9158df1a11)
    Signed-off-by: Joseph K. Bradley <joseph@databricks.com>
    
    Conflicts:
    	mllib/src/main/scala/org/apache/spark/ml/Pipeline.scala
    	mllib/src/test/scala/org/apache/spark/ml/PipelineSuite.scala
    
    (cherry picked from commit 5e53d4a8dc68390d0cc2722fc4a5b4f341b8125f)

commit ab3c8bcad9c6065685d96c8c4b8afa484709644e
Author: Joseph K. Bradley <joseph@databricks.com>
Date:   Tue Apr 26 14:00:39 2016 -0700

    [SPARK-14159][ML] Fixed bug in StringIndexer + related issue in RFormula - 1.6 backport
    
    Backport of [https://github.com/apache/spark/pull/11965] for branch-1.6.
    There were no merge conflicts.
    
    ## What changes were proposed in this pull request?
    
    StringIndexerModel.transform sets the output column metadata to use name inputCol.  It should not.  Fixing this causes a problem with the metadata produced by RFormula.
    
    Fix in RFormula: I added the StringIndexer columns to prefixesToRewrite, and I modified VectorAttributeRewriter to find and replace all "prefixes" since attributes collect multiple prefixes from StringIndexer + Interaction.
    
    Note that "prefixes" is no longer accurate since internal strings may be replaced.
    
    ## How was this patch tested?
    
    Unit test which failed before this fix.
    
    Author: Joseph K. Bradley <joseph@databricks.com>
    
    Closes #12595 from jkbradley/StringIndexer-fix-1.6.
    
    (cherry picked from commit 496496b2548387ad2889eaaf884ab039096436ef)

commit fcbb0de1addb7f9995d6ac364b4cdcc21e3834c3
Author: hyukjinkwon <gurwls223@gmail.com>
Date:   Thu Apr 21 11:32:27 2016 +0100

    [SPARK-14787][SQL] Upgrade Joda-Time library from 2.9 to 2.9.3
    
    ## What changes were proposed in this pull request?
    https://issues.apache.org/jira/browse/SPARK-14787
    
    The possible problems are described in the JIRA above. Please refer this if you are wondering the purpose of this PR.
    
    This PR upgrades Joda-Time library from 2.9 to 2.9.3.
    
    ## How was this patch tested?
    
    `sbt scalastyle` and Jenkins tests in this PR.
    
    closes #11847
    
    Author: hyukjinkwon <gurwls223@gmail.com>
    
    Closes #12552 from HyukjinKwon/SPARK-14787.
    
    (cherry picked from commit ec2a276022568944e19a51d4d39305710cdc7c0f)
    Signed-off-by: Sean Owen <sowen@cloudera.com>
    (cherry picked from commit c3ed9504d4b323106b44b03ac69c425f7aae8d3d)

commit e7b3e373d3dcb849089199cbb6fb871c6fba93b4
Author: Joseph K. Bradley <joseph@databricks.com>
Date:   Fri Apr 15 11:50:21 2016 -0700

    [SPARK-14665][ML][PYTHON] Fixed bug with StopWordsRemover default stopwords
    
    The default stopwords were a Java object.  They are no longer.
    
    Unit test which failed before the fix
    
    Author: Joseph K. Bradley <joseph@databricks.com>
    
    Closes #12422 from jkbradley/pyspark-stopwords.
    
    (cherry picked from commit d6ae7d4637d23c57c4eeab79d1177216f380ec9c)
    Signed-off-by: Joseph K. Bradley <joseph@databricks.com>
    
    Conflicts:
    	python/pyspark/ml/feature.py
    	python/pyspark/ml/tests.py
    
    (cherry picked from commit 58dfba66ee9b87cdbce6bc2b01324025c7514669)

commit 7b7d0f3dc23a4ac99e2227b5fa0de48c39f5def6
Author: Joseph K. Bradley <joseph@databricks.com>
Date:   Thu Apr 14 12:44:59 2016 -0700

    [SPARK-14618][ML][DOC] Updated RegressionEvaluator.metricName param doc
    
    ## What changes were proposed in this pull request?
    
    In Spark 1.4, we negated some metrics from RegressionEvaluator since CrossValidator always maximized metrics. This was fixed in 1.5, but the docs were not updated. This PR updates the docs.
    
    ## How was this patch tested?
    
    no tests
    
    Author: Joseph K. Bradley <joseph@databricks.com>
    
    Closes #12377 from jkbradley/regeval-doc.
    
    (cherry picked from commit bf65c87f706019d235d7093637341668a13b1be1)
    Signed-off-by: Joseph K. Bradley <joseph@databricks.com>
    (cherry picked from commit 93c9a63ea6b31f1c0208e73f3ce59f255c0460f5)

commit e82603f6cf645cdcd968e22870b5a1f8e44c1bad
Author: Davies Liu <davies@databricks.com>
Date:   Tue Apr 12 15:03:00 2016 -0700

    [SPARK-14544] [SQL] improve performance of SQL UI tab
    
    ## What changes were proposed in this pull request?
    
    This PR improve the performance of SQL UI by:
    
    1) remove the details column in all executions page (the first page in SQL tab). We can check the details by enter the execution page.
    2) break-all is super slow in Chrome recently, so switch to break-word.
    3) Using "display: none" to hide a block.
    4) using one js closure for  for all the executions, not one for each.
    5) remove the height limitation of details, don't need to scroll it in the tiny window.
    
    ## How was this patch tested?
    
    Exists tests.
    
    ![ui](https://cloud.githubusercontent.com/assets/40902/14445712/68d7b258-0004-11e6-9b48-5d329b05d165.png)
    
    Author: Davies Liu <davies@databricks.com>
    
    Closes #12311 from davies/ui_perf.
    
    (cherry picked from commit 582ed8a6e01bcb06e8b0eac9f691476a4485fe72)

commit 5488b75e0d665d979c54087f2799de3c3496d19e
Author: Xiangrui Meng <meng@databricks.com>
Date:   Tue Apr 12 11:30:09 2016 -0700

    [SPARK-14563][ML] use a random table name instead of __THIS__ in SQLTransformer
    
    ## What changes were proposed in this pull request?
    
    Use a random table name instead of `__THIS__` in SQLTransformer, and add a test for `transformSchema`. The problems of using `__THIS__` are:
    
    * It doesn't work under HiveContext (in Spark 1.6)
    * Race conditions
    
    ## How was this patch tested?
    
    * Manual test with HiveContext.
    * Added a unit test for `transformSchema` to improve coverage.
    
    cc: yhuai
    
    Author: Xiangrui Meng <meng@databricks.com>
    
    Closes #12330 from mengxr/SPARK-14563.
    
    (cherry picked from commit 1995c2e6482bf4af5a4be087bfc156311c1bec19)
    Signed-off-by: Xiangrui Meng <meng@databricks.com>
    (cherry picked from commit 2554c35e77bd9f58a45f2ded7e2ff291af6ecc78)

commit 4637ce1012a67a406b5634e96a3f33745f7b0ff1
Author: Yanbo Liang <ybliang8@gmail.com>
Date:   Fri Apr 8 11:49:44 2016 -0700

    [SPARK-14298][ML][MLLIB] LDA should support disable checkpoint
    
    ## What changes were proposed in this pull request?
    In the doc of [```checkpointInterval```](https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/ml/param/shared/sharedParams.scala#L241), we told users that they can disable checkpoint by setting ```checkpointInterval = -1```. But we did not handle this situation for LDA actually, we should fix this bug.
    ## How was this patch tested?
    Existing tests.
    
    cc jkbradley
    
    Author: Yanbo Liang <ybliang8@gmail.com>
    
    Closes #12089 from yanboliang/spark-14298.
    
    (cherry picked from commit 56af8e85cca056096fe4e765d8d287e0f9efc0d2)
    Signed-off-by: Joseph K. Bradley <joseph@databricks.com>
    (cherry picked from commit 05dbc28463ef5730708732a288953b76842cdd92)

commit 710b89c701f5991137a5443ebdd7ad02fdfa5b95
Author: Sameer Agarwal <sameer@databricks.com>
Date:   Mon Apr 11 10:20:22 2016 -0700

    [SPARK-14454] [1.6] Better exception handling while marking tasks as failed
    
    Backports https://github.com/apache/spark/pull/12234 to 1.6. Original description below:
    
    ## What changes were proposed in this pull request?
    
    This patch adds support for better handling of exceptions inside catch blocks if the code within the block throws an exception. For instance here is the code in a catch block before this change in `WriterContainer.scala`:
    
    ```scala
    logError("Aborting task.", cause)
    // call failure callbacks first, so we could have a chance to cleanup the writer.
    TaskContext.get().asInstanceOf[TaskContextImpl].markTaskFailed(cause)
    if (currentWriter != null) {
      currentWriter.close()
    }
    abortTask()
    throw new SparkException("Task failed while writing rows.", cause)
    ```
    
    If `markTaskFailed` or `currentWriter.close` throws an exception, we currently lose the original cause. This PR fixes this problem by implementing a utility function `Utils.tryWithSafeCatch` that suppresses (`Throwable.addSuppressed`) the exception that are thrown within the catch block and rethrowing the original exception.
    
    ## How was this patch tested?
    
    No new functionality added
    
    Author: Sameer Agarwal <sameer@databricks.com>
    
    Closes #12272 from sameeragarwal/fix-exception-1.6.
    
    (cherry picked from commit c12db0d3361e152698521ad077e7a16f2188a4b8)

commit 4f24fd1c1ba0dcdbfdb55fc93a0fa08280a96880
Author: Jason Moore <jasonmoore2k@outlook.com>
Date:   Sat Apr 9 23:34:57 2016 -0700

    [SPARK-14357][CORE] Properly handle the root cause being a commit denied exception
    
    ## What changes were proposed in this pull request?
    
    When deciding whether a CommitDeniedException caused a task to fail, consider the root cause of the Exception.
    
    ## How was this patch tested?
    
    Added a test suite for the component that extracts the root cause of the error.
    Made a distribution after cherry-picking this commit to branch-1.6 and used to run our Spark application that would quite often fail due to the CommitDeniedException.
    
    Author: Jason Moore <jasonmoore2k@outlook.com>
    
    Closes #12228 from jasonmoore2k/SPARK-14357.
    
    (cherry picked from commit 22014e6fb919a35c31d852b7c2f5b7eb05751208)
    Signed-off-by: Andrew Or <andrew@databricks.com>
    (cherry picked from commit 7a02c446fe89e79efb3bbdd4355044fad4e7d97b)

commit 642246dc6b8a024d6fcfa28f9a27c81490f2ba7d
Author: Andrew Or <andrew@databricks.com>
Date:   Thu Apr 7 17:49:39 2016 -0700

    [SPARK-14468] Always enable OutputCommitCoordinator
    
    ## What changes were proposed in this pull request?
    
    `OutputCommitCoordinator` was introduced to deal with concurrent task attempts racing to write output, leading to data loss or corruption. For more detail, read the [JIRA description](https://issues.apache.org/jira/browse/SPARK-14468).
    
    Before: `OutputCommitCoordinator` is enabled only if speculation is enabled.
    After: `OutputCommitCoordinator` is always enabled.
    
    Users may still disable this through `spark.hadoop.outputCommitCoordination.enabled`, but they really shouldn't...
    
    ## How was this patch tested?
    
    `OutputCommitCoordinator*Suite`
    
    Author: Andrew Or <andrew@databricks.com>
    
    Closes #12244 from andrewor14/always-occ.
    
    (cherry picked from commit 3e29e372ff518827bae9dcd26087946fde476843)
    Signed-off-by: Andrew Or <andrew@databricks.com>
    (cherry picked from commit 77ebae36779b39dd9f9feafafaeb36910c90b8cb)

commit 072a5404e6b96ae2380c1b249c820646a2574316
Author: Michael Gummelt <mgummelt@mesosphere.io>
Date:   Thu Apr 7 17:41:37 2016 -0700

    [DOCS][MINOR] Remove sentence about Mesos not supporting cluster mode.
    
    Docs change to remove the sentence about Mesos not supporting cluster mode.
    
    It was not.
    
    Author: Michael Gummelt <mgummelt@mesosphere.io>
    
    Closes #12249 from mgummelt/fix-mesos-cluster-docs.
    
    (cherry picked from commit 30e980ad8e6443dddd54f3c2d48b3904499545cf)
    Signed-off-by: Andrew Or <andrew@databricks.com>
    (cherry picked from commit 8a94a59f9e2efc10f1222eca1f3620336d6ebcb3)

commit 29ae58bfc180062f5afccece58c7953c0961af4a
Author: Yuhao Yang <hhbyyh@gmail.com>
Date:   Wed Apr 6 11:36:26 2016 -0700

    [SPARK-14322][MLLIB] Use treeAggregate instead of reduce in OnlineLDAOptimizer
    
    ## What changes were proposed in this pull request?
    jira: https://issues.apache.org/jira/browse/SPARK-14322
    
    OnlineLDAOptimizer uses RDD.reduce in two places where it could use treeAggregate. This can cause scalability issues. This should be an easy fix.
    This is also a bug since it modifies the first argument to reduce, so we should use aggregate or treeAggregate.
    See this line: https://github.com/apache/spark/blob/f12f11e578169b47e3f8b18b299948c0670ba585/mllib/src/main/scala/org/apache/spark/mllib/clustering/LDAOptimizer.scala#L452
    and a few lines below it.
    
    ## How was this patch tested?
    unit tests
    
    Author: Yuhao Yang <hhbyyh@gmail.com>
    
    Closes #12106 from hhbyyh/ldaTreeReduce.
    
    (cherry picked from commit 8cffcb60deb82d04a5c6e144ec9927f6f7addc8b)
    Signed-off-by: Joseph K. Bradley <joseph@databricks.com>
    (cherry picked from commit dca0d9a481afef1e85703ced7e8ad3955895f256)

commit 753728b3e35c86c66014eb285d3c86eff34c5bc6
Author: jeanlyn <jeanlyn92@gmail.com>
Date:   Tue Apr 5 12:32:03 2016 -0700

    [SPARK-14243][CORE][BACKPORT-1.6] update task metrics when removing blocks
    
    ## What changes were proposed in this pull request?
    
    This patch try to  update the `updatedBlockStatuses ` when removing blocks, making sure `BlockManager` correctly updates `updatedBlockStatuses`
    
    ## How was this patch tested?
    
    test("updated block statuses") in BlockManagerSuite.scala
    
    Author: jeanlyn <jeanlyn92@gmail.com>
    
    Closes #12150 from jeanlyn/updataBlock1.6.
    
    (cherry picked from commit cfe9f02ca81b2c47f8425b3a7c278149caba58d2)

commit 582f7a06a7b67322e89e96f39ac619997d2b224a
Author: Yong Tang <yong.tang.github@outlook.com>
Date:   Tue Apr 5 12:19:20 2016 +0900

    [SPARK-14368][PYSPARK] Support python.spark.worker.memory with upper-case unit.
    
    ## What changes were proposed in this pull request?
    
    This fix tries to address the issue in PySpark where `spark.python.worker.memory`
    could only be configured with a lower case unit (`k`, `m`, `g`, `t`). This fix
    allows the upper case unit (`K`, `M`, `G`, `T`) to be used as well. This is to
    conform to the JVM memory string as is specified in the documentation .
    
    ## How was this patch tested?
    
    This fix adds additional test to cover the changes.
    
    Author: Yong Tang <yong.tang.github@outlook.com>
    
    Closes #12163 from yongtang/SPARK-14368.
    
    (cherry picked from commit 7db56244fa3dba92246bad6694f31bbf68ea47ec)
    Signed-off-by: Kousuke Saruta <sarutak@oss.nttdata.co.jp>
    (cherry picked from commit 285cb9c66238d67ea8dc8c07358802b57a0d9f84)

commit 0ca816ed9841329a4f2f99cc9839b27128fa8492
Author: Jo Voordeckers <jo.voordeckers@gmail.com>
Date:   Mon Apr 4 13:29:48 2016 -0700

    [SPARK-11327][MESOS] Backport dispatcher does not respect all args f…
    
    Backport for https://github.com/apache/spark/pull/10370 andrewor14
    
    Author: Jo Voordeckers <jo.voordeckers@gmail.com>
    
    Closes #12101 from jayv/mesos_cluster_params_backport.
    
    (cherry picked from commit 91530b09e76c23c7358c2421e5f4708a84bce7c7)

commit 20b480c445153ba9d3b57c50c46fb7ed5987074b
Author: Kazuaki Ishizaki <ishizaki@jp.ibm.com>
Date:   Thu Mar 31 15:05:48 2016 -0700

    [SPARK-14138] [SQL] Fix generated SpecificColumnarIterator code can exceed JVM size limit for cached DataFrames
    
    This PR reduces Java byte code size of method in ```SpecificColumnarIterator``` by using two approaches:
    1. Generate and call ```getTYPEColumnAccessor()``` for each type, which is actually used, for instantiating accessors
    2. Group a lot of method calls (more than 4000) into a method
    
    Added a new unit test to ```InMemoryColumnarQuerySuite```
    
    Here is generate code
    
    ```java
    /* 033 */   private org.apache.spark.sql.execution.columnar.CachedBatch batch = null;
    /* 034 */
    /* 035 */   private org.apache.spark.sql.execution.columnar.IntColumnAccessor accessor;
    /* 036 */   private org.apache.spark.sql.execution.columnar.IntColumnAccessor accessor1;
    /* 037 */
    /* 038 */   public SpecificColumnarIterator() {
    /* 039 */     this.nativeOrder = ByteOrder.nativeOrder();
    /* 030 */     this.mutableRow = new MutableUnsafeRow(rowWriter);
    /* 041 */   }
    /* 042 */
    /* 043 */   public void initialize(Iterator input, DataType[] columnTypes, int[] columnIndexes,
    /* 044 */     boolean columnNullables[]) {
    /* 044 */     this.input = input;
    /* 046 */     this.columnTypes = columnTypes;
    /* 047 */     this.columnIndexes = columnIndexes;
    /* 048 */   }
    /* 049 */
    /* 050 */
    /* 051 */   private org.apache.spark.sql.execution.columnar.IntColumnAccessor getIntColumnAccessor(int idx) {
    /* 052 */     byte[] buffer = batch.buffers()[columnIndexes[idx]];
    /* 053 */     return new org.apache.spark.sql.execution.columnar.IntColumnAccessor(ByteBuffer.wrap(buffer).order(nativeOrder));
    /* 054 */   }
    /* 055 */
    /* 056 */
    /* 057 */
    /* 058 */
    /* 059 */
    /* 060 */
    /* 061 */   public boolean hasNext() {
    /* 062 */     if (currentRow < numRowsInBatch) {
    /* 063 */       return true;
    /* 064 */     }
    /* 065 */     if (!input.hasNext()) {
    /* 066 */       return false;
    /* 067 */     }
    /* 068 */
    /* 069 */     batch = (org.apache.spark.sql.execution.columnar.CachedBatch) input.next();
    /* 070 */     currentRow = 0;
    /* 071 */     numRowsInBatch = batch.numRows();
    /* 072 */     accessor = getIntColumnAccessor(0);
    /* 073 */     accessor1 = getIntColumnAccessor(1);
    /* 074 */
    /* 075 */     return hasNext();
    /* 076 */   }
    /* 077 */
    /* 078 */   public InternalRow next() {
    /* 079 */     currentRow += 1;
    /* 080 */     bufferHolder.reset();
    /* 081 */     rowWriter.zeroOutNullBytes();
    /* 082 */     accessor.extractTo(mutableRow, 0);
    /* 083 */     accessor1.extractTo(mutableRow, 1);
    /* 084 */     unsafeRow.setTotalSize(bufferHolder.totalSize());
    /* 085 */     return unsafeRow;
    /* 086 */   }
    ```
    
    (If this patch involves UI changes, please attach a screenshot; otherwise, remove this)
    
    Author: Kazuaki Ishizaki <ishizaki@jp.ibm.com>
    
    Closes #11984 from kiszk/SPARK-14138.
    
    (cherry picked from commit f12f11e578169b47e3f8b18b299948c0670ba585)

commit 270437a0ec1baab75a2d303ec997cb00c0cc2926
Author: Yuhao Yang <hhbyyh@gmail.com>
Date:   Wed Mar 30 15:58:19 2016 -0700

    [SPARK-11507][MLLIB] add compact in Matrices fromBreeze
    
    jira: https://issues.apache.org/jira/browse/SPARK-11507
    "In certain situations when adding two block matrices, I get an error regarding colPtr and the operation fails. External issue URL includes full error and code for reproducing the problem."
    
    root cause: colPtr.last does NOT always equal to values.length in breeze SCSMatrix, which fails the require in SparseMatrix.
    
    easy step to repro:
    ```
    val m1: BM[Double] = new CSCMatrix[Double] (Array (1.0, 1, 1), 3, 3, Array (0, 1, 2, 3), Array (0, 1, 2) )
    val m2: BM[Double] = new CSCMatrix[Double] (Array (1.0, 2, 2, 4), 3, 3, Array (0, 0, 2, 4), Array (1, 2, 1, 2) )
    val sum = m1 + m2
    Matrices.fromBreeze(sum)
    ```
    
    Solution: By checking the code in [CSCMatrix](https://github.com/scalanlp/breeze/blob/28000a7b901bc3cfbbbf5c0bce1d0a5dda8281b0/math/src/main/scala/breeze/linalg/CSCMatrix.scala), CSCMatrix in breeze can have extra zeros in the end of data array. Invoking compact will make sure it aligns with the require of SparseMatrix. This should add limited overhead as the actual compact operation is only performed when necessary.
    
    Author: Yuhao Yang <hhbyyh@gmail.com>
    
    Closes #9520 from hhbyyh/matricesFromBreeze.
    
    (cherry picked from commit ca458618d8ee659ffa9a081083cd475a440fa8ff)
    Signed-off-by: Joseph K. Bradley <joseph@databricks.com>
    
    Conflicts:
    	mllib/src/test/scala/org/apache/spark/mllib/linalg/MatricesSuite.scala
    
    (cherry picked from commit 3cc3d8578248f8bdeab007708c98f93639b44cf5)

commit 90a6ba5c9811fc3066174731e9633fa1669724a0
Author: Carson Wang <carson.wang@intel.com>
Date:   Tue Mar 29 11:07:58 2016 -0700

    [SPARK-14232][WEBUI] Fix event timeline display issue when an executor is removed with a multiple line reason.
    
    ## What changes were proposed in this pull request?
    The event timeline doesn't show on job page if an executor is removed with a multiple line reason. This PR replaces all new line characters in the reason string with spaces.
    
    ![timelineerror](https://cloud.githubusercontent.com/assets/9278199/14100211/5fd4cd30-f5be-11e5-9cea-f32651a4cd62.jpg)
    
    ## How was this patch tested?
    Verified on the Web UI.
    
    Author: Carson Wang <carson.wang@intel.com>
    
    Closes #12029 from carsonwang/eventTimeline.
    
    (cherry picked from commit 15c0b0006b3d04434b505210df541aeb28a51de8)
    Signed-off-by: Andrew Or <andrew@databricks.com>
    (cherry picked from commit 84ad2544f900e264a6a0bb32e317d11b77c75487)

commit 86108e0d3e253a78c6812fe9a1556ed1e3698ce5
Author: jeanlyn <jeanlyn92@gmail.com>
Date:   Tue Mar 29 10:51:00 2016 -0700

    [SPARK-13845][CORE][BACKPORT-1.6] Using onBlockUpdated to replace onTaskEnd avioding driver OOM
    
    We have a streaming job using `FlumePollInputStream` always driver OOM after few days, here is some driver heap dump before OOM
    ```
     num     #instances         #bytes  class name
    ----------------------------------------------
       1:      13845916      553836640  org.apache.spark.storage.BlockStatus
       2:      14020324      336487776  org.apache.spark.storage.StreamBlockId
       3:      13883881      333213144  scala.collection.mutable.DefaultEntry
       4:          8907       89043952  [Lscala.collection.mutable.HashEntry;
       5:         62360       65107352  [B
       6:        163368       24453904  [Ljava.lang.Object;
       7:        293651       20342664  [C
    ...
    ```
    `BlockStatus` and `StreamBlockId` keep on growing, and the driver OOM in the end.
    After investigated, i found the `executorIdToStorageStatus` in `StorageStatusListener` seems never remove the blocks from `StorageStatus`.
    In order to fix the issue, i try to use `onBlockUpdated` replace `onTaskEnd ` , so we can update the block informations(add blocks, drop the block from memory to disk and delete the blocks) in time.
    
    Existing unit tests and manual tests
    
    Author: jeanlyn <jeanlyn92@gmail.com>
    
    Closes #12028 from jeanlyn/fixoom1.6.
    
    (cherry picked from commit c2ce247ead836a3ae593a6e4f2a5758c34a35bb4)

commit 794ed0b3d8fc8709e51b377f9079d965b94cb6f0
Author: Dongjoon Hyun <dongjoon@apache.org>
Date:   Mon Mar 28 21:00:00 2016 -0700

    [SPARK-14219][GRAPHX] Fix `pickRandomVertex` not to fall into infinit…
    
    ## What changes were proposed in this pull request?
    
    Currently, `GraphOps.pickRandomVertex()` falls into infinite loops for graphs having only one vertex. This PR fixes it by modifying the following termination-checking condition.
    ```scala
    -      if (selectedVertices.count > 1) {
    +      if (selectedVertices.count > 0) {
    ```
    
    ## How was this patch tested?
    
    Pass the Jenkins tests (including new test case).
    
    Author: Dongjoon Hyun <dongjoon@apache.org>
    
    Closes #12021 from dongjoon-hyun/SPARK-14219-2.
    
    (cherry picked from commit a7579444d577f4f53ee2ce1470764126b175ba21)

commit dc8d0071926a821779c787511bb85a0fb57c0d29
Author: Chenliang Xu <chexu@groupon.com>
Date:   Mon Mar 28 08:33:37 2016 -0700

    [SPARK-14187][MLLIB] Fix incorrect use of binarySearch in SparseMatrix
    
    ## What changes were proposed in this pull request?
    
    Fix incorrect use of binarySearch in SparseMatrix
    
    ## How was this patch tested?
    
    Unit test added.
    
    Author: Chenliang Xu <chexu@groupon.com>
    
    Closes #11992 from luckyrandom/SPARK-14187.
    
    (cherry picked from commit c8388297c436691a236520d2396deaf556aedb0e)
    Signed-off-by: Xiangrui Meng <meng@databricks.com>
    (cherry picked from commit 546569ec119bae25b77bf4bc69febd088b08f327)

commit 92a7905ffd2e0d079afa487e51057c318617c4cf
Author: Reynold Xin <rxin@databricks.com>
Date:   Fri Mar 25 01:17:23 2016 -0700

    [SPARK-14149] Log exceptions in tryOrIOException
    
    ## What changes were proposed in this pull request?
    We ran into a problem today debugging some class loading problem during deserialization, and JVM was masking the underlying exception which made it very difficult to debug. We can however log the exceptions using try/catch ourselves in serialization/deserialization. The good thing is that all these methods are already using Utils.tryOrIOException, so we can just put the try catch and logging in a single place.
    
    ## How was this patch tested?
    A logging change with a manual test.
    
    Author: Reynold Xin <rxin@databricks.com>
    
    Closes #11951 from rxin/SPARK-14149.
    
    (cherry picked from commit 70a6f0bb57ca2248444157e2707fbcc3cb04e3bc)
    Signed-off-by: Reynold Xin <rxin@databricks.com>
    (cherry picked from commit fba84d177b6b20b39e9fa8a2dff6826a9d427f7f)

commit 065864ef1fdff8cd42c88e45669ae47c564abd65
Author: sethah <seth.hendrickson16@gmail.com>
Date:   Thu Mar 24 19:14:24 2016 -0700

    [SPARK-14107][PYSPARK][ML] Add seed as named argument to GBTs in pyspark
    
    ## What changes were proposed in this pull request?
    
    GBTs in pyspark previously had seed parameters, but they could not be passed as keyword arguments through the class constructor. This patch adds seed as a keyword argument and also sets default value.
    
    ## How was this patch tested?
    
    Doc tests were updated to pass a random seed through the GBTClassifier and GBTRegressor constructors.
    
    Author: sethah <seth.hendrickson16@gmail.com>
    
    Closes #11944 from sethah/SPARK-14107.
    
    (cherry picked from commit 585097716c1979ea538ef733cf33225ef7be06f5)
    Signed-off-by: Xiangrui Meng <meng@databricks.com>
    (cherry picked from commit 70b587841f5670a825d22ecdf9727bd7ceb684c3)

commit f75e65177e4644343cb79f0b948734cc0a028d01
Author: Sun Rui <rui.sun@intel.com>
Date:   Wed Mar 23 07:57:03 2016 -0700

    [SPARK-14074][SPARKR] Specify commit sha1 ID when using install_github to install intr package.
    
    ## What changes were proposed in this pull request?
    
    In dev/lint-r.R, `install_github` makes our builds depend on a unstable source. This may cause un-expected test failures and then build break. This PR adds a specified commit sha1 ID to `install_github` to get a stable source.
    
    ## How was this patch tested?
    dev/lint-r
    
    Author: Sun Rui <rui.sun@intel.com>
    
    Closes #11913 from sun-rui/SPARK-14074.
    
    (cherry picked from commit 7d1175011c976756efcd4e4e4f70a8fd6f287026)
    Signed-off-by: Xiangrui Meng <meng@databricks.com>
    (cherry picked from commit b75f970948d94d12505f0414e22e4688229b90af)

commit b653728c3fc196b377f4339c97cb235222f52d36
Author: jerryshao <sshao@hortonworks.com>
Date:   Wed Mar 23 09:14:29 2016 -0500

    [SPARK-13642][YARN][1.6-BACKPORT] Properly handle signal kill in ApplicationMaster
    
    ## What changes were proposed in this pull request?
    
    This patch is fixing the race condition in ApplicationMaster when receiving a signal. In the current implementation, if signal is received and with no any exception, this application will be finished with successful state in Yarn, and there's no another attempt. Actually the application is killed by signal in the runtime, so another attempt is expected.
    
    This patch adds a signal handler to handle the signal things, if signal is received, marking this application finished with failure, rather than success.
    
    ## How was this patch tested?
    
    This patch is tested with following situations:
    
    Application is finished normally.
    Application is finished by calling System.exit(n).
    Application is killed by yarn command.
    ApplicationMaster is killed by "SIGTERM" send by kill pid command.
    ApplicationMaster is killed by NM with "SIGTERM" in case of NM failure.
    
    Author: jerryshao <sshao@hortonworks.com>
    
    Closes #11690 from jerryshao/SPARK-13642-1.6-backport.
    
    (cherry picked from commit 5e9cefc8ccfaa0ef0bb0f2052f9aa755197b0184)

commit 14edbabd191b22704321fbb0b5e72d2ddbad7af2
Author: Davies Liu <davies@databricks.com>
Date:   Tue Mar 22 16:45:20 2016 -0700

    [SPARK-13806] [SQL] fix rounding mode of negative float/double
    
    Round() in database usually round the number up (away from zero), it's different than Math.round() in Java.
    
    For example:
    ```
    scala> java.lang.Math.round(-3.5)
    res3: Long = -3
    ```
    In Database, we should return -4.0 in this cases.
    
    This PR remove the buggy special case for scale=0.
    
    Add tests for negative values with tie.
    
    Author: Davies Liu <davies@databricks.com>
    
    Closes #11894 from davies/fix_round.
    
    (cherry picked from commit 4700adb98e4a37c2b0ef7123eca8a9a03bbdbe78)
    Signed-off-by: Davies Liu <davies.liu@gmail.com>
    
    Conflicts:
    	sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/mathExpressions.scala
    
    (cherry picked from commit 179f6e323583deff4461d88c9315be170ee6fffd)

commit 6bca9b885c31e84be4229a9e17c54bf0467e1757
Author: Sun Rui <rui.sun@intel.com>
Date:   Tue Mar 22 11:17:19 2016 -0700

    [SPARK-14006][SPARKR] Fix SparkR lint-r test errors in branch-1.6.
    
    ## What changes were proposed in this pull request?
    
    A backport of #11652 for branch 1.6. It fixes all newly captured SparkR lint-r errors after the lintr package is updated from github.
    
    ## How was this patch tested?
    dev/lint-r
    SparkR unit tests
    
    Author: Sun Rui <rui.sun@intel.com>
    
    Closes #11884 from sun-rui/SPARK-14006.
    
    (cherry picked from commit 3e21f7f7204b97d183fc75c2fb573449d576ff01)

commit 5a2441deb07f626048d45fe5555c6461dbcc1d74
Author: cenyuhai <cenyuhai@didichuxing.com>
Date:   Tue Mar 22 20:53:11 2016 +0800

    [SPARK-13772][SQL] Fix data type mismatch for decimal
    
    Fix data type mismatch for decimal, patch for branch-1.6.
    
    Author: cenyuhai <cenyuhai@didichuxing.com>
    
    Closes #11605 from cenyuhai/SPARK-13772.
    
    (cherry picked from commit 3243d46ab8c3f76c1a7ee2cc5588f08ec7b51cfe)

commit d287a4a3074daefa481fffac6a161920ab56c9aa
Author: zero323 <matthew.szymkiewicz@gmail.com>
Date:   Mon Mar 21 23:52:33 2016 -0700

    [SPARK-14058][PYTHON] Incorrect docstring in Window.order
    
    ## What changes were proposed in this pull request?
    
    Replaces current docstring ("Creates a :class:`WindowSpec` with the partitioning defined.") with "Creates a :class:`WindowSpec` with the ordering defined."
    
    ## How was this patch tested?
    
    PySpark unit tests (no regression introduced). No changes to the code.
    
    Author: zero323 <matthew.szymkiewicz@gmail.com>
    
    Closes #11877 from zero323/order-by-description.
    
    (cherry picked from commit 8193a266b50460078f64f59c63eae53cdbceeace)
    Signed-off-by: Reynold Xin <rxin@databricks.com>
    (cherry picked from commit f9221ad79b18a94b0525c0e4cf993d9e9f0e8791)

commit 5489f7b46348e6247764614232ba6eb3779540b3
Author: Sital Kedia <skedia@fb.com>
Date:   Fri Mar 18 12:56:06 2016 -0700

    [SPARK-13958] Executor OOM due to unbounded growth of pointer array in…
    
    ## What changes were proposed in this pull request?
    
    This change fixes the executor OOM which was recently introduced in PR apache/spark#11095
    (Please fill in changes proposed in this fix)
    
    ## How was this patch tested?
    Tested by running a spark job on the cluster.
    (Please explain how this patch was tested. E.g. unit tests, integration tests, manual tests)
    
    (If this patch involves UI changes, please attach a screenshot; otherwise, remove this)
    
    … Sorter
    
    Author: Sital Kedia <skedia@fb.com>
    
    Closes #11794 from sitalkedia/SPARK-13958.
    
    (cherry picked from commit 2e0c5284fd88ba89f53f93dcf1eb26bca2be49c5)
    Signed-off-by: Davies Liu <davies.liu@gmail.com>
    (cherry picked from commit 022e06d18471bf54954846c815c8a3666aef9fc3)

commit 757c97eb18a3fe039b1e3aa3f09decc6bcd5796e
Author: trueyao <501663994@qq.com>
Date:   Thu Mar 17 09:45:06 2016 +0000

    [SPARK-13901][CORE] correct the logDebug information when jump to the next locality level
    
    JIRA Issue:https://issues.apache.org/jira/browse/SPARK-13901
    In getAllowedLocalityLevel method of TaskSetManager,we get wrong logDebug information when jump to the next locality level.So we should fix it.
    
    Author: trueyao <501663994@qq.com>
    
    Closes #11719 from trueyao/logDebug-localityWait.
    
    (cherry picked from commit ea9ca6f04ce1fb83612f907d6d0996d6fb362bd2)
    Signed-off-by: Sean Owen <sowen@cloudera.com>
    (cherry picked from commit 1fcd17f84ac497caaf1540f6fa5f32f23b93e657)

commit 88d4d7ff3bec6dbb2e6f0c0f431c9810340d6671
Author: CodingCat <zhunansjtu@gmail.com>
Date:   Tue Mar 15 10:10:23 2016 +0000

    [SPARK-13803] restore the changes in SPARK-3411
    
    ## What changes were proposed in this pull request?
    
    This patch contains the functionality to balance the load of the cluster-mode drivers among workers
    
    This patch restores the changes in https://github.com/apache/spark/pull/1106 which was erased due to the merging of https://github.com/apache/spark/pull/731
    
    ## How was this patch tested?
    
    test with existing test cases
    
    Author: CodingCat <zhunansjtu@gmail.com>
    
    Closes #11702 from CodingCat/SPARK-13803.
    
    (cherry picked from commit bd5365bbe9ff6518cde9402ee8843ec1002fff5b)
    Signed-off-by: Sean Owen <sowen@cloudera.com>
    (cherry picked from commit 765307f415301bcf660b7d2d75d50f0e6aa32ddd)

commit b3cd0aaedfcf39d042a0b4f24382dc85321d93b9
Author: Yin Huai <yhuai@databricks.com>
Date:   Mon Mar 14 23:42:05 2016 -0700

    [SPARK-13207][SQL][BRANCH-1.6] Make partitioning discovery ignore _SUCCESS files.
    
    If a _SUCCESS appears in the inner partitioning dir, partition discovery will treat that _SUCCESS file as a data file. Then, partition discovery will fail because it finds that the dir structure is not valid. We should ignore those `_SUCCESS` files.
    
    In future, it is better to ignore all files/dirs starting with `_` or `.`. This PR does not make this change. I am thinking about making this change simple, so we can consider of getting it in branch 1.6.
    
    To ignore all files/dirs starting with `_` or `, the main change is to let ParquetRelation have another way to get metadata files. Right now, it relies on FileStatusCache's cachedLeafStatuses, which returns file statuses of both metadata files (e.g. metadata files used by parquet) and data files, which requires more changes.
    
    https://issues.apache.org/jira/browse/SPARK-13207
    
    Author: Yin Huai <yhuai@databricks.com>
    
    Closes #11697 from yhuai/SPARK13207_branch16.
    
    (cherry picked from commit 6935b50801cb10e9e5b7583931cdc972e8981793)

commit a31fb296b122add642c34de31053a9fd4f48119b
Author: Bjorn Jonsson <bjornjon@gmail.com>
Date:   Mon Mar 14 12:27:49 2016 -0700

    [MINOR][COMMON] Fix copy-paste oversight in variable naming
    
    ## What changes were proposed in this pull request?
    
    JavaUtils.java has methods to convert time and byte strings for internal use, this change renames a variable used in byteStringAs(), from timeError to byteError.
    
    Author: Bjorn Jonsson <bjornjon@gmail.com>
    
    Closes #11695 from bjornjon/master.
    
    (cherry picked from commit e06493cb7b790623a9106241a8d496ecea703328)
    Signed-off-by: Andrew Or <andrew@databricks.com>
    (cherry picked from commit 589d0420a6e9fb42f18e80695be22ff73cd14c6a)

commit 02c3eb17c630f67dc0abeed77788c9a408401159
Author: Daniel Santana <mestresan@gmail.com>
Date:   Mon Mar 14 12:26:08 2016 -0700

    [MINOR][DOCS] Added Missing back slashes
    
    ## What changes were proposed in this pull request?
    
    When studying spark many users just copy examples on the documentation and paste on their terminals
    and because of that the missing backlashes lead them run into some shell errors.
    
    The added backslashes avoid that problem for spark users with that behavior.
    
    ## How was this patch tested?
    
    I generated the documentation locally using jekyll and checked the generated pages
    
    Author: Daniel Santana <mestresan@gmail.com>
    
    Closes #11699 from danielsan/master.
    
    (cherry picked from commit 9f13f0fc1724e407d3be54152b82150a16395421)
    Signed-off-by: Andrew Or <andrew@databricks.com>
    (cherry picked from commit 13bd122644d2514d76c8899b9585298125cfd6f3)

commit 1ac4e97d149f281ccdff946e128ffab3f8e33106
Author: Jacky Li <jacky.likun@huawei.com>
Date:   Sun Mar 13 18:44:02 2016 -0700

    [SQL] fix typo in DataSourceRegister
    
    ## What changes were proposed in this pull request?
    fix typo in DataSourceRegister
    
    ## How was this patch tested?
    
    found when going through latest code
    
    Author: Jacky Li <jacky.likun@huawei.com>
    
    Closes #11686 from jackylk/patch-12.
    
    (cherry picked from commit f3daa099bf60edbd6ebf997c00e46db1e09f6dda)
    Signed-off-by: Reynold Xin <rxin@databricks.com>
    (cherry picked from commit 3519ce9f16b4f053f46df37ab9eb07a27a859c39)

commit 4b7c51d3c57bd0ed5d6fe4663b4284a7de7ba32e
Author: Oscar D. Lara Yejas <odlaraye@oscars-mbp.attlocal.net>
Date:   Thu Mar 10 17:10:23 2016 -0800

    [SPARK-13327][SPARKR] Added parameter validations for colnames<-
    
    Author: Oscar D. Lara Yejas <odlaraye@oscars-mbp.attlocal.net>
    Author: Oscar D. Lara Yejas <odlaraye@oscars-mbp.usca.ibm.com>
    
    Closes #11220 from olarayej/SPARK-13312-3.
    
    (cherry picked from commit 416e71af4d26e67afb715ea1d625341cdea4873d)
    Signed-off-by: Shivaram Venkataraman <shivaram@cs.berkeley.edu>
    (cherry picked from commit db4795a7eb1bac039e9e96237cf77e47ed76dde8)

commit a8df053a0a40a07b3234a02ddab2c28bde698523
Author: Dongjoon Hyun <dongjoon@apache.org>
Date:   Thu Mar 10 17:07:18 2016 -0800

    [MINOR][DOC] Fix supported hive version in doc
    
    ## What changes were proposed in this pull request?
    
    Today, Spark 1.6.1 and updated docs are release. Unfortunately, there is obsolete hive version information on docs: [Building Spark](http://spark.apache.org/docs/latest/building-spark.html#building-with-hive-and-jdbc-support). This PR fixes the following two lines.
    ```
    -By default Spark will build with Hive 0.13.1 bindings.
    +By default Spark will build with Hive 1.2.1 bindings.
    -# Apache Hadoop 2.4.X with Hive 13 support
    +# Apache Hadoop 2.4.X with Hive 1.2.1 support
    ```
    `sql/README.md` file also describe
    
    ## How was this patch tested?
    
    Manual.
    
    (If this patch involves UI changes, please attach a screenshot; otherwise, remove this)
    
    Author: Dongjoon Hyun <dongjoon@apache.org>
    
    Closes #11639 from dongjoon-hyun/fix_doc_hive_version.
    
    (cherry picked from commit 88fa866620b85522bae2f71601cebb744c09ba7e)
    Signed-off-by: Reynold Xin <rxin@databricks.com>
    (cherry picked from commit 078c714666a750b9a3530f380d9b2ffd881f3a46)

commit af427be2093ce136a83b0ad856f0d5d4604f6377
Author: Yin Huai <yhuai@databricks.com>
Date:   Wed Mar 9 18:41:38 2016 -0800

    Revert "[SPARK-13760][SQL] Fix BigDecimal constructor for FloatType"
    
    This reverts commit 926e9c45a21c5b71ef0832d63b8dae7d4f3d8826.
    
    (cherry picked from commit 60cb27040c3cae531f71985f84f4c0321aa91c94)

commit f4118e966f9abe303f3771a6af72e535ee89f37d
Author: Sameer Agarwal <sameer@databricks.com>
Date:   Wed Mar 9 18:16:29 2016 -0800

    [SPARK-13760][SQL] Fix BigDecimal constructor for FloatType
    
    ## What changes were proposed in this pull request?
    
    A very minor change for using `BigDecimal.decimal(f: Float)` instead of `BigDecimal(f: float)`. The latter is deprecated and can result in inconsistencies due to an implicit conversion to `Double`.
    
    ## How was this patch tested?
    
    N/A
    
    cc yhuai
    
    Author: Sameer Agarwal <sameer@databricks.com>
    
    Closes #11597 from sameeragarwal/bigdecimal.
    
    (cherry picked from commit 926e9c45a21c5b71ef0832d63b8dae7d4f3d8826)
    Signed-off-by: Yin Huai <yhuai@databricks.com>
    (cherry picked from commit 8a1bd5834cec61a882c54fdf56385c76b221cb4f)

commit f2018a5244c55abc1d173f14b8112f1e8a7af919
Author: Davies Liu <davies@databricks.com>
Date:   Wed Mar 9 12:05:34 2016 -0800

    [SPARK-13242] [SQL] codegen fallback in case-when if there many branches
    
    ## What changes were proposed in this pull request?
    
    If there are many branches in a CaseWhen expression, the generated code could go above the 64K limit for single java method, will fail to compile. This PR change it to fallback to interpret mode if there are more than 20 branches.
    
    ## How was this patch tested?
    
    Add tests
    
    Author: Davies Liu <davies@databricks.com>
    
    Closes #11606 from davies/fix_when_16.
    
    (cherry picked from commit bea91a9e94341b4cab1977911e91d56016c55cb3)

commit 9dce20c01a4e7d7e267fddb5ee595230b4d92eaa
Author: Andy Sloane <asloane@tetrationanalytics.com>
Date:   Wed Mar 9 10:25:47 2016 +0000

    [SPARK-13631][CORE] Thread-safe getLocationsWithLargestOutputs
    
    ## What changes were proposed in this pull request?
    
    If a job is being scheduled in one thread which has a dependency on an
    RDD currently executing a shuffle in another thread, Spark would throw a
    NullPointerException. This patch synchronizes access to `mapStatuses` and
    skips null status entries (which are in-progress shuffle tasks).
    
    ## How was this patch tested?
    
    Our client code unit test suite, which was reliably reproducing the race
    condition with 10 threads, shows that this fixes it. I have not found a minimal
    test case to add to Spark, but I will attempt to do so if desired.
    
    The same test case was tripping up on SPARK-4454, which was fixed by
    making other DAGScheduler code thread-safe.
    
    shivaram srowen
    
    Author: Andy Sloane <asloane@tetrationanalytics.com>
    
    Closes #11505 from a1k0n/SPARK-13631.
    
    (cherry picked from commit cbff2803ef117d7cffe6f05fc1bbd395a1e9c587)
    Signed-off-by: Sean Owen <sowen@cloudera.com>
    (cherry picked from commit 95105b0e6e38f5f13f41b06695c0b059ff911a44)

commit 09678d6e16c11f08bc99308d54c29c706ce60002
Author: Josh Rosen <joshrosen@databricks.com>
Date:   Tue Mar 8 16:28:22 2016 -0800

    [SPARK-13755] Escape quotes in SQL plan visualization node labels
    
    When generating Graphviz DOT files in the SQL query visualization we need to escape double-quotes inside node labels. This is a followup to #11309, which fixed a similar graph in Spark Core's DAG visualization.
    
    Author: Josh Rosen <joshrosen@databricks.com>
    
    Closes #11587 from JoshRosen/graphviz-escaping.
    
    (cherry picked from commit 81f54acc9cc0fb9d4ee552f6f56a26c78654a33b)
    Signed-off-by: Josh Rosen <joshrosen@databricks.com>
    (cherry picked from commit 8ec4f159ae6bbf836403ae3c72de12757ab96146)

commit da58f495806ccb60f15a1d0d947627037cafc3fa
Author: Shixiong Zhu <shixiong@databricks.com>
Date:   Mon Mar 7 20:56:08 2016 -0800

    [SPARK-13711][CORE] Don't call SparkUncaughtExceptionHandler in AppClient as it's in driver
    
    ## What changes were proposed in this pull request?
    
    AppClient runs in the driver side. It should not call `Utils.tryOrExit` as it will send exception to SparkUncaughtExceptionHandler and call `System.exit`. This PR just removed `Utils.tryOrExit`.
    
    ## How was this patch tested?
    
    manual tests.
    
    Author: Shixiong Zhu <shixiong@databricks.com>
    
    Closes #11566 from zsxwing/SPARK-13711.
    
    (cherry picked from commit bace137f28cc08eb78e964fd1cfb2fd2c73688fa)

commit 872d909a6441939f2f6d6d0163ddb7af21e97d31
Author: Tim Preece <tim.preece.in.oz@gmail.com>
Date:   Mon Mar 7 15:23:07 2016 -0800

    [SPARK-13648] Add Hive Cli to classes for isolated classloader
    
    ## What changes were proposed in this pull request?
    
    Adding the hive-cli classes to the classloader
    
    ## How was this patch tested?
    
    The hive Versionssuite tests were run
    
    This is my original work and I license the work to the project under the project's open source license.
    
    Author: Tim Preece <tim.preece.in.oz@gmail.com>
    
    Closes #11495 from preecet/master.
    
    (cherry picked from commit 46f25c241385fe4f1fff42ac4a1b6e652deb2d02)
    Signed-off-by: Michael Armbrust <michael@databricks.com>
    (cherry picked from commit 695c8a2573cd7ecf0dc9b0b8e408afb25bd88302)

commit 1c492ffb828e9062887c5d1e3379c0fce0849075
Author: CodingCat <zhunansjtu@gmail.com>
Date:   Mon Mar 7 12:07:50 2016 -0800

    [MINOR][DOC] improve the doc for "spark.memory.offHeap.size"
    
    The description of "spark.memory.offHeap.size" in the current document does not clearly state that memory is counted with bytes....
    
    This PR contains a small fix for this tiny issue
    
    document fix
    
    Author: CodingCat <zhunansjtu@gmail.com>
    
    Closes #11561 from CodingCat/master.
    
    (cherry picked from commit a3ec50a4bc867aec7c0796457c4442c14d1bcc2c)
    Signed-off-by: Shixiong Zhu <shixiong@databricks.com>
    (cherry picked from commit cf4e62ec276ab96afa4f0b81d216e0dad2231b9f)

commit 41b780ae497232ddad88d5265cbd5addc2b34e50
Author: Steve Loughran <stevel@hortonworks.com>
Date:   Mon Mar 7 13:05:07 2016 +0000

    [SPARK-13599][BUILD] remove transitive groovy dependencies from spark-hive and spark-hiveserver (branch 1.6)
    
    ## What changes were proposed in this pull request?
    
    This is just the patch of #11449 cherry picked to branch-1.6; the enforcer and dep/ diffs are cut
    
    Modifies the dependency declarations of the all the hive artifacts, to explicitly exclude the groovy-all JAR.
    
    This stops the groovy classes *and everything else in that uber-JAR* from getting into spark-assembly JAR.
    
    ## How was this patch tested?
    
    1. Pre-patch build was made: `mvn clean install -Pyarn,hive,hive-thriftserver`
    1. spark-assembly expanded, observed to have the org.codehaus.groovy packages and JARs
    1. A maven dependency tree was created `mvn dependency:tree -Pyarn,hive,hive-thriftserver  -Dverbose > target/dependencies.txt`
    1. This text file examined to confirm that groovy was being imported as a dependency of `org.spark-project.hive`
    1. Patch applied
    1. Repeated step1: clean build of project with ` -Pyarn,hive,hive-thriftserver` set
    1. Examined created spark-assembly, verified no org.codehaus packages
    1. Verified that the maven dependency tree no longer references groovy
    
    The `master` version updates the dependency files and an enforcer rule to keep groovy out; this patch strips it out.
    
    Author: Steve Loughran <stevel@hortonworks.com>
    
    Closes #11473 from steveloughran/fixes/SPARK-13599-groovy+branch-1.6.
    
    (cherry picked from commit 2434f16cc743d10bb03eadc051dfd568ebe7a79e)

commit 5748d74bb5b95875f72641ada3b3dac7a8300203
Author: rmishra <rmishra@pivotal.io>
Date:   Mon Mar 7 09:55:49 2016 +0000

    [SPARK-13705][DOCS] UpdateStateByKey Operation documentation incorrectly refers to StatefulNetworkWordCount
    
    ## What changes were proposed in this pull request?
    The reference to StatefulNetworkWordCount.scala from updateStatesByKey documentation should be removed, till there is a example for updateStatesByKey.
    
    ## How was this patch tested?
    Have tested the new documentation with jekyll build.
    
    Author: rmishra <rmishra@pivotal.io>
    
    Closes #11545 from rishitesh/SPARK-13705.
    
    (cherry picked from commit 4b13896ebf7cecf9d50514a62165b612ee18124a)
    Signed-off-by: Sean Owen <sowen@cloudera.com>
    (cherry picked from commit 18ef2f25e4463c2679feaf3bcb7155f340bbc059)

commit f7f67ff8d1d89ee6b81b31ec4a53b79a702e495f
Author: Shixiong Zhu <shixiong@databricks.com>
Date:   Sun Mar 6 08:57:01 2016 -0800

    [SPARK-13697] [PYSPARK] Fix the missing module name of TransformFunctionSerializer.loads
    
    ## What changes were proposed in this pull request?
    
    Set the function's module name to `__main__` if it's missing in `TransformFunctionSerializer.loads`.
    
    ## How was this patch tested?
    
    Manually test in the shell.
    
    Before this patch:
    ```
    >>> from pyspark.streaming import StreamingContext
    >>> from pyspark.streaming.util import TransformFunction
    >>> ssc = StreamingContext(sc, 1)
    >>> func = TransformFunction(sc, lambda x: x, sc.serializer)
    >>> func.rdd_wrapper(lambda x: x)
    TransformFunction(<function <lambda> at 0x106ac8b18>)
    >>> bytes = bytearray(ssc._transformerSerializer.serializer.dumps((func.func, func.rdd_wrap_func, func.deserializers)))
    >>> func2 = ssc._transformerSerializer.loads(bytes)
    >>> print(func2.func.__module__)
    None
    >>> print(func2.rdd_wrap_func.__module__)
    None
    >>>
    ```
    After this patch:
    ```
    >>> from pyspark.streaming import StreamingContext
    >>> from pyspark.streaming.util import TransformFunction
    >>> ssc = StreamingContext(sc, 1)
    >>> func = TransformFunction(sc, lambda x: x, sc.serializer)
    >>> func.rdd_wrapper(lambda x: x)
    TransformFunction(<function <lambda> at 0x108bf1b90>)
    >>> bytes = bytearray(ssc._transformerSerializer.serializer.dumps((func.func, func.rdd_wrap_func, func.deserializers)))
    >>> func2 = ssc._transformerSerializer.loads(bytes)
    >>> print(func2.func.__module__)
    __main__
    >>> print(func2.rdd_wrap_func.__module__)
    __main__
    >>>
    ```
    
    Author: Shixiong Zhu <shixiong@databricks.com>
    
    Closes #11535 from zsxwing/loads-module.
    
    (cherry picked from commit ee913e6e2d58dfac20f3f06ff306081bd0e48066)
    Signed-off-by: Davies Liu <davies.liu@gmail.com>
    (cherry picked from commit 704a54c5528ab917b15adb3b325ec95f78d97fed)

commit 7ad0e97bd3f6db9dfcfeb7b62b8efb845c4b1cae
Author: Davies Liu <davies.liu@gmail.com>
Date:   Fri Mar 4 13:21:35 2016 -0800

    [HOTFIX] fix the conflict when cherry-pick
    
    (cherry picked from commit ffaf7c080b46243bc0ee3726d84e599f8c7730fa)

commit 371c654961d1b6b9aac80d9215c9e12066f53f3a
Author: Oliver Pierson <ocp@gatech.edu>
Date:   Thu Feb 25 13:24:46 2016 +0000

    [SPARK-13444][MLLIB] QuantileDiscretizer chooses bad splits on large DataFrames
    
    ## What changes were proposed in this pull request?
    
    Change line 113 of QuantileDiscretizer.scala to
    
    `val requiredSamples = math.max(numBins * numBins, 10000.0)`
    
    so that `requiredSamples` is a `Double`.  This will fix the division in line 114 which currently results in zero if `requiredSamples < dataset.count`
    
    ## How was the this patch tested?
    Manual tests.  I was having a problems using QuantileDiscretizer with my a dataset and after making this change QuantileDiscretizer behaves as expected.
    
    Author: Oliver Pierson <ocp@gatech.edu>
    Author: Oliver Pierson <opierson@umd.edu>
    
    Closes #11319 from oliverpierson/SPARK-13444.
    
    (cherry picked from commit f0cc511ecaff8b5b36bdcce2d014ebbd8b8a37e5)

commit ce85c9c107b4bdefdc6f8a35cf991120e0edb690
Author: Yu ISHIKAWA <yuu.ishikawa@gmail.com>
Date:   Thu Feb 11 15:05:34 2016 -0800

    [SPARK-11515][ML] QuantileDiscretizer should take random seed
    
    cc jkbradley
    
    Author: Yu ISHIKAWA <yuu.ishikawa@gmail.com>
    
    Closes #9535 from yu-iskw/SPARK-11515.
    
    (cherry picked from commit 574571c87098795a2206a113ee9ed4bafba8f00f)
    Signed-off-by: Sean Owen <sowen@cloudera.com>
    (cherry picked from commit 5a27129528f4e42eebb897d81fb1de26296933bf)

commit 1e97f49c6ece22ec1a0fb18c59236f6c2eb2381b
Author: Davies Liu <davies@databricks.com>
Date:   Thu Mar 3 17:46:28 2016 -0800

    [SPARK-13601] [TESTS] use 1 partition in tests to avoid race conditions
    
    Fix race conditions when cleanup files.
    
    Existing tests.
    
    Author: Davies Liu <davies@databricks.com>
    
    Closes #11507 from davies/flaky.
    
    (cherry picked from commit d062587dd2c4ed13998ee8bcc9d08f29734df228)
    Signed-off-by: Davies Liu <davies.liu@gmail.com>
    
    Conflicts:
    	sql/hive/src/test/scala/org/apache/spark/sql/sources/CommitFailureTestRelationSuite.scala
    
    (cherry picked from commit b3a512965d9c27a7078448a6e8eae9ecfcaf1553)

commit 868ecfb5314141e9dc020abf79fdc78e5d185d3e
Author: Davies Liu <davies.liu@gmail.com>
Date:   Thu Mar 3 10:08:53 2016 -0800

    [SPARK-13601] call failure callbacks before writer.close()
    
    In order to tell OutputStream that the task has failed or not, we should call the failure callbacks BEFORE calling writer.close().
    
    Added new unit tests.
    
    Author: Davies Liu <davies@databricks.com>
    
    Closes #11450 from davies/callback.
    
    (cherry picked from commit fa86dc47a1527e8a5893edeec98ecddaec3c7776)

commit 0479035362ea9a997a3f917d4671d33a0776803a
Author: Davies Liu <davies@databricks.com>
Date:   Thu Mar 3 08:43:38 2016 -0800

    [SPARK-13465] Add a task failure listener to TaskContext
    
    ## What changes were proposed in this pull request?
    
    TaskContext supports task completion callback, which gets called regardless of task failures. However, there is no way for the listener to know if there is an error. This patch adds a new listener that gets called when a task fails.
    
    ## How was this patch tested?
    
    New unit test case and integration test case covering the code path
    
    Author: Davies Liu <davies@databricks.com>
    
    Closes #11478 from davies/add_failure_1.6.
    
    (cherry picked from commit 1ce2c123565630914cdcafe6549582843047d874)

commit fa946b46b3e7defcd0b2b35d642bed9d7283302e
Author: Josh Rosen <joshrosen@databricks.com>
Date:   Fri Feb 26 20:05:44 2016 -0800

    Update CHANGES.txt and spark-ec2 and R package versions for 1.6.1
    
    This patch updates a few more 1.6.0 version numbers for the 1.6.1 release candidate.
    
    Verified this by running
    
    ```
    git grep "1\.6\.0" | grep -v since | grep -v deprecated | grep -v Since | grep -v versionadded | grep 1.6.0
    ```
    
    and inspecting the output.
    
    Author: Josh Rosen <joshrosen@databricks.com>
    
    Closes #11407 from JoshRosen/version-number-updates.
    
    (cherry picked from commit eb6f6da484b4390c5b196d8426a49609b6a6fc7c)

commit 56651850f042bceeb481c41de60478cd2766f722
Author: Josh Rosen <joshrosen@databricks.com>
Date:   Fri Feb 26 18:40:00 2016 -0800

    [SPARK-13474][PROJECT INFRA] Update packaging scripts to push artifacts to home.apache.org
    
    Due to the people.apache.org -> home.apache.org migration, we need to update our packaging scripts to publish artifacts to the new server. Because the new server only supports sftp instead of ssh, we need to update the scripts to use lftp instead of ssh + rsync.
    
    Author: Josh Rosen <joshrosen@databricks.com>
    
    Closes #11350 from JoshRosen/update-release-scripts-for-apache-home.
    
    (cherry picked from commit f77dc4e1e202942aa8393fb5d8f492863973fe17)
    Signed-off-by: Josh Rosen <joshrosen@databricks.com>
    (cherry picked from commit 8a43c3bfbcd9d6e3876e09363dba604dc7e63dc3)

commit 375ea79acec8aaf969a91104b05d22b8a63e453c
Author: Yin Huai <yhuai@databricks.com>
Date:   Fri Feb 26 12:34:03 2016 -0800

    [SPARK-13454][SQL] Allow users to drop a table with a name starting with an underscore.
    
    ## What changes were proposed in this pull request?
    
    This change adds a workaround to allow users to drop a table with a name starting with an underscore. Without this patch, we can create such a table, but we cannot drop it. The reason is that Hive's parser unquote an quoted identifier (see https://github.com/apache/hive/blob/release-1.2.1/ql/src/java/org/apache/hadoop/hive/ql/parse/HiveLexer.g#L453). So, when we issue a drop table command to Hive, a table name starting with an underscore is actually not quoted. Then, Hive will complain about it because it does not support a table name starting with an underscore without using backticks (underscores are allowed as long as it is not the first char though).
    
    ## How was this patch tested?
    
    Add a test to make sure we can drop a table with a name starting with an underscore.
    
    https://issues.apache.org/jira/browse/SPARK-13454
    
    Author: Yin Huai <yhuai@databricks.com>
    
    Closes #11349 from yhuai/fixDropTable.
    
    (cherry picked from commit a57f87ee4aafdb97c15f4076e20034ea34c7e2e5)

commit d743ae9bbccc621842da800cb59623a52d4fc3cf
Author: Yu ISHIKAWA <yuu.ishikawa@gmail.com>
Date:   Thu Feb 25 13:21:33 2016 -0800

    [SPARK-12874][ML] ML StringIndexer does not protect itself from column name duplication
    
    ## What changes were proposed in this pull request?
    ML StringIndexer does not protect itself from column name duplication.
    
    We should still improve a way to validate a schema of `StringIndexer` and `StringIndexerModel`.  However, it would be great to fix at another issue.
    
    ## How was this patch tested?
    unit test
    
    Author: Yu ISHIKAWA <yuu.ishikawa@gmail.com>
    
    Closes #11370 from yu-iskw/SPARK-12874.
    
    (cherry picked from commit 14e2700de29d06460179a94cc9816bcd37344cf7)
    Signed-off-by: Xiangrui Meng <meng@databricks.com>
    (cherry picked from commit abe8f991a32bef92fbbcd2911836bb7d8e61ca97)

commit 4d485296286e749d66112a6ccb4bbecb85fad42e
Author: Xiangrui Meng <meng@databricks.com>
Date:   Thu Feb 25 12:28:03 2016 -0800

    Revert "[SPARK-13444][MLLIB] QuantileDiscretizer chooses bad splits on large DataFrames"
    
    This reverts commit cb869a143d338985c3d99ef388dd78b1e3d90a73.
    
    (cherry picked from commit d59a08f7c1c455d86e7ee3d6522a3e9c55f9ee02)

commit 3b9c19bbd3fd4264d4261a1cc8d746f6193c9f23
Author: huangzhaowei <carlmartinmax@gmail.com>
Date:   Thu Feb 25 09:14:19 2016 -0600

    [SPARK-12316] Wait a minutes to avoid cycle calling.
    
    When application end, AM will clean the staging dir.
    But if the driver trigger to update the delegation token, it will can't find the right token file and then it will endless cycle call the method 'updateCredentialsIfRequired'.
    Then it lead driver StackOverflowError.
    https://issues.apache.org/jira/browse/SPARK-12316
    
    Author: huangzhaowei <carlmartinmax@gmail.com>
    
    Closes #10475 from SaintBacchus/SPARK-12316.
    
    (cherry picked from commit 5fcf4c2bfce4b7e3543815c8e49ffdec8072c9a2)
    Signed-off-by: Tom Graves <tgraves@yahoo-inc.com>
    (cherry picked from commit 5f7440b2529a0f6edfed5038756c004acecbce39)

commit 7129859253ff29fdd03c5dbe0ad9703a3ad55e36
Author: Michael Gummelt <mgummelt@mesosphere.io>
Date:   Thu Feb 25 13:32:09 2016 +0000

    [SPARK-13439][MESOS] Document that spark.mesos.uris is comma-separated
    
    Author: Michael Gummelt <mgummelt@mesosphere.io>
    
    Closes #11311 from mgummelt/document_csv.
    
    (cherry picked from commit c98a93ded36db5da2f3ebd519aa391de90927688)
    Signed-off-by: Sean Owen <sowen@cloudera.com>
    (cherry picked from commit e3802a7522a83b91c84d0ee6f721a768a485774b)

commit e12021232e9e1b45a32b8f7b059092645beadf68
Author: Oliver Pierson <ocp@gatech.edu>
Date:   Thu Feb 25 13:24:46 2016 +0000

    [SPARK-13444][MLLIB] QuantileDiscretizer chooses bad splits on large DataFrames
    
    Change line 113 of QuantileDiscretizer.scala to
    
    `val requiredSamples = math.max(numBins * numBins, 10000.0)`
    
    so that `requiredSamples` is a `Double`.  This will fix the division in line 114 which currently results in zero if `requiredSamples < dataset.count`
    
    Manual tests.  I was having a problems using QuantileDiscretizer with my a dataset and after making this change QuantileDiscretizer behaves as expected.
    
    Author: Oliver Pierson <ocp@gatech.edu>
    Author: Oliver Pierson <opierson@umd.edu>
    
    Closes #11319 from oliverpierson/SPARK-13444.
    
    (cherry picked from commit 6f8e835c68dff6fcf97326dc617132a41ff9d043)
    Signed-off-by: Sean Owen <sowen@cloudera.com>
    (cherry picked from commit cb869a143d338985c3d99ef388dd78b1e3d90a73)

commit d391cd3c0800e657d08e8bca7fffc553e071d7cf
Author: Cheng Lian <lian@databricks.com>
Date:   Thu Feb 25 20:43:03 2016 +0800

    [SPARK-13473][SQL] Don't push predicate through project with nondeterministic field(s)
    
    ## What changes were proposed in this pull request?
    
    Predicates shouldn't be pushed through project with nondeterministic field(s).
    
    See https://github.com/graphframes/graphframes/pull/23 and SPARK-13473 for more details.
    
    This PR targets master, branch-1.6, and branch-1.5.
    
    ## How was this patch tested?
    
    A test case is added in `FilterPushdownSuite`. It constructs a query plan where a filter is over a project with a nondeterministic field. Optimized query plan shouldn't change in this case.
    
    Author: Cheng Lian <lian@databricks.com>
    
    Closes #11348 from liancheng/spark-13473-no-ppd-through-nondeterministic-project-field.
    
    (cherry picked from commit 3fa6491be66dad690ca5329dd32e7c82037ae8c1)
    Signed-off-by: Wenchen Fan <wenchen@databricks.com>
    (cherry picked from commit 3cc938ac8124b8445f171baa365fa44a47962cc9)

commit 4fa437f4295882d7a1dd93011443a0b2f5a8c5db
Author: huangzhaowei <carlmartinmax@gmail.com>
Date:   Wed Feb 24 23:52:17 2016 -0800

    [SPARK-13482][MINOR][CONFIGURATION] Make consistency of the configuraiton named in TransportConf.
    
    `spark.storage.memoryMapThreshold` has two kind of the value, one is 2*1024*1024 as integer and the other one is '2m' as string.
    "2m" is recommanded in document but it will go wrong if the code goes into `TransportConf#memoryMapBytes`.
    
    [Jira](https://issues.apache.org/jira/browse/SPARK-13482)
    
    Author: huangzhaowei <carlmartinmax@gmail.com>
    
    Closes #11360 from SaintBacchus/SPARK-13482.
    
    (cherry picked from commit 264533b553be806b6c45457201952e83c028ec78)
    Signed-off-by: Reynold Xin <rxin@databricks.com>
    (cherry picked from commit 897599601a5ca0f95fd70f16e89df58b9b17705c)

commit 73ca17207c2c9ad6b7b46adffa3dd76075d82584
Author: Yin Huai <yhuai@databricks.com>
Date:   Wed Feb 24 13:34:53 2016 -0800

    [SPARK-13475][TESTS][SQL] HiveCompatibilitySuite should still run in PR builder even if a PR only changes sql/core
    
    ## What changes were proposed in this pull request?
    
    `HiveCompatibilitySuite` should still run in PR build even if a PR only changes sql/core. So, I am going to remove `ExtendedHiveTest` annotation from `HiveCompatibilitySuite`.
    
    https://issues.apache.org/jira/browse/SPARK-13475
    
    Author: Yin Huai <yhuai@databricks.com>
    
    Closes #11351 from yhuai/SPARK-13475.
    
    (cherry picked from commit bc353805bd98243872d520e05fa6659da57170bf)
    Signed-off-by: Yin Huai <yhuai@databricks.com>
    (cherry picked from commit fe71cabd46e4d384e8790dbfdda892df24b48e92)

commit 22bd21dd042f148505382fc5c38f003b8389f873
Author: Franklyn D'souza <franklynd@gmail.com>
Date:   Tue Feb 23 15:34:04 2016 -0800

    [SPARK-13410][SQL] Support unionAll for DataFrames with UDT columns.
    
    ## What changes were proposed in this pull request?
    
    This PR adds equality operators to UDT classes so that they can be correctly tested for dataType equality during union operations.
    
    This was previously causing `"AnalysisException: u"unresolved operator 'Union;""` when trying to unionAll two dataframes with UDT columns as below.
    
    ```
    from pyspark.sql.tests import PythonOnlyPoint, PythonOnlyUDT
    from pyspark.sql import types
    
    schema = types.StructType([types.StructField("point", PythonOnlyUDT(), True)])
    
    a = sqlCtx.createDataFrame([[PythonOnlyPoint(1.0, 2.0)]], schema)
    b = sqlCtx.createDataFrame([[PythonOnlyPoint(3.0, 4.0)]], schema)
    
    c = a.unionAll(b)
    ```
    
    ## How was the this patch tested?
    
    Tested using two unit tests in sql/test.py and the DataFrameSuite.
    
    Additional information here : https://issues.apache.org/jira/browse/SPARK-13410
    
    rxin
    
    Author: Franklyn D'souza <franklynd@gmail.com>
    
    Closes #11333 from damnMeddlingKid/udt-union-patch.
    
    (cherry picked from commit 573a2c97e9a9b8feae22f8af173fb158d59e5332)

commit 25f2e80a7a2bc6dab386d8af4964a765de490063
Author: Daoyuan Wang <daoyuan.wang@intel.com>
Date:   Mon Feb 22 18:13:32 2016 -0800

    [SPARK-11624][SPARK-11972][SQL] fix commands that need hive to exec
    
    In SparkSQLCLI, we have created a `CliSessionState`, but then we call `SparkSQLEnv.init()`, which will start another `SessionState`. This would lead to exception because `processCmd` need to get the `CliSessionState` instance by calling `SessionState.get()`, but the return value would be a instance of `SessionState`. See the exception below.
    
    spark-sql> !echo "test";
    Exception in thread "main" java.lang.ClassCastException: org.apache.hadoop.hive.ql.session.SessionState cannot be cast to org.apache.hadoop.hive.cli.CliSessionState
    	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:112)
    	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.processCmd(SparkSQLCLIDriver.scala:301)
    	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376)
    	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver$.main(SparkSQLCLIDriver.scala:242)
    	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.main(SparkSQLCLIDriver.scala)
    	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
    	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    	at java.lang.reflect.Method.invoke(Method.java:606)
    	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:691)
    	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180)
    	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205)
    	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120)
    	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
    
    Author: Daoyuan Wang <daoyuan.wang@intel.com>
    
    Closes #9589 from adrian-wang/clicommand.
    
    (cherry picked from commit 5d80fac58f837933b5359a8057676f45539e53af)
    Signed-off-by: Michael Armbrust <michael@databricks.com>
    
    Conflicts:
    	sql/hive/src/main/scala/org/apache/spark/sql/hive/client/ClientWrapper.scala
    
    (cherry picked from commit f7898f9e2df131fa78200f6034508e74a78c2a44)

commit 9c055759cadafe936431b554b2aa749abb902f8b
Author: Miles Yucht <miles@databricks.com>
Date:   Tue Feb 16 13:01:21 2016 +0000

    Correct SparseVector.parse documentation
    
    There's a small typo in the SparseVector.parse docstring (which says that it returns a DenseVector rather than a SparseVector), which seems to be incorrect.
    
    Author: Miles Yucht <miles@databricks.com>
    
    Closes #11213 from mgyucht/fix-sparsevector-docs.
    
    (cherry picked from commit 827ed1c06785692d14857bd41f1fd94a0853874a)
    Signed-off-by: Sean Owen <sowen@cloudera.com>
    (cherry picked from commit d95089190d714e3e95579ada84ac42d463f824b5)

commit 7ec51297f0801de94b885f6327253014b3a6ce1b
Author: Yuhao Yang <hhbyyh@gmail.com>
Date:   Wed Feb 3 21:19:44 2016 -0800

    [ML][DOC] fix wrong api link in ml onevsrest
    
    minor fix for api link in ml onevsrest
    
    Author: Yuhao Yang <hhbyyh@gmail.com>
    
    Closes #11068 from hhbyyh/onevsrestDoc.
    
    (cherry picked from commit c2c956bcd1a75fd01868ee9ad2939a6d3de52bc2)
    Signed-off-by: Xiangrui Meng <meng@databricks.com>
    (cherry picked from commit 2f390d3066297466d98e17a78c5433f37f70cc95)

commit b32203d111b2bd8ae0f541ccae42a668bcb8e500
Author: Kevin (Sangwoo) Kim <sangwookim.me@gmail.com>
Date:   Tue Feb 2 13:24:09 2016 -0800

    [DOCS] Update StructType.scala
    
    The example will throw error like
    <console>:20: error: not found: value StructType
    
    Need to add this line:
    import org.apache.spark.sql.types._
    
    Author: Kevin (Sangwoo) Kim <sangwookim.me@gmail.com>
    
    Closes #10141 from swkimme/patch-1.
    
    (cherry picked from commit b377b03531d21b1d02a8f58b3791348962e1f31b)
    Signed-off-by: Michael Armbrust <michael@databricks.com>
    (cherry picked from commit e81333be05cc5e2a41e5eb1a630c5af59a47dd23)

commit 8a900a5bc611fa535c974b477bc77a246494e6bb
Author: Takeshi YAMAMURO <linguin.m.s@gmail.com>
Date:   Mon Feb 1 12:02:06 2016 -0800

    [DOCS] Fix the jar location of datanucleus in sql-programming-guid.md
    
    ISTM `lib` is better because `datanucleus` jars are located in `lib` for release builds.
    
    Author: Takeshi YAMAMURO <linguin.m.s@gmail.com>
    
    Closes #10901 from maropu/DocFix.
    
    (cherry picked from commit da9146c91a33577ff81378ca7e7c38a4b1917876)
    Signed-off-by: Michael Armbrust <michael@databricks.com>
    (cherry picked from commit 215d5d8845b6e52d75522e1c0766d324d11e4d42)

commit 9b8c31bb8da4d7c155b0c0695281034939ca5c3c
Author: Wojciech Jurczyk <wojtek.jurczyk@gmail.com>
Date:   Tue Jan 19 09:36:45 2016 +0000

    [MLLIB] Fix CholeskyDecomposition assertion's message
    
    Change assertion's message so it's consistent with the code. The old message says that the invoked method was lapack.dports, where in fact it was lapack.dppsv method.
    
    Author: Wojciech Jurczyk <wojtek.jurczyk@gmail.com>
    
    Closes #10818 from wjur/wjur/rename_error_message.
    
    (cherry picked from commit ebd9ce0f1f55f7d2d3bd3b92c4b0a495c51ac6fd)
    Signed-off-by: Sean Owen <sowen@cloudera.com>
    (cherry picked from commit 962e618ec159f8cd26543f42b2ce484fd5a5d8c5)

commit a07ef07e71054ad9781203e09864d88915e7a647
Author: proflin <proflin.me@gmail.com>
Date:   Tue Jan 19 00:15:43 2016 -0800

    [SQL][MINOR] Fix one little mismatched comment according to the codes in interface.scala
    
    Author: proflin <proflin.me@gmail.com>
    
    Closes #10824 from proflin/master.
    
    (cherry picked from commit c00744e60f77edb238aff1e30b450dca65451e91)
    Signed-off-by: Reynold Xin <rxin@databricks.com>
    (cherry picked from commit 30f55e5232d85fd070892444367d2bb386dfce13)

commit e945f1edec0d59664dba32af1e2414961551f9d8
Author: Yin Huai <yhuai@databricks.com>
Date:   Mon Jan 11 19:59:15 2016 -0800

    [SPARK-11823] Ignores HiveThriftBinaryServerSuite's test jdbc cancel
    
    https://issues.apache.org/jira/browse/SPARK-11823
    
    This test often hangs and times out, leaving hanging processes. Let's ignore it for now and improve the test.
    
    Author: Yin Huai <yhuai@databricks.com>
    
    Closes #10715 from yhuai/SPARK-11823-ignore.
    
    (cherry picked from commit aaa2c3b628319178ca1f3f68966ff253c2de49cb)
    Signed-off-by: Josh Rosen <joshrosen@databricks.com>
    (cherry picked from commit a6c9c68d8855e3a8bfc92f26b3877b92367087a4)

commit 413abd611cfb5cbb1860194b2e685f066c24e3dc
Author: Jacek Laskowski <jacek@japila.pl>
Date:   Mon Jan 11 11:29:15 2016 -0800

    [STREAMING][MINOR] Typo fixes
    
    Author: Jacek Laskowski <jacek@japila.pl>
    
    Closes #10698 from jaceklaskowski/streaming-kafka-typo-fixes.
    
    (cherry picked from commit b313badaa049f847f33663c61cd70ee2f2cbebac)
    Signed-off-by: Shixiong Zhu <shixiong@databricks.com>
    (cherry picked from commit ce906b33de64f55653b52376316aa2625fd86b47)

commit 3d27502069902778490b0237ff8d7bb9d48bf4b8
Author: Udo Klein <git@blinkenlight.net>
Date:   Mon Jan 11 09:30:08 2016 +0000

    removed lambda from sortByKey()
    
    According to the documentation the sortByKey method does not take a lambda as an argument, thus the example is flawed. Removed the argument completely as this will default to ascending sort.
    
    Author: Udo Klein <git@blinkenlight.net>
    
    Closes #10640 from udoklein/patch-1.
    
    (cherry picked from commit bd723bd53d9a28239b60939a248a4ea13340aad8)
    Signed-off-by: Sean Owen <sowen@cloudera.com>
    (cherry picked from commit d4cfd2acd62f2b0638a12bbbb48a38263c04eaf8)

commit f7b4aa3558d02b9351f4d2bd3a92f1f1b29d7501
Author: Udo Klein <git@blinkenlight.net>
Date:   Fri Jan 8 20:32:37 2016 +0000

    fixed numVertices in transitive closure example
    
    Author: Udo Klein <git@blinkenlight.net>
    
    Closes #10642 from udoklein/patch-2.
    
    (cherry picked from commit 8c70cb4c62a353bea99f37965dfc829c4accc391)
    Signed-off-by: Sean Owen <sowen@cloudera.com>
    (cherry picked from commit e4227cb3e19afafe3a7b5a2847478681db2f2044)

commit fe512a1cf4b247e10cff21f22e623b333d018404
Author: zzcclp <xm_zzc@sina.com>
Date:   Wed Jan 6 23:06:21 2016 -0800

    [DOC] fix 'spark.memory.offHeap.enabled' default value to false
    
    modify 'spark.memory.offHeap.enabled' default value to false
    
    Author: zzcclp <xm_zzc@sina.com>
    
    Closes #10633 from zzcclp/fix_spark.memory.offHeap.enabled_default_value.
    
    (cherry picked from commit 84e77a15df18ba3f1cc871a3c52c783b46e52369)
    Signed-off-by: Reynold Xin <rxin@databricks.com>
    (cherry picked from commit 47a58c799206d011587e03178a259974be47d3bc)

commit 2f27e1941c0a317b89fa9a5560ee6f130d77e56a
Author: Yin Huai <yhuai@databricks.com>
Date:   Wed Jan 6 22:03:31 2016 -0800

    Revert "[SPARK-12006][ML][PYTHON] Fix GMM failure if initialModel is not None"
    
    This reverts commit fcd013cf70e7890aa25a8fe3cb6c8b36bf0e1f04.
    
    Author: Yin Huai <yhuai@databricks.com>
    
    Closes #10632 from yhuai/pythonStyle.
    
    (cherry picked from commit e5cde7ab11a43334fa01b1bb8904da5c0774bc62)
    Signed-off-by: Yin Huai <yhuai@databricks.com>
    (cherry picked from commit 34effc46cd54735cc660d8b43f0a190e91747a06)

commit 9aa994d4818db59e4205a31cea9511d7e3d405ec
Author: Shixiong Zhu <shixiong@databricks.com>
Date:   Wed Jan 6 13:53:25 2016 -0800

    Revert "[SPARK-12672][STREAMING][UI] Use the uiRoot function instead of default root path to gain the streaming batch url."
    
    This reverts commit 8f0ead3e79beb2c5f2731ceaa34fe1c133763386. Will merge #10618 instead.
    
    (cherry picked from commit 39b0a348008b6ab532768b90fd578b77711af98c)

commit e57e0a638d19fb0093ea8f396e8c4131140fa166
Author: huangzhaowei <carlmartinmax@gmail.com>
Date:   Wed Jan 6 12:48:57 2016 -0800

    [SPARK-12672][STREAMING][UI] Use the uiRoot function instead of default root path to gain the streaming batch url.
    
    Author: huangzhaowei <carlmartinmax@gmail.com>
    
    Closes #10617 from SaintBacchus/SPARK-12672.
    
    (cherry picked from commit 8f0ead3e79beb2c5f2731ceaa34fe1c133763386)

commit e453e051ff68d60a3487b60f34370623ab85acbf
Author: felixcheung <felixcheung_m@hotmail.com>
Date:   Tue Jan 5 08:39:58 2016 +0530

    [SPARKR][DOC] minor doc update for version in migration guide
    
    checked that the change is in Spark 1.6.0.
    shivaram
    
    Author: felixcheung <felixcheung_m@hotmail.com>
    
    Closes #10574 from felixcheung/rwritemodedoc.
    
    (cherry picked from commit 8896ec9f02a6747917f3ae42a517ff0e3742eaf6)
    Signed-off-by: Shivaram Venkataraman <shivaram@cs.berkeley.edu>
    (cherry picked from commit 8950482ee5e9132d11dc5b5d41132bb1fe1e7ba2)

commit 320886e3b8f9424a6a622ba5089b69676ea9c8aa
Author: tedyu <yuzhihong@gmail.com>
Date:   Mon Jan 4 12:38:04 2016 -0800

    [DOC] Adjust coverage for partitionBy()
    
    This is the related thread: http://search-hadoop.com/m/q3RTtO3ReeJ1iF02&subj=Re+partitioning+json+data+in+spark
    
    Michael suggested fixing the doc.
    
    Please review.
    
    Author: tedyu <yuzhihong@gmail.com>
    
    Closes #10499 from ted-yu/master.
    
    (cherry picked from commit 40d03960d79debdff5cef21997417c4f8a8ce2e9)
    Signed-off-by: Michael Armbrust <michael@databricks.com>
    (cherry picked from commit 1005ee396f74dc4fcf127613b65e1abdb7f1934c)

commit eb2ba60ee2428f488419603d45554793303cea0b
Author: Josh Rosen <joshrosen@databricks.com>
Date:   Mon Dec 21 23:12:05 2015 -0800

    [SPARK-11823][SQL] Fix flaky JDBC cancellation test in HiveThriftBinaryServerSuite
    
    This patch fixes a flaky "test jdbc cancel" test in HiveThriftBinaryServerSuite. This test is prone to a race-condition which causes it to block indefinitely with while waiting for an extremely slow query to complete, which caused many Jenkins builds to time out.
    
    For more background, see my comments on #6207 (the PR which introduced this test).
    
    Author: Josh Rosen <joshrosen@databricks.com>
    
    Closes #10425 from JoshRosen/SPARK-11823.
    
    (cherry picked from commit 2235cd44407e3b6b401fb84a2096ade042c51d36)
    Signed-off-by: Josh Rosen <joshrosen@databricks.com>
    (cherry picked from commit 0f905d7df43b20d9335ec880b134d8d4f962c297)

commit d7baceee1efb982af32b8a834ad82ecbabc92a06
Author: Shixiong Zhu <shixiong@databricks.com>
Date:   Mon Dec 21 22:28:18 2015 -0800

    [MINOR] Fix typos in JavaStreamingContext
    
    Author: Shixiong Zhu <shixiong@databricks.com>
    
    Closes #10424 from zsxwing/typo.
    
    (cherry picked from commit 93da8565fea42d8ac978df411daced4a9ea3a9c8)
    Signed-off-by: Reynold Xin <rxin@databricks.com>
    (cherry picked from commit 309ef355fc511b70765983358d5c92b5f1a26bce)

commit b9dcb832a27b0fc558bf5f22cd6bcf1dddcf4cc8
Author: Brandon Bradley <bradleytastic@gmail.com>
Date:   Fri Feb 19 14:43:21 2016 -0800

    [SPARK-12966][SQL] ArrayType(DecimalType) support in Postgres JDBC
    
    Fixes error `org.postgresql.util.PSQLException: Unable to find server array type for provided name decimal(38,18)`.
    
    * Passes scale metadata to JDBC dialect for usage in type conversions.
    * Removes unused length/scale/precision parameters from `createArrayOf` parameter `typeName` (for writing).
    * Adds configurable precision and scale to Postgres `DecimalType` (for reading).
    * Adds a new kind of test that verifies the schema written by `DataFrame.write.jdbc`.
    
    Author: Brandon Bradley <bradleytastic@gmail.com>
    
    Closes #10928 from blbradley/spark-12966.
    
    (cherry picked from commit d1cd5ca1c2c007de7a2778cdcec406ec95c87ce3)
    (cherry picked from commit 8a655e821650509c3f86ef134499f6073a79de41)

commit 7485feaf4e976840a9901f70ba536788b92c62e5
Author: Tejas Patil <tejasp@fb.com>
Date:   Fri Jul 15 14:27:16 2016 -0700

    [SPARK-16230][CORE] CoarseGrainedExecutorBackend to self kill if there is an exception while creating an Executor
    
    With the fix from SPARK-13112, I see that `LaunchTask` is always processed after `RegisteredExecutor` is done and so it gets chance to do all retries to startup an executor. There is still a problem that if `Executor` creation itself fails and there is some exception, it gets unnoticed and the executor is killed when it tries to process the `LaunchTask` as `executor` is null : https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/executor/CoarseGrainedExecutorBackend.scala#L88 So if one looks at the logs, it does not tell that there was problem during `Executor` creation and thats why it was killed.
    
    This PR explicitly catches exception in `Executor` creation, logs a proper message and then exits the JVM. Also, I have changed the `exitExecutor` method to accept `reason` so that backends can use that reason and do stuff like logging to a DB to get an aggregate of such exits at a cluster level
    
    I am relying on existing tests
    
    Author: Tejas Patil <tejasp@fb.com>
    
    Closes #14202 from tejasapatil/exit_executor_failure.
    
    (cherry picked from commit b2f24f94591082d3ff82bd3db1760b14603b38aa)

commit 213d289f5f5f5904d53bbf32c6232e261415302b
Author: Shixiong Zhu <shixiong@databricks.com>
Date:   Wed Apr 6 16:18:04 2016 -0700

    [SPARK-13112][CORE] Make sure RegisterExecutorResponse arrive before LaunchTask
    
    Send `RegisterExecutorResponse` using `executorRef` in order to make sure RegisterExecutorResponse and LaunchTask are both sent using the same channel. Then RegisterExecutorResponse will always arrive before LaunchTask
    
    Existing unit tests
    
    Closes #12078
    
    Author: Shixiong Zhu <shixiong@databricks.com>
    
    Closes #12211 from zsxwing/SPARK-13112.
    
    (cherry picked from commit f1def573f4c1c757f727476521a1509b5285051d)

commit ba76d874add4e525aeaf9d9deecad4e21c53fec5
Author: Artur Sukhenko <artur.sukhenko@gmail.com>
Date:   Tue Aug 2 16:13:12 2016 -0700

    [SPARK-16796][WEB UI] Visible passwords on Spark environment page
    
    ## What changes were proposed in this pull request?
    
    Mask spark.ssl.keyPassword, spark.ssl.keyStorePassword, spark.ssl.trustStorePassword in Web UI environment page.
    (Changes their values to ***** in env. page)
    
    ## How was this patch tested?
    
    I've built spark, run spark shell and checked that this values have been masked with *****.
    
    Also run tests:
    ./dev/run-tests
    
    [info] ScalaTest
    [info] Run completed in 1 hour, 9 minutes, 5 seconds.
    [info] Total number of tests run: 2166
    [info] Suites: completed 65, aborted 0
    [info] Tests: succeeded 2166, failed 0, canceled 0, ignored 590, pending 0
    [info] All tests passed.
    
    ![mask](https://cloud.githubusercontent.com/assets/15244468/17262154/7641e132-55e2-11e6-8a6c-30ead77c7372.png)
    
    Author: Artur Sukhenko <artur.sukhenko@gmail.com>
    
    Closes #14409 from Devian-ua/maskpass.
    
    (cherry picked from commit 3861273771c2631e88e1f37a498c644ad45ac1c0)
    Signed-off-by: Sean Owen <sowen@cloudera.com>
    (cherry picked from commit 52d8837c62808329ae619e2c63bf01d9c3bb0c7a)

commit fd2aec689d4d33cf59a4f09c5c61b32914a17101
Author: Marcelo Vanzin <vanzin@cloudera.com>
Date:   Wed Aug 17 11:12:21 2016 -0700

    [SPARK-16930][YARN] Fix a couple of races in cluster app initialization.
    
    There are two narrow races that could cause the ApplicationMaster to miss
    when the user application instantiates the SparkContext, which could cause
    app failures when nothing was wrong with the app. It was also possible for
    a failing application to get stuck in the loop that waits for the context
    for a long time, instead of failing quickly.
    
    The change uses a promise to track the SparkContext instance, which gets
    rid of the races and allows for some simplification of the code.
    
    Tested with existing unit tests, and a new one being added to test the
    timeout code.
    
    Author: Marcelo Vanzin <vanzin@cloudera.com>
    
    Closes #14542 from vanzin/SPARK-16930.
    
    (cherry picked from commit e3fec51fa1ed161789ab7aa32ed36efe357b5d31)

commit c51e614783759b3e9d67dca81b5dab2f828e7031
Author: Imran Rashid <irashid@cloudera.com>
Date:   Thu Oct 27 20:57:19 2016 -0700

    [CDH-46529] Fix backport of SPARK-15891.
    
    This change fixes the way executor memory is passed to yarn -- we need
    to convert "spark.executor.memory" to MB, since an "m" is automatically
    appended to it to build the jvm args.  This fixes a bug in the backport
    of SPARK-15891, because the original chance used a conf feature not
    present in 1.6.

commit 9e038fb5cf0198bfbd116eb2b36ff06eaf1b881e
Author: Marcelo Vanzin <vanzin@cloudera.com>
Date:   Tue Sep 6 15:54:54 2016 -0700

    [SPARK-15891][YARN] Clean up some logging in the YARN AM.
    
    To make the log file more readable, rework some of the logging done
    by the AM:
    
    - log executor command / env just once, since they're all almost the same;
      the information that changes, such as executor ID, is already available
      in other log messages.
    - avoid printing logs when nothing happens, especially when updating the
      container requests in the allocator.
    - print fewer log messages when requesting many unlocalized executors,
      instead of repeating the same message multiple times.
    - removed some logs that seemed unnecessary.
    
    In the process, I slightly fixed up the wording in a few log messages, and
    did some minor clean up of method arguments that were redundant.
    
    Tested by running existing unit tests, and analyzing the logs of an
    application that exercises dynamic allocation by forcing executors
    to be allocated and be killed in waves.
    
    Author: Marcelo Vanzin <vanzin@cloudera.com>
    
    Closes #14943 from vanzin/SPARK-15891.
    
    (cherry picked from commit 0bd00ff2454c5046e4cb084ee64d432c4d3dcbc3)

commit 5c99eacfa6ede8c482d7ae675cbd88f52eb38063
Author: Angus Gerry <angolon@gmail.com>
Date:   Thu Sep 1 10:35:31 2016 -0700

    [SPARK-16533][CORE] resolve deadlocking in driver when executors die
    
    This pull request reverts the changes made as a part of #14605, which simply side-steps the deadlock issue. Instead, I propose the following approach:
    * Use `scheduleWithFixedDelay` when calling `ExecutorAllocationManager.schedule` for scheduling executor requests. The intent of this is that if invocations are delayed beyond the default schedule interval on account of lock contention, then we avoid a situation where calls to `schedule` are made back-to-back, potentially releasing and then immediately reacquiring these locks - further exacerbating contention.
    * Replace a number of calls to `askWithRetry` with `ask` inside of message handling code in `CoarseGrainedSchedulerBackend` and its ilk. This allows us queue messages with the relevant endpoints, release whatever locks we might be holding, and then block whilst awaiting the response. This change is made at the cost of being able to retry should sending the message fail, as retrying outside of the lock could easily cause race conditions if other conflicting messages have been sent whilst awaiting a response. I believe this to be the lesser of two evils, as in many cases these RPC calls are to process local components, and so failures are more likely to be deterministic, and timeouts are more likely to be caused by lock contention.
    
    Existing tests, and manual tests under yarn-client mode.
    
    Author: Angus Gerry <angolon@gmail.com>
    
    Closes #14710 from angolon/SPARK-16533.
    
    (cherry picked from commit a0aac4b775bc8c275f96ad0fbf85c9d8a3690588)

commit 85f8367ecfd26ed2a8f914bde3b84120eeb55351
Author: jerryshao <sshao@hortonworks.com>
Date:   Thu Jun 9 17:31:19 2016 -0700

    [SPARK-12447][YARN] Only update the states when executor is successfully launched
    
    The details is described in https://issues.apache.org/jira/browse/SPARK-12447.
    
    vanzin Please help to review, thanks a lot.
    
    Author: jerryshao <sshao@hortonworks.com>
    
    Closes #10412 from jerryshao/SPARK-12447.
    
    (cherry picked from commit aa0364510792c18a0973b6096cd38f611fc1c1a6)

commit b65a6dce6c6368f25e6e8716527bfae658bdf51c
Author: Marcelo Vanzin <vanzin@cloudera.com>
Date:   Fri Sep 16 14:02:56 2016 -0700

    [SPARK-17549][SQL] Only collect table size stat in driver for cached relation.
    
    The existing code caches all stats for all columns for each partition
    in the driver; for a large relation, this causes extreme memory usage,
    which leads to gc hell and application failures.
    
    It seems that only the size in bytes of the data is actually used in the
    driver, so instead just colllect that. In executors, the full stats are
    still kept, but that's not a big problem; we expect the data to be distributed
    and thus not really incur in too much memory pressure in each individual
    executor.
    
    There are also potential improvements on the executor side, since the data
    being stored currently is very wasteful (e.g. storing boxed types vs.
    primitive types for stats). But that's a separate issue.
    
    On a mildly related change, I'm also adding code to catch exceptions in the
    code generator since Janino was breaking with the test data I tried this
    patch on.
    
    Tested with unit tests and by doing a count a very wide table (20k columns)
    with many partitions.
    
    Author: Marcelo Vanzin <vanzin@cloudera.com>
    
    Closes #15112 from vanzin/SPARK-17549.
    
    (cherry picked from commit 39e2bad6a866d27c3ca594d15e574a1da3ee84cc)

commit 860de8907393049583f59a8e7b513888cb5800da
Author: sharkd <sharkd.tu@gmail.com>
Date:   Tue Jul 12 10:10:35 2016 -0700

    [SPARK-16414][YARN] Fix bugs for "Can not get user config when calling SparkHadoopUtil.get.conf on yarn cluser mode"
    
    The `SparkHadoopUtil` singleton was instantiated before `ApplicationMaster` in `ApplicationMaster.main` when deploying spark on yarn cluster mode, the `conf` in the `SparkHadoopUtil` singleton didn't include user's configuration.
    
    So, we should load the properties file with the Spark configuration and set entries as system properties before `SparkHadoopUtil` first instantiate.
    
    Add a test case
    
    Author: sharkd <sharkd.tu@gmail.com>
    Author: sharkdtu <sharkdtu@tencent.com>
    
    Closes #14088 from sharkdtu/master.
    
    (cherry picked from commit d513c99c19e229f72d03006e251725a43c13fefd)

commit b3ba3aa67f461c8a444faa03dcfb376ea09e482d
Author: Simon Scott <simon.scott@viavisolutions.com>
Date:   Thu May 26 08:13:28 2016 -0500

    [SPARK-10722] RDDBlockId not found in driver-heartbeater
    
    ## What changes were proposed in this pull request?
    
    To ensure that the deserialization of TaskMetrics uses a ClassLoader that knows about RDDBlockIds. The problem occurs only very rarely since it depends on which thread of the thread pool is used for the heartbeat.
    
    I observe that the code in question has been largely rewritten for v2.0.0 of Spark and the problem no longer manifests. However it would seem reasonable to fix this for those users who need to continue with the 1.6 version for some time yet. Hence I have created a fix for the 1.6 code branch.
    
    ## How was this patch tested?
    
    Due to the nature of the problem a reproducible testcase is difficult to produce. This problem was causing our application's nightly integration tests to fail randomly. Since applying the fix the tests have not failed due to this problem, for nearly six weeks now.
    
    Author: Simon Scott <simon.scott@viavisolutions.com>
    
    Closes #13222 from simonjscott/fix-10722.
    
    (cherry picked from commit 5cc1e2cec71ef18c76973608e909b4f37fcfcf6b)

commit c1667fd1956d647a28f223c9451ce84b1d76f6a6
Author: Nezih Yigitbasi <nyigitbasi@netflix.com>
Date:   Fri Mar 11 11:11:53 2016 -0800

    [SPARK-13328][CORE] Poor read performance for broadcast variables with dynamic resource allocation
    
    When dynamic resource allocation is enabled fetching broadcast variables from removed executors were causing job failures and SPARK-9591 fixed this problem by trying all locations of a block before giving up. However, the locations of a block is retrieved only once from the driver in this process and the locations in this list can be stale due to dynamic resource allocation. This situation gets worse when running on a large cluster as the size of this location list can be in the order of several hundreds out of which there may be tens of stale entries. What we have observed is with the default settings of 3 max retries and 5s between retries (that's 15s per location) the time it takes to read a broadcast variable can be as high as ~17m (70 failed attempts * 15s/attempt)
    
    Author: Nezih Yigitbasi <nyigitbasi@netflix.com>
    
    Closes #11241 from nezihyigitbasi/SPARK-13328.
    
    (cherry picked from commit ed76151bac67853be9d02505098843f75e7239d0)

commit cb92fc8c9dca0749c2d9bc923f7be2f8c6b98e95
Author: Takeshi YAMAMURO <linguin.m.s@gmail.com>
Date:   Mon Dec 21 14:02:40 2015 -0800

    [SPARK-12392][CORE] Optimize a location order of broadcast blocks by considering preferred local hosts
    
    When multiple workers exist in a host, we can bypass unnecessary remote access for broadcasts; block managers fetch broadcast blocks from the same host instead of remote hosts.
    
    Author: Takeshi YAMAMURO <linguin.m.s@gmail.com>
    
    Closes #10346 from maropu/OptimizeBlockLocationOrder.
    
    (cherry picked from commit c7f3911f78b850899d5623c5982884278cdafb62)

commit 31f64ac9b49638bd5070f4794f30caa4b2030f97
Author: Yuming Wang <wgyumg@gmail.com>
Date:   Fri Aug 5 16:11:54 2016 +0100

    [SPARK-16625][SQL] General data types to be mapped to Oracle
    
    Spark will convert **BooleanType** to **BIT(1)**, **LongType** to **BIGINT**, **ByteType**  to **BYTE** when saving DataFrame to Oracle, but Oracle does not support BIT, BIGINT and BYTE types.
    
    This PR is convert following _Spark Types_ to _Oracle types_ refer to [Oracle Developer's Guide](https://docs.oracle.com/cd/E19501-01/819-3659/gcmaz/)
    
    Spark Type | Oracle
    ----|----
    BooleanType | NUMBER(1)
    IntegerType | NUMBER(10)
    LongType | NUMBER(19)
    FloatType | NUMBER(19, 4)
    DoubleType | NUMBER(19, 4)
    ByteType | NUMBER(3)
    ShortType | NUMBER(5)
    
    Add new tests in [JDBCSuite.scala](https://github.com/wangyum/spark/commit/22b0c2a4228cb8b5098ad741ddf4d1904e745ff6#diff-dc4b58851b084b274df6fe6b189db84d) and [OracleDialect.scala](https://github.com/wangyum/spark/commit/22b0c2a4228cb8b5098ad741ddf4d1904e745ff6#diff-5e0cadf526662f9281aa26315b3750ad)
    
    Author: Yuming Wang <wgyumg@gmail.com>
    
    Closes #14377 from wangyum/SPARK-16625.
    
    (cherry picked from commit cb741e0b7dac2afd1af1f1672b131914af8aa50e)

commit 75d102853f6a19ea2b67c54d82b7125d9f4d5344
Author: thomastechs <thomas.sebastian@tcs.com>
Date:   Thu Mar 3 20:35:40 2016 -0800

    [SPARK-12941][SQL][MASTER] Spark-SQL JDBC Oracle dialect fails to map string datatypes to Oracle VARCHAR datatype mapping
    
    A test suite added for the bug fix -SPARK 12941; for the mapping of the StringType to corresponding in Oracle
    
    manual tests done
    (Please explain how this patch was tested. E.g. unit tests, integration tests, manual tests)
    
    (If this patch involves UI changes, please attach a screenshot; otherwise, remove this)
    
    Author: thomastechs <thomas.sebastian@tcs.com>
    Author: THOMAS SEBASTIAN <thomas.sebastian@tcs.com>
    
    Closes #11489 from thomastechs/thomastechs-12941-master-new.
    
    (cherry picked from commit 778609720c422eba18d93f7657fd0632fa817276)

commit af1ec04806180f4299245d407b56b72e406514c1
Author: thomastechs <thomas.sebastian@tcs.com>
Date:   Thu Feb 25 22:52:25 2016 -0800

    [SPARK-12941][SQL][MASTER] Spark-SQL JDBC Oracle dialect fails to map string datatypes to Oracle VARCHAR datatype
    
    ## What changes were proposed in this pull request?
    
    This Pull request is used for the fix SPARK-12941, creating a data type mapping to Oracle for the corresponding data type"Stringtype" from dataframe. This PR is for the master branch fix, where as another PR is already tested with the branch 1.4
    
    ## How was the this patch tested?
    
    (Please explain how this patch was tested. E.g. unit tests, integration tests, manual tests)
    This patch was tested using the Oracle docker .Created a new integration suite for the same.The oracle.jdbc jar was to be downloaded from the maven repository.Since there was no jdbc jar available in the maven repository, the jar was downloaded from oracle site manually and installed in the local; thus tested. So, for SparkQA test case run, the ojdbc jar might be manually placed in the local maven repository(com/oracle/ojdbc6/11.2.0.2.0) while Spark QA test run.
    
    Author: thomastechs <thomas.sebastian@tcs.com>
    
    Closes #11306 from thomastechs/master.
    
    (cherry picked from commit 2c05e3f8e451024802c15cf5d0088598881bff25)

commit 7d37c9afefc4b26d4398f22af3adcc2387f49ddc
Author: w00228970 <wangfei1@huawei.com>
Date:   Wed Sep 28 12:02:59 2016 -0700

    [SPARK-17644][CORE] Do not add failedStages when abortStage for fetch failure
    
    | Time        |Thread 1 ,  Job1          | Thread 2 ,  Job2  |
    |:-------------:|:-------------:|:-----:|
    | 1 | abort stage due to FetchFailed |  |
    | 2 | failedStages += failedStage |    |
    | 3 |      |  task failed due to  FetchFailed |
    | 4 |      |  can not post ResubmitFailedStages because failedStages is not empty |
    
    Then job2 of thread2 never resubmit the failed stage and hang.
    
    We should not add the failedStages when abortStage for fetch failure
    
    added unit test
    
    Author: w00228970 <wangfei1@huawei.com>
    Author: wangfei <wangfei_hello@126.com>
    
    Closes #15213 from scwf/dag-resubmit.
    
    (cherry picked from commit 46d1203bf2d01b219c4efc7e0e77a844c0c664da)

commit 2684acc14b626204add4a1f1605c48649993b802
Author: Charles Allen <charles@allen-net.com>
Date:   Wed Sep 28 14:39:50 2016 -0700

    [SPARK-17696][SPARK-12330][CORE] Partial backport of to branch-1.6.
    
    From the original commit message:
    
    This PR also fixes a regression caused by [SPARK-10987] whereby submitting a shutdown causes a race between the local shutdown procedure and the notification of the scheduler driver disconnection. If the scheduler driver disconnection wins the race, the coarse executor incorrectly exits with status 1 (instead of the proper status 0)
    
    Author: Charles Allen <charlesallen-net.com>
    
    (cherry picked from commit 2eaeafe8a2aa31be9b230b8d53d3baccd32535b1)
    
    Author: Charles Allen <charles@allen-net.com>
    
    Closes #15270 from vanzin/SPARK-17696.
    
    (cherry picked from commit b999fa43ea0b509341ac2e130cc3787e5f8a75e5)

commit b6ab3b2deb7d5d73638dc030aad7abe2740f26a2
Author: hushan <hushan@xiaomi.com>
Date:   Thu Feb 25 16:57:41 2016 -0800

    [SPARK-12009][YARN] Avoid to re-allocating yarn container while driver want to stop all Executors
    
    Author: hushan <hushan@xiaomi.com>
    
    Closes #9992 from suyanNone/tricky.
    
    (cherry picked from commit 7a6ee8a8fe0fad78416ed7e1ac694959de5c5314)

commit a2755c3a1f8b5926fd19f6ee2aa0d289709f5ea1
Author: Marcelo Vanzin <vanzin@cloudera.com>
Date:   Thu Sep 22 19:36:18 2016 -0700

    CLOUDERA-BUILD. CDH-44281. Work around issue with CDH jackson version.

commit ef52f971d4b6661ceeac439ba4dc2400cf64011c
Author: Marcelo Vanzin <vanzin@cloudera.com>
Date:   Tue Sep 20 14:17:49 2016 -0700

    [SPARK-17611][YARN][TEST] Make shuffle service test really test auth.
    
    Currently, the code is just swallowing exceptions, and not really checking
    whether the auth information was being recorded properly. Fix both problems,
    and also avoid tests inadvertently affecting other tests by modifying the
    shared config variable (by making it not shared).
    
    Author: Marcelo Vanzin <vanzin@cloudera.com>
    
    Closes #15161 from vanzin/SPARK-17611.
    
    (cherry picked from commit 7e418e99cff4cf512ab2a9fa74221c4655048c8d)

commit 5be709fcc380d35e5979ae12a7acd7c81ef710a9
Author: Thomas Graves <tgraves@prevailsail.corp.gq1.yahoo.com>
Date:   Fri Sep 9 13:43:32 2016 -0500

    [SPARK-17433] YarnShuffleService doesn't handle moving credentials levelDb
    
    The secrets leveldb isn't being moved if you run spark shuffle services without yarn nm recovery on and then turn it on.  This fixes that.  I unfortunately missed this when I ported the patch from our internal branch 2 to master branch due to the changes for the recovery path.  Note this only applies to master since it is the only place the yarn nm recovery dir is used.
    
    Unit tests ran and tested on 8 node cluster.  Fresh startup with NM recovery, fresh startup no nm recovery, switching between no nm recovery and recovery.  Also tested running applications to make sure wasn't affected by rolling upgrade.
    
    Author: Thomas Graves <tgraves@prevailsail.corp.gq1.yahoo.com>
    Author: Tom Graves <tgraves@apache.org>
    
    Closes #14999 from tgravescs/SPARK-17433.
    
    (cherry picked from commit a3981c28c956a82ccf5b1c61d45b6bd252d4abed)

commit 4053133f6acf6cbe3ec73d845df91f79050427d5
Author: Thomas Graves <tgraves@staydecay.corp.gq1.yahoo.com>
Date:   Fri Sep 2 10:42:13 2016 -0700

    [SPARK-16711] YarnShuffleService doesn't re-init properly on YARN rolling upgrade
    
    The Spark Yarn Shuffle Service doesn't re-initialize the application credentials early enough which causes any other spark executors trying to fetch from that node during a rolling upgrade to fail with "java.lang.NullPointerException: Password cannot be null if SASL is enabled".  Right now the spark shuffle service relies on the Yarn nodemanager to re-register the applications, unfortunately this is after we open the port for other executors to connect. If other executors connected before the re-register they get a null pointer exception which isn't a re-tryable exception and cause them to fail pretty quickly. To solve this I added another leveldb file so that it can save and re-initialize all the applications before opening the port for other executors to connect to it.  Adding another leveldb was simpler from the code structure point of view.
    
    Most of the code changes are moving things to common util class.
    
    Patch was tested manually on a Yarn cluster with rolling upgrade was happing while spark job was running. Without the patch I consistently get the NullPointerException, with the patch the job gets a few Connection refused exceptions but the retries kick in and the it succeeds.
    
    Author: Thomas Graves <tgraves@staydecay.corp.gq1.yahoo.com>
    
    Closes #14718 from tgravescs/SPARK-16711.
    
    (cherry picked from commit e79962f2f3955485aecf32939207d8ee6ccd2704)

commit e6569f6773d34604e589e2150b447af09093542f
Author: cenyuhai <cenyuhai@didichuxing.com>
Date:   Mon Sep 12 11:52:56 2016 +0100

    [SPARK-17171][WEB UI] DAG will list all partitions in the graph
    
    DAG will list all partitions in the graph, it is too slow and hard to see all graph.
    Always we don't want to see all partitions，we just want to see the relations of DAG graph.
    So I just show 2 root nodes for Rdds.
    
    Before this PR, the DAG graph looks like [dag1.png](https://issues.apache.org/jira/secure/attachment/12824702/dag1.png), [dag3.png](https://issues.apache.org/jira/secure/attachment/12825456/dag3.png), after this PR, the DAG graph looks like [dag2.png](https://issues.apache.org/jira/secure/attachment/12824703/dag2.png),[dag4.png](https://issues.apache.org/jira/secure/attachment/12825457/dag4.png)
    
    Author: cenyuhai <cenyuhai@didichuxing.com>
    Author: 岑玉海 <261810726@qq.com>
    
    Closes #14737 from cenyuhai/SPARK-17171.
    
    (cherry picked from commit cc87280fcd065b01667ca7a59a1a32c7ab757355)

commit 6c9c7cd2ad7775e02ef577a26cdaf0d9c027aadf
Author: Thomas Graves <tgraves@staydecay.corp.gq1.yahoo.com>
Date:   Fri May 6 19:31:26 2016 -0700

    [SPARK-1239] Improve fetching of map output statuses
    
    The main issue we are trying to solve is the memory bloat of the Driver when tasks request the map output statuses.  This means with a large number of tasks you either need a huge amount of memory on Driver or you have to repartition to smaller number.  This makes it really difficult to run over say 50000 tasks.
    
    The main issues that cause the memory bloat are:
    1) no flow control on sending the map output status responses.  We serialize the map status output  and then hand off to netty to send.  netty is sending asynchronously and it can't send them fast enough to keep up with incoming requests so we end up with lots of copies of the serialized map output statuses sitting there and this causes huge bloat when you have 10's of thousands of tasks and map output status is in the 10's of MB.
    2) When initial reduce tasks are started up, they all request the map output statuses from the Driver. These requests are handled by multiple threads in parallel so even though we check to see if we have a cached version, initially when we don't have a cached version yet, many of initial requests can all end up serializing the exact same map output statuses.
    
    This patch does a couple of things:
    - When the map output status size is over a threshold (default 512K) then it uses broadcast to send the map statuses.  This means we no longer serialize a large map output status and thus we don't have issues with memory bloat.  the messages sizes are now in the 300-400 byte range and the map status output are broadcast. If its under the threadshold it sends it as before, the message contains the DIRECT indicator now.
    - synchronize the incoming requests to allow one thread to cache the serialized output and broadcast the map output status  that can then be used by everyone else.  This ensures we don't create multiple broadcast variables when we don't need to.  To ensure this happens I added a second thread pool which the Dispatcher hands the requests to so that those threads can block without blocking the main dispatcher threads (which would cause things like heartbeats and such not to come through)
    
    Note that some of design and code was contributed by mridulm
    
    Unit tests and a lot of manually testing.
    Ran with akka and netty rpc. Ran with both dynamic allocation on and off.
    
    one of the large jobs I used to test this was a join of 15TB of data.  it had 200,000 map tasks, and  20,000 reduce tasks. Executors ranged from 200 to 2000.  This job ran successfully with 5GB of memory on the driver with these changes. Without these changes I was using 20GB and only had 500 reduce tasks.  The job has 50mb of serialized map output statuses and took roughly the same amount of time for the executors to get the map output statuses as before.
    
    Ran a variety of other jobs, from large wordcounts to small ones not using broadcasts.
    
    Author: Thomas Graves <tgraves@staydecay.corp.gq1.yahoo.com>
    
    Closes #12113 from tgravescs/SPARK-1239.
    
    (cherry picked from commit cc95f1ed5fdf2566bcefe8d10116eee544cf9184)

commit 0047b97b1bc1969f00eb73d9a7aa60f50e8efce2
Author: Alex Bozarth <ajbozart@us.ibm.com>
Date:   Mon Dec 21 14:06:36 2015 -0800

    [SPARK-12339][SPARK-11206][WEBUI] Added a null check that was removed in
    
    Updates made in SPARK-11206 missed an edge case which cause's a NullPointerException when a task is killed. In some cases when a task ends in failure taskMetrics is initialized as null (see JobProgressListener.onTaskEnd()). To address this a null check was added. Before the changes in SPARK-11206 this null check was called at the start of the updateTaskAccumulatorValues() function.
    
    Author: Alex Bozarth <ajbozart@us.ibm.com>
    
    Closes #10405 from ajbozarth/spark12339.
    
    (cherry picked from commit b0849b8aeafa801bb0561f1f6e46dc1d56c37c19)

commit 2e1bd1dd268d9d527ec08fa63ef5984e5b0d5e68
Author: Sameer Agarwal <sameer@databricks.com>
Date:   Thu May 26 18:49:54 2016 -0700

    [BUILD][1.6] Fix compilation
    
    ## What changes were proposed in this pull request?
    
    Makes `UnsafeSortDataFormat`  and `RecordPointerAndKeyPrefix` public. These are already public in 2.0 and are used in an `ExternalSorterSuite` test (see https://github.com/apache/spark/commit/0b8bdf793a98296fd1ac1fc499946929c6a5959d)
    
    ## How was this patch tested?
    
    Successfully builds locally
    
    Author: Sameer Agarwal <sameer@databricks.com>
    
    Closes #13339 from sameeragarwal/fix-compile.
    
    (cherry picked from commit c53c83ce84ff57570d48cfc06583c8e5b9c9a90d)

commit 0d693000a0005d7078717a167d9474b554fc03b8
Author: Sameer Agarwal <sameer@databricks.com>
Date:   Thu May 26 15:49:16 2016 -0700

    [SPARK-8428][SPARK-13850] Fix integer overflows in TimSort
    
    This patch fixes a few integer overflows in `UnsafeSortDataFormat.copyRange()` and `ShuffleSortDataFormat copyRange()` that seems to be the most likely cause behind a number of `TimSort` contract violation errors seen in Spark 2.0 and Spark 1.6 while sorting large datasets.
    
    Added a test in `ExternalSorterSuite` that instantiates a large array of the form of [150000000, 150000001, 150000002, ...., 300000000, 0, 1, 2, ..., 149999999] that triggers a `copyRange` in `TimSort.mergeLo` or `TimSort.mergeHi`. Note that the input dataset should contain at least 268.43 million rows with a certain data distribution for an overflow to occur.
    
    Author: Sameer Agarwal <sameer@databricks.com>
    
    Closes #13336 from sameeragarwal/timsort-bug.
    
    (cherry picked from commit fe6de16f781ff659b34e0ddda427d371d3d94536)
    Signed-off-by: Reynold Xin <rxin@databricks.com>
    (cherry picked from commit 0b8bdf793a98296fd1ac1fc499946929c6a5959d)

commit a41fb1f9c4e884a3dbd6fe2fdb7caf7740d90216
Author: Marcelo Vanzin <vanzin@cloudera.com>
Date:   Fri Apr 29 23:13:50 2016 -0700

    [SPARK-14391][LAUNCHER] Fix launcher communication test, take 2.
    
    There's actually a race here: the state of the handler was changed before
    the connection was set, so the test code could be notified of the state
    change, wake up, and still see the connection as null, triggering the assert.
    
    Author: Marcelo Vanzin <vanzin@cloudera.com>
    
    Closes #12785 from vanzin/SPARK-14391.
    
    (cherry picked from commit 73c20bf32524c2232febc8c4b12d5fa228347163)

commit be681557e65a2489db37f4e9a5f1defb4c84528c
Author: Imran Rashid <irashid@cloudera.com>
Date:   Wed Jul 13 11:32:37 2016 -0500

    CLOUDERA-BUILD. CDH-27213. Preview of SPARK-8425.
    
    Note that in this preview, blacklisting is turned off by default.
    
    (cherry picked from commit f01f64ab06130154709f2082fdf737c5e27b442b)

commit 6914ad6bf0f564b10bbe91d94272ee70d19b634a
Author: Imran Rashid <irashid@cloudera.com>
Date:   Wed Jun 22 08:35:41 2016 -0500

    [SPARK-15783][CORE] Fix Flakiness in BlacklistIntegrationSuite
    
    Three changes here -- first two were causing failures w/ BlacklistIntegrationSuite
    
    1. The testing framework didn't include the reviveOffers thread, so the test which involved delay scheduling might never submit offers late enough for the delay scheduling to kick in.  So added in the periodic revive offers, just like the real scheduler.
    
    2. `assertEmptyDataStructures` would occasionally fail, because it appeared there was still an active job.  This is because in DAGScheduler, the jobWaiter is notified of the job completion before the data structures are cleaned up.  Most of the time the test code that is waiting on the jobWaiter won't become active until after the data structures are cleared, but occasionally the race goes the other way, and the assertions fail.
    
    3. `DAGSchedulerSuite` was not stopping all the inner parts it was setting up, so each test was leaking a number of threads.  So we stop those parts too.
    
    4. Turns out that `assertMapOutputAvailable` is not terribly useful in this framework -- most of the places I was trying to use it suffer from some race.
    
    5. When there is an exception in the backend, try to improve the error msg a little bit.  Before the exception was printed to the console, but the test would fail w/ a timeout, and the logs wouldn't show anything.
    
    I ran all the tests in `BlacklistIntegrationSuite` 5k times and everything in `DAGSchedulerSuite` 1k times on my laptop.  Also I ran a full jenkins build with `BlacklistIntegrationSuite` 500 times and `DAGSchedulerSuite` 50 times, see https://github.com/apache/spark/pull/13548.  (I tried more times but jenkins timed out.)
    
    To check for more leaked threads, I added some code to dump the list of all threads at the end of each test in DAGSchedulerSuite, which is how I discovered the mapOutputTracker and eventLoop were leaking threads.  (I removed that code from the final pr, just part of the testing.)
    
    And I'll run Jenkins on this a couple of times to do one more check.
    
    Author: Imran Rashid <irashid@cloudera.com>
    
    Closes #13565 from squito/blacklist_extra_tests.

commit 4c0a3f62589a34baa28fd3407b4cd981db892a6c
Author: Imran Rashid <irashid@cloudera.com>
Date:   Mon Jun 6 12:53:11 2016 -0700

    [SPARK-15783][CORE] still some flakiness in these blacklist tests so ignore for now
    
    ## What changes were proposed in this pull request?
    
    There is still some flakiness in BlacklistIntegrationSuite, so turning it off for the moment to avoid breaking more builds -- will turn it back with more fixes.
    
    ## How was this patch tested?
    
    jenkins.
    
    Author: Imran Rashid <irashid@cloudera.com>
    
    Closes #13528 from squito/ignore_blacklist.

commit de8096096ff60a85ffa57277c295e8b4e0b67fb9
Author: Imran Rashid <irashid@cloudera.com>
Date:   Thu Jun 30 13:36:06 2016 -0500

    [SPARK-15865][CORE] Blacklist should not result in job hanging with less than 4 executors
    
    Before this change, when you turn on blacklisting with `spark.scheduler.executorTaskBlacklistTime`, but you have fewer than `spark.task.maxFailures` executors, you can end with a job "hung" after some task failures.
    
    Whenever a taskset is unable to schedule anything on resourceOfferSingleTaskSet, we check whether the last pending task can be scheduled on *any* known executor.  If not, the taskset (and any corresponding jobs) are failed.
    * Worst case, this is O(maxTaskFailures + numTasks).  But unless many executors are bad, this should be small
    * This does not fail as fast as possible -- when a task becomes unschedulable, we keep scheduling other tasks.  This is to avoid an O(numPendingTasks * numExecutors) operation
    * Also, it is conceivable this fails too quickly.  You may be 1 millisecond away from unblacklisting a place for a task to run, or acquiring a new executor.
    
    Added unit test which failed before the change, ran new test 5k times manually, ran all scheduler tests manually, and the full suite via jenkins.
    
    Author: Imran Rashid <irashid@cloudera.com>
    
    Closes #13603 from squito/progress_w_few_execs_and_blacklist.
    
    (cherry picked from commit fdf9f94f8c8861a00cd8415073f842b857c397f7)

commit 5f41d0325bbca394c6d5b0c7ed86b884e4b9259f
Author: Imran Rashid <irashid@cloudera.com>
Date:   Fri Jun 3 11:49:33 2016 -0500

    [SPARK-15714][CORE] Fix flaky o.a.s.scheduler.BlacklistIntegrationSuite
    
    ## What changes were proposed in this pull request?
    
    BlacklistIntegrationSuite (introduced by SPARK-10372) is a bit flaky because of some race conditions:
    1. Failed jobs might have non-empty results, because the resultHandler will be invoked for successful tasks (if there are task successes before failures)
    2. taskScheduler.taskIdToTaskSetManager must be protected by a lock on taskScheduler
    
    (1) has failed a handful of jenkins builds recently.  I don't think I've seen (2) in jenkins, but I've run into with some uncommitted tests I'm working on where there are lots more tasks.
    
    While I was in there, I also made an unrelated fix to `runningTasks`in the test framework -- there was a pointless `O(n)` operation to remove completed tasks, could be `O(1)`.
    
    ## How was this patch tested?
    
    I modified the o.a.s.scheduler.BlacklistIntegrationSuite to have it run the tests 1k times on my laptop.  It failed 11 times before this change, and none with it.  (Pretty sure all the failures were problem (1), though I didn't check all of them).
    
    Also the full suite of tests via jenkins.
    
    Author: Imran Rashid <irashid@cloudera.com>
    
    Closes #13454 from squito/SPARK-15714.
    
    (cherry picked from commit c2f0cb4f6380c500f9ba37b2429503b762204973)

commit 9049e593274bdaba22156a9c6891ea118cd6b9c2
Author: Imran Rashid <irashid@cloudera.com>
Date:   Wed Jun 15 16:44:18 2016 -0500

    [HOTFIX][CORE] fix flaky BasicSchedulerIntegrationTest
    
    ## What changes were proposed in this pull request?
    
    SPARK-15927 exacerbated a race in BasicSchedulerIntegrationTest, so it went from very unlikely to fairly frequent.  The issue is that stage numbering is not completely deterministic, but these tests treated it like it was.  So turn off the tests.
    
    ## How was this patch tested?
    
    on my laptop the test failed abotu 10% of the time before this change, and didn't fail in 500 runs after the change.
    
    Author: Imran Rashid <irashid@cloudera.com>
    
    Closes #13688 from squito/hotfix_basic_scheduler.
    
    (cherry picked from commit cafc696d095ae06dd64805574d55a19637743aa6)

commit f074ecfc5dea70d15f2b97071987a4f392ab2872
Author: Imran Rashid <irashid@cloudera.com>
Date:   Thu May 26 00:29:09 2016 -0500

    [SPARK-10372] [CORE] basic test framework for entire spark scheduler
    
    This is a basic framework for testing the entire scheduler.  The tests this adds aren't very interesting -- the point of this PR is just to setup the framework, to keep the initial change small, but it can be built upon to test more features (eg., speculation, killing tasks, blacklisting, etc.).
    
    Author: Imran Rashid <irashid@cloudera.com>
    
    Closes #8559 from squito/SPARK-10372-scheduler-integs.
    
    (cherry picked from commit dfc9fc02ccbceb09213c394177d54b9ca56b6f24)

commit 07873eab052c88581d9281103c24cf2872afd067
Author: Hemant Bhanawat <hemant@snappydata.io>
Date:   Sat Apr 16 23:43:32 2016 -0700

    [SPARK-13904][SCHEDULER] Add support for pluggable cluster manager
    
    This commit adds support for pluggable cluster manager. And also allows a cluster manager to clean up tasks without taking the parent process down.
    
    To plug a new external cluster manager, ExternalClusterManager trait should be implemented. It returns task scheduler and backend scheduler that will be used by SparkContext to schedule tasks. An external cluster manager is registered using the java.util.ServiceLoader mechanism (This mechanism is also being used to register data sources like parquet, json, jdbc etc.). This allows auto-loading implementations of ExternalClusterManager interface.
    
    Currently, when a driver fails, executors exit using system.exit. This does not bode well for cluster managers that would like to reuse the parent process of an executor. Hence,
    
      1. Moving system.exit to a function that can be overriden in subclasses of CoarseGrainedExecutorBackend.
      2. Added functionality of killing all the running tasks in an executor.
    
    ExternalClusterManagerSuite.scala was added to test this patch.
    
    Author: Hemant Bhanawat <hemant@snappydata.io>
    
    Closes #11723 from hbhanawat/pluggableScheduler.
    
    (cherry picked from commit af1f4da76268115c5a4cc3035d3236ad27f7240a)

commit b3e9731e07453056dc98c276b192033fe0fb1dd2
Author: Jenkins <dev-kitchen@cloudera.com>
Date:   Thu Aug 18 13:44:19 2016 -0700

    Updating Maven version to 5.10.0-SNAPSHOT

commit 9b720b7804e767798585b84283cfe0ad603e3490
Author: felixcheung <felixcheung_m@hotmail.com>
Date:   Sun Apr 24 22:51:18 2016 -0700

    [SPARK-14881] [PYTHON] [SPARKR] pyspark and sparkR shell default log level should match spark-shell/Scala
    
    Cloudera-only: includes certain parts of the fix for SPARK-13626 (8301fadd) which
    expose the needed APIs in Logging.scala.
    
    Change default logging to WARN for pyspark shell and sparkR shell for a much cleaner environment.
    
    Manually running pyspark and sparkR shell
    
    Author: felixcheung <felixcheung_m@hotmail.com>
    
    Closes #12648 from felixcheung/pylogging.
    
    (cherry picked from commit c752b6c5ec488b87c3aaaa86902dd4da9b4b406f)

commit 14d49ced7b0363280120c048f380bc975567a318
Author: Mark Grover <mark@apache.org>
Date:   Fri Aug 5 09:16:26 2016 -0700

    CLOUDERA-BUILD. CDH-40964. (SPARK-5847 preview) Make metrics namespace configurable.
    
    Preview of SPARK-5847.
    
    Note that CDH5 behaves slightly differently than Apache Spark. In Apache Spark and Cloudera Spark2, one would configure metrics namepsace setting `spark.metrics.namespace` to `${spark.app.name}` while in CDH5, expansion in Spark configuration is not supported and `spark.metrics.namespace` should simply be set to `spark.app.name`.
    
    Differences between upstream change and downstream change:
    MetricsConfig*.scala are the same.
    Tests are the same but set namespace to spark.app.name instead of ${spark.app.name}
    MetricsConfig.scala is probably the most different where we are not doing variable expansion.

commit 13445b55de8d41792dfdf5753e6f4a4d9f7eb516
Author: Marcelo Vanzin <vanzin@cloudera.com>
Date:   Fri Jul 15 18:16:44 2016 -0700

    CLOUDERA-BUILD. CDH-40989. Change default to stop NM on service init failure.

commit d66590d9df37bc36f35ce7add7f507e22eb2c1ec
Author: Marcelo Vanzin <vanzin@cloudera.com>
Date:   Thu Jul 14 09:42:32 2016 -0500

    [SPARK-16505][YARN] Optionally propagate error during shuffle service startup.
    
    This prevents the NM from starting when something is wrong, which would
    lead to later errors which are confusing and harder to debug.
    
    Added a unit test to verify startup fails if something is wrong.
    
    Author: Marcelo Vanzin <vanzin@cloudera.com>
    
    Closes #14162 from vanzin/SPARK-16505.
    
    (cherry picked from commit b7b5e17876f65c6644505c356f1a0db24ce1d142)

commit 65a4d0244241f4ae52f907f69775edd50d2ba5ac
Author: jerryshao <sshao@hortonworks.com>
Date:   Thu Jul 14 08:31:04 2016 -0500

    [SPARK-14963][MINOR][YARN] Fix typo in YarnShuffleService recovery file name
    
    ## What changes were proposed in this pull request?
    
    Due to the changes of [SPARK-14963](https://issues.apache.org/jira/browse/SPARK-14963), external shuffle recovery file name is changed mistakenly, so here change it back to the previous file name.
    
    This only affects the master branch, branch-2.0 is correct [here](https://github.com/apache/spark/blob/branch-2.0/common/network-yarn/src/main/java/org/apache/spark/network/yarn/YarnShuffleService.java#L195).
    
    ## How was this patch tested?
    
    N/A
    
    Author: jerryshao <sshao@hortonworks.com>
    
    Closes #14197 from jerryshao/fix-typo-file-name.
    
    (cherry picked from commit c4bc2ed844ea045d2e8218154690b5b2b023f1e5)
    (cherry picked from commit 5377df1dcb0b470d6d13fadf050c20ff0c9b717e)

commit 399e7d4d34795075485ab0c4a73630d030138bf9
Author: jerryshao <sshao@hortonworks.com>
Date:   Tue May 10 10:28:36 2016 -0500

    [SPARK-14963][YARN] Using recoveryPath if NM recovery is enabled
    
    From Hadoop 2.5+, Yarn NM supports NM recovery which using recovery path for auxiliary services such as spark_shuffle, mapreduce_shuffle. So here change to use this path install of NM local dir if NM recovery is enabled.
    
    Unit test + local test.
    
    Author: jerryshao <sshao@hortonworks.com>
    
    Closes #12994 from jerryshao/SPARK-14963.
    
    (cherry picked from commit aab99d31a927adfa9216dd14e76493a187b6d6e7)

commit 84a2b664f3bf8432f9c5849a85534c483e81d020
Author: Kousuke Saruta <sarutak@oss.nttdata.co.jp>
Date:   Fri May 20 15:50:06 2016 -0700

    [SPARK-15165] [SPARK-15205] [SQL] Introduce place holder for comments in generated code (branch-1.6)
    
    This PR introduce place holder for comment in generated code and the purpose is same for #12939 but much safer.
    
    Generated code to be compiled doesn't include actual comments but includes place holder instead.
    
    Place holders in generated code will be replaced with actual comments only at the time of logging.
    
    Also, this PR can resolve SPARK-15205.
    
    (Please explain how this patch was tested. E.g. unit tests, integration tests, manual tests)
    
    Added new test cases.
    
    Author: Kousuke Saruta <sarutak@oss.nttdata.co.jp>
    
    Closes #13230 from sarutak/SPARK-15165-branch-1.6.
    
    (cherry picked from commit 9a18115a82c8bdc4f6f50df2e968e5aba979f53b)

commit 25e0af7d4845648741879f70b9dc082729f9b4b4
Author: Imran Rashid <irashid@cloudera.com>
Date:   Mon Jun 27 16:38:03 2016 -0500

    [SPARK-16106][CORE] TaskSchedulerImpl should properly track executors added to existing hosts
    
    ## What changes were proposed in this pull request?
    
    TaskSchedulerImpl used to only set `newExecAvailable` when a new *host* was added, not when a new executor was added to an existing host.  It also didn't update some internal state tracking live executors until a task was scheduled on the executor.  This patch changes it to properly update as soon as it knows about a new executor.
    
    ## How was this patch tested?
    
    added a unit test, ran everything via jenkins.
    
    Author: Imran Rashid <irashid@cloudera.com>
    
    Closes #13826 from squito/SPARK-16106_executorByHosts.
    
    (cherry picked from commit c15b552dd547a129c7f0d082dab4eebbd64bee02)

commit b642471f2ed2f1a2b1f25ef72e941f7eef9ec8d0
Author: Subroto Sanyal <ssanyal@datameer.com>
Date:   Fri Jun 3 16:50:00 2016 -0700

    [SPARK-15754][YARN] Not letting the credentials containing hdfs delegation tokens to be added in current user credential.
    
    ## What changes were proposed in this pull request?
    The credentials are not added to the credentials of UserGroupInformation.getCurrentUser(). Further if the client has possibility to login using keytab then the updateDelegationToken thread is not started on client.
    
    ## How was this patch tested?
    ran dev/run-tests
    
    Author: Subroto Sanyal <ssanyal@datameer.com>
    
    Closes #13499 from subrotosanyal/SPARK-15754-save-ugi-from-changing.
    
    (cherry picked from commit 61d729abdaf9bdd54aea74a643828d570b036d87)

commit 56195ea08d49efa7362b74abc99cb5ec64f58b70
Author: Srinivasa Reddy Vundela <vsr@cloudera.com>
Date:   Fri May 27 13:10:01 2016 -0700

    Revert "CLOUDERA-BUILD CDH-27093 Checking for the required hadoop libraries before starting the spark"
    
    This reverts commit 165238c8629942b8c199737bb8bec8ff3bc2c5d0.

commit 0682457e8e481acd84eb11f7084e263a7ce4a487
Author: Robert Kanter <rkanter@cloudera.com>
Date:   Wed May 25 15:56:37 2016 -0700

    CLOUDERA-BUILD. CDH-40455. make py4j-0.9-src.zip and pyspark.zip available as Maven artifacts

commit 165238c8629942b8c199737bb8bec8ff3bc2c5d0
Author: Srinivasa Reddy Vundela <vsr@cloudera.com>
Date:   Tue Dec 1 15:04:36 2015 -0800

    CLOUDERA-BUILD CDH-27093 Checking for the required hadoop libraries before starting the spark
    
    (cherry picked from commit 2468d55923357f68eed8f5e9d41cbda4d9160950)

commit 711db37d01994536ad053ba2090c2d3adb9be3e9
Author: Jenkins <dev-kitchen@cloudera.com>
Date:   Mon May 16 14:00:58 2016 -0700

    Update to 5.9.0-SNAPSHOT on Mon May 16 14:00:09 PDT 2016
    
    JOB_NAME : 'Cut-Release-Branches'
    BUILD_NUMBER : '333'
    CODE_BRANCH : ''
    OLD_CDH_BRANCH : 'cdh5'
    
    Pushed to remote origin	git@github.sf.cloudera.com:CDH/spark.git (push)

commit 3d79645600af3f5c8f6df4cb4e00d81aca915440
Author: Lianhui Wang <lianhuiwang09@gmail.com>
Date:   Thu Apr 21 10:02:23 2016 -0700

    [SPARK-4452] [CORE] Shuffle data structures can starve others on the same thread for memory
    
    In #9241 It implemented a mechanism to call spill() on those SQL operators that support spilling if there is not enough memory for execution.
    But ExternalSorter and AppendOnlyMap in Spark core are not worked. So this PR make them benefit from #9241. Now when there is not enough memory for execution, it can get memory by spilling ExternalSorter and AppendOnlyMap in Spark core.
    
    add two unit tests for it.
    
    Author: Lianhui Wang <lianhuiwang09@gmail.com>
    
    Closes #10024 from lianhuiwang/SPARK-4452-2.
    
    (cherry picked from commit 4f369176b750c980682a6be468cefa8627769c72)

commit 663a00e250223492507af65086683afcbc018e9a
Author: Marcelo Vanzin <vanzin@cloudera.com>
Date:   Wed May 4 11:16:31 2016 -0700

    CLOUDERA-BUILD. CDH-39922. Distribute conf archive to executors also.
    
    This change is a partial backport of SPARK-14062. It makes sure
    executors also see the log4j configuration the user is using on
    the launcher node.

commit c4e1f54c4e6c121e00555cc289bda2395151c354
Author: Bjorn Jonsson <bjornjon@gmail.com>
Date:   Sun Mar 13 10:18:24 2016 +0000

    [SPARK-13810][CORE] Add Port Configuration Suggestions on Bind Exceptions
    
    ## What changes were proposed in this pull request?
    Currently, when a java.net.BindException is thrown, it displays the following message:
    
    java.net.BindException: Address already in use: Service '$serviceName' failed after 16 retries!
    
    This change adds port configuration suggestions to the BindException, for example, for the UI, it now displays
    
    java.net.BindException: Address already in use: Service 'SparkUI' failed after 16 retries! Consider explicitly setting the appropriate port for 'SparkUI' (for example spark.ui.port for SparkUI) to an available port or increasing spark.port.maxRetries.
    
    ## How was this patch tested?
    Manual tests
    
    Author: Bjorn Jonsson <bjornjon@gmail.com>
    
    Closes #11644 from bjornjon/master.
    
    (cherry picked from commit 515e4afbc7ec957609451ea75772d6ef1b914908)
    Signed-off-by: Sean Owen <sowen@cloudera.com>
    (cherry picked from commit 5e08db30d55a6be8996d9de452720f9a465ebcc5)

commit 3bd1fb739713daaf20051061dfd7baf31b34e0e7
Author: Sital Kedia <skedia@fb.com>
Date:   Tue Apr 12 16:10:07 2016 -0700

    [SPARK-14363] Fix executor OOM due to memory leak in the Sorter
    
    Fix memory leak in the Sorter. When the UnsafeExternalSorter spills the data to disk, it does not free up the underlying pointer array. As a result, we see a lot of executor OOM and also memory under utilization.
    This is a regression partially introduced in PR https://github.com/apache/spark/pull/9241
    
    Tested by running a job and observed around 30% speedup after this change.
    
    Author: Sital Kedia <skedia@fb.com>
    
    Closes #12285 from sitalkedia/executor_oom.
    
    (cherry picked from commit d187e7dea9540d26b7800de4eb79863ef5f574bf)
    Signed-off-by: Davies Liu <davies.liu@gmail.com>
    
    Conflicts:
    	core/src/main/java/org/apache/spark/shuffle/sort/ShuffleInMemorySorter.java
    	core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeInMemorySorter.java
    
    (cherry picked from commit 413d0600ed61990e657c97d50d1431a8cd1ab0ed)

commit 90ed1860641f6950a096eeb7ad5de80a30446d1f
Author: Arash Parsa <arash@ip-192-168-50-106.ec2.internal>
Date:   Thu Apr 21 11:29:24 2016 +0100

    [SPARK-14739][PYSPARK] Fix Vectors parser bugs
    
    ## What changes were proposed in this pull request?
    
    The PySpark deserialization has a bug that shows while deserializing all zero sparse vectors. This fix filters out empty string tokens before casting, hence properly stringified SparseVectors successfully get parsed.
    
    ## How was this patch tested?
    
    Standard unit-tests similar to other methods.
    
    Author: Arash Parsa <arash@ip-192-168-50-106.ec2.internal>
    Author: Arash Parsa <arashpa@gmail.com>
    Author: Vishnu Prasad <vishnu667@gmail.com>
    Author: Vishnu Prasad S <vishnu667@gmail.com>
    
    Closes #12516 from arashpa/SPARK-14739.
    
    (cherry picked from commit 2b8906c43760591f2e2da99bf0e34fa1bb63bfd1)
    Signed-off-by: Sean Owen <sowen@cloudera.com>
    (cherry picked from commit 1cda10b4c83b9bbc70f6d9bf500b1bcc99f842a0)

commit 784ab7e20881da2666c20766f5fbd0b5883cfc05
Author: Ryan Blue <blue@apache.org>
Date:   Wed Apr 20 11:26:42 2016 +0100

    [SPARK-14679][UI] Fix UI DAG visualization OOM.
    
    ## What changes were proposed in this pull request?
    
    The DAG visualization can cause an OOM when generating the DOT file.
    This happens because clusters are not correctly deduped by a contains
    check because they use the default equals implementation. This adds a
    working equals implementation.
    
    ## How was this patch tested?
    
    This adds a test suite that checks the new equals implementation.
    
    Author: Ryan Blue <blue@apache.org>
    
    Closes #12437 from rdblue/SPARK-14679-fix-ui-oom.
    
    (cherry picked from commit a3451119d951949f24f3a4c5e33a5daea615dfed)
    Signed-off-by: Sean Owen <sowen@cloudera.com>
    (cherry picked from commit 17b1384431dd39e361bc3e88dffbe7ec6b4cc3d5)

commit 3c10ce2071cec4a766c8a93df643964d138e74d6
Author: Zhang, Liye <liye.zhang@intel.com>
Date:   Mon Apr 11 10:06:57 2016 -0700

    [SPARK-14290] [SPARK-13352] [CORE] [BACKPORT-1.6] avoid significant memory copy in Netty's tran…
    
    ## What changes were proposed in this pull request?
    When netty transfer data that is not `FileRegion`, data will be in format of `ByteBuf`, If the data is large, there will occur significant performance issue because there is memory copy underlying in `sun.nio.ch.IOUtil.write`, the CPU is 100% used, and network is very low.
    
    In this PR, if data size is large, we will split it into small chunks to call `WritableByteChannel.write()`, so that avoid wasting of memory copy. Because the data can't be written within a single write, and it will call `transferTo` multiple times.
    
    ## How was this patch tested?
    Spark unit test and manual test.
    Manual test:
    `sc.parallelize(Array(1,2,3),3).mapPartitions(a=>Array(new Array[Double](1024 * 1024 * 50)).iterator).reduce((a,b)=> a).length`
    
    For more details, please refer to [SPARK-14290](https://issues.apache.org/jira/browse/SPARK-14290)
    
    Author: Zhang, Liye <liye.zhang@intel.com>
    
    Closes #12296 from liyezhang556520/apache-branch-1.6-spark-14290.
    
    (cherry picked from commit baf29854eaa41976c75ecd2c472806c4a1c02c2a)

commit 7db90452957e0194a052fa2480b64e6d1714c386
Author: Zhang, Liye <liye.zhang@intel.com>
Date:   Thu Mar 31 20:17:52 2016 -0700

    [SPARK-14242][CORE][NETWORK] avoid copy in compositeBuffer for frame decoder
    
    ## What changes were proposed in this pull request?
    In this patch, we set the initial `maxNumComponents` to `Integer.MAX_VALUE` instead of the default size ( which is 16) when allocating `compositeBuffer` in `TransportFrameDecoder` because `compositeBuffer` will introduce too many memory copies underlying if `compositeBuffer` is with default `maxNumComponents` when the frame size is large (which result in many transport messages). For details, please refer to [SPARK-14242](https://issues.apache.org/jira/browse/SPARK-14242).
    
    ## How was this patch tested?
    spark unit tests and manual tests.
    For manual tests, we can reproduce the performance issue with following code:
    `sc.parallelize(Array(1,2,3),3).mapPartitions(a=>Array(new Array[Double](1024 * 1024 * 50)).iterator).reduce((a,b)=> a).length`
    It's easy to see the performance gain, both from the running time and CPU usage.
    
    Author: Zhang, Liye <liye.zhang@intel.com>
    
    Closes #12038 from liyezhang556520/spark-14242.
    
    (cherry picked from commit 663a492f0651d757ea8e5aeb42107e2ece429613)

commit 333477ee687d8d58256da691950facf7f4bf25e8
Author: Alex Bozarth <ajbozart@us.ibm.com>
Date:   Fri Mar 4 17:04:09 2016 -0600

    [SPARK-13459][WEB UI] Separate Alive and Dead Executors in Executor Totals Table
    
    Now that dead executors are shown in the executors table (#10058) the totals table is updated to include the separate totals for alive and dead executors as well as the current total, as originally discussed in #10668
    
    Manually verified by running the Standalone Web UI in the latest Safari and Firefox ESR
    
    Author: Alex Bozarth <ajbozart@us.ibm.com>
    
    Closes #11381 from ajbozarth/spark13459.
    
    (cherry picked from commit 5f42c28b119b79c0ea4910c478853d451cd1a967)

commit 46a9a2d00ec1bdb66af7a47d680b4ea7ef8051c9
Author: Lianhui Wang <lianhuiwang09@gmail.com>
Date:   Tue Feb 23 11:08:39 2016 -0800

    [SPARK-7729][UI] Executor which has been killed should also be displayed on Executor Tab
    
    andrewor14 squito Dead Executors should also be displayed on Executor Tab.
    as following:
    ![image](https://cloud.githubusercontent.com/assets/545478/11492707/ae55d7f6-982b-11e5-919a-b62cd84684b2.png)
    
    Author: Lianhui Wang <lianhuiwang09@gmail.com>
    
    This patch had conflicts when merged, resolved by
    Committer: Andrew Or <andrew@databricks.com>
    
    Closes #10058 from lianhuiwang/SPARK-7729.
    
    (cherry picked from commit 9f4263392e492b5bc0acecec2712438ff9a257b7)

commit a9338d76350873933056ba444769057fe017d23d
Author: Alex Bozarth <ajbozart@us.ibm.com>
Date:   Wed Feb 3 15:53:10 2016 -0800

    [SPARK-3611][WEB UI] Show number of cores for each executor in application web UI
    
    Added a Cores column in the Executors UI
    
    Author: Alex Bozarth <ajbozart@us.ibm.com>
    
    Closes #11039 from ajbozarth/spark3611.
    
    (cherry picked from commit 3221eddb8f9728f65c579969a3a88baeeb7577a9)

commit d8ba8d8bce691fdca5e666adf269734781b116c5
Author: Alex Bozarth <ajbozart@us.ibm.com>
Date:   Mon Jan 25 14:42:44 2016 -0600

    [SPARK-12149][WEB UI] Executor UI improvement suggestions - Color UI
    
    Added color coding to the Executors page for Active Tasks, Failed Tasks, Completed Tasks and Task Time.
    
    Active Tasks is shaded blue with it's range based on percentage of total cores used.
    Failed Tasks is shaded red ranging over the first 10% of total tasks failed
    Completed Tasks is shaded green ranging over 10% of total tasks including failed and active tasks, but only when there are active or failed tasks on that executor.
    Task Time is shaded red when GC Time goes over 10% of total time with it's range directly corresponding to the percent of total time.
    
    Author: Alex Bozarth <ajbozart@us.ibm.com>
    
    Closes #10154 from ajbozarth/spark12149.
    
    (cherry picked from commit c037d25482ea63430fb42bfd86124c268be5a4a4)

commit d4c0896fb5e0935be280691412a0b42c7f3ac6fd
Author: Alex Bozarth <ajbozart@us.ibm.com>
Date:   Fri Jan 15 16:03:21 2016 -0600

    [SPARK-12716][WEB UI] Add a TOTALS row to the Executors Web UI
    
    Added a Totals table to the top of the page to display the totals of each applicable column in the executors table.
    
    Old Description:
    ~~Created a TOTALS row containing the totals of each column in the executors UI. By default the TOTALS row appears at the top of the table. When a column is sorted the TOTALS row will always sort to either the top or bottom of the table.~~
    
    Author: Alex Bozarth <ajbozart@us.ibm.com>
    
    Closes #10668 from ajbozarth/spark12716.
    
    (cherry picked from commit 61c45876fb532cdb7278dea48cc141208b63737c)

commit 6c31b0ce0396dcf1f1c9f0a698dca38c353b60e4
Author: Imran Rashid <irashid@cloudera.com>
Date:   Mon Mar 28 15:57:51 2016 -0500

    CLOUDERA-BUILD. [CDH-38800]  Turn off bogus warnings about deprecated memory configuration options

commit 19c790ec9f61b43c91a960851cc1633fbadab350
Author: Imran Rashid <irashid@cloudera.com>
Date:   Wed Apr 6 15:18:46 2016 -0500

    CLOUDERA-BUILD. alternate snappy version on a mac.
    
    Since the CDH snappy version doesn't work nicely on a mac, use
    a different version just to let you do local development.

commit 03d7787530ac837db924fb8c1a4e6ba2e26ca6e2
Author: Misha Dmitriev <misha@cloudera.com>
Date:   Wed Apr 13 16:17:15 2016 -0700

    [SPARK-13780][SQL] Add missing dependency to build.
    
    This is needed to avoid odd compiler errors when building just the
    sql package with maven, because of odd interactions between scalac
    and shaded classes.
    
    Author: Marcelo Vanzin <vanzin@cloudera.com>
    
    Closes #11640 from vanzin/SPARK-13780.
    
    (cherry picked from commit 99b7187c2dce4c73829b9b32de80b02a053763cc)

commit cead9b694d3b8a10163dce1c8d9c07743c2f47a2
Author: Marcelo Vanzin <vanzin@cloudera.com>
Date:   Fri Apr 8 10:18:12 2016 -0700

    CLOUDERA-BUILD. Override APACHE_MIRROR for post-commit builds.

commit f5454fa1645f64d7a0bbfc0b59bc5495ad5540a9
Author: Mark Grover <mark@apache.org>
Date:   Fri Apr 8 10:10:10 2016 -0700

    [SPARK-14477][BUILD] Allow custom mirrors for downloading artifacts in build/mvn
    
    Allows to override locations for downloading Apache and Typesafe artifacts in build/mvn script.
    
    By running script like
    ````
    rm -rf build/apache-maven*
    rm -rf build/zinc-*
    rm -rf build/scala-*
    
    ...
    
    build/mvn -Pyarn -Phadoop-2.4 -Dhadoop.version=2.6.0 package
    
    APACHE_MIRROR=http://mirror.infra.cloudera.com/apache/ build/mvn -Pyarn -Phadoop-2.4 -Dhadoop.version=2.6.0 package
    
    APACHE_MIRROR=http://mirror.infra.cloudera.com/apache build/mvn -Pyarn -Phadoop-2.4 -Dhadoop.version=2.6.0 package
    
    APACHE_MIRROR=xyz build/mvn -Pyarn -Phadoop-2.4 -Dhadoop.version=2.6.0 package
    ````
    
    Author: Mark Grover <mark@apache.org>
    
    Closes #12250 from markgrover/spark-14477.
    
    (cherry picked from commit a9b630f42ac0c6be3437f206beddaf0ef737f5c8)

commit a899194ff2dfa29cd3aa0c76cf49db9ee3a1ade2
Author: Marcelo Vanzin <vanzin@cloudera.com>
Date:   Wed Mar 30 15:13:47 2016 -0700

    CLOUDERA-BUILD. Increase some test timeouts.

commit 7c720b13d482025eac2e4d7cecef9a40f4b45eec
Author: Marcelo Vanzin <vanzin@cloudera.com>
Date:   Mon Mar 28 14:55:10 2016 -0700

    CLOUDERA-BUILD. CDH-38784. Fix ExternalSorter when shuffle encryption is enabled.
    
    This fixes an issue in the original change for SPARK-5682 that was backported
    to CDH 5.7.0.

commit 98987fd888a9b0cb91dfc1e2d6c32aeab7a1eb4f
Author: nfraison <nfraison@yahoo.fr>
Date:   Mon Mar 28 14:10:25 2016 -0700

    [SPARK-13622][YARN] Issue creating level db for YARN shuffle service
    
    ## What changes were proposed in this pull request?
    This patch will ensure that we trim all path set in yarn.nodemanager.local-dirs and that the the scheme is well removed so the level db can be created.
    
    ## How was this patch tested?
    manual tests.
    
    Author: nfraison <nfraison@yahoo.fr>
    
    Closes #11475 from ashangit/level_db_creation_issue.
    
    (cherry picked from commit ff3bea38ed2ac8dac5832f0bf8eac70192a512ef)
    (cherry picked from commit 504b992623a42f23f5ab305bc8908c8111b6f258)

commit 43d947ed6f3f09e374dc4f4f4dc29a464c5d2397
Author: Misha Dmitriev <root@misha-trusty.gce.cloudera.com>
Date:   Thu Mar 17 16:04:33 2016 -0700

    CLOUDERA-BUILD. Make clean optional in the maven build of spark.

commit 28e182c0d555b6ef59fc6fc0bc52492b22ade68f
Author: Shixiong Zhu <shixiong@databricks.com>
Date:   Tue Jan 26 17:24:40 2016 -0800

    [SPARK-12614][CORE] Don't throw non fatal exception from ask
    
    Right now RpcEndpointRef.ask may throw exception in some corner cases, such as calling ask after stopping RpcEnv. It's better to avoid throwing exception from RpcEndpointRef.ask. We can send the exception to the future for `ask`.
    
    Author: Shixiong Zhu <shixiong@databricks.com>
    
    Closes #10568 from zsxwing/send-ask-fail.
    
    (cherry picked from commit 22662b241629b56205719ede2f801a476e10a3cd)

commit 14bb96619ab24ee9f0ccfe1767444155d13d031c
Author: Shixiong Zhu <shixiong@databricks.com>
Date:   Thu Mar 3 22:53:07 2016 -0800

    [SPARK-13652][CORE] Copy ByteBuffer in sendRpcSync as it will be recycled
    
    ## What changes were proposed in this pull request?
    
    `sendRpcSync` should copy the response content because the underlying buffer will be recycled and reused.
    
    ## How was this patch tested?
    
    Jenkins unit tests.
    
    Author: Shixiong Zhu <shixiong@databricks.com>
    
    Closes #11499 from zsxwing/SPARK-13652.
    
    (cherry picked from commit 465c665db1dc65e3b02c584cf7f8d06b24909b0c)
    Signed-off-by: Shixiong Zhu <shixiong@databricks.com>
    (cherry picked from commit 51c676e46c9be28aa9ac37fda45482b38f2eb1d5)

commit 9dd403829bbab9d3afe872697a54146877a7b451
Author: Mark Grover <mark@apache.org>
Date:   Thu Mar 3 22:04:26 2016 -0800

    CLOUDERA-BUILD. Disable docker tests in CDH

commit 62e61d516ef4e16201a380160990177de0078806
Author: Mark Grover <mark@apache.org>
Date:   Fri Feb 26 13:27:58 2016 -0800

    CLOUDERA-BUILD. CDH-36918: Run unit tests in Spark's post-commit hook

commit a258d47d34efab869dabeb013710964804966c7b
Author: Marcelo Vanzin <vanzin@cloudera.com>
Date:   Fri Feb 26 11:07:57 2016 -0800

    CLOUDERA-BUILD. CDH-37735. Add ivysettings.xml to make Hive tests happy.
    
    The code for HIVE-9664 depends on ivysettings.xml being available
    in the system class path when HIVE_HOME is not defined, and for some
    reason that doesn't happen during unit tests.

commit 45c5e66aa4ee610e920e27ec8ead6ebacba03a44
Author: Marcelo Vanzin <vanzin@cloudera.com>
Date:   Wed Feb 24 15:30:39 2016 -0800

    CLOUDERA-BUILD. Preview of "[SPARK-13478] [yarn] Use real user when fetching delegation tokens.".
    
    The Hive client library is not smart enough to notice that the current
    user is a proxy user; so when using a proxy user, it fails to fetch
    delegation tokens from the metastore because of a missing kerberos
    TGT for the current user.
    
    To fix it, just run the code that fetches the delegation token as the
    real logged in user.
    
    Tested on a kerberos cluster both submitting normally and with a proxy
    user; Hive and HBase tokens are retrieved correctly in both cases.

commit a8e9c420d9318e9ab0679aeb8a0853ea86875543
Author: Daniel Jalova <djalova@us.ibm.com>
Date:   Wed Feb 24 12:15:11 2016 +0000

    [SPARK-12759][Core][Spark should fail fast if --executor-memory is too small for spark to start]
    
    Added an exception to be thrown in UnifiedMemoryManager.scala if the configuration given for executor memory is too low. Also modified the exception message thrown when driver memory is too low.
    
    This patch was tested manually by passing in config options to Spark shell. I also added a test in UnifiedMemoryManagerSuite.scala
    
    Author: Daniel Jalova <djalova@us.ibm.com>
    
    Closes #11255 from djalova/SPARK-12759.
    
    (cherry picked from commit bcfd55fa982b24184c07fcd4ccdd55dcf6465bf4)

commit ea69fe8b20b611a46168c7b33a4e35876fb46b35
Author: Shixiong Zhu <shixiong@databricks.com>
Date:   Wed Feb 24 13:35:36 2016 +0000

    [SPARK-13390][SQL][BRANCH-1.6] Fix the issue that Iterator.map().toSeq is not Serializable
    
    ## What changes were proposed in this pull request?
    
    `scala.collection.Iterator`'s methods (e.g., map, filter) will return an `AbstractIterator` which is not Serializable. E.g.,
    ```Scala
    scala> val iter = Array(1, 2, 3).iterator.map(_ + 1)
    iter: Iterator[Int] = non-empty iterator
    
    scala> println(iter.isInstanceOf[Serializable])
    false
    ```
    If we call something like `Iterator.map(...).toSeq`, it will create a `Stream` that contains a non-serializable `AbstractIterator` field and make the `Stream` be non-serializable.
    
    This PR uses `toArray` instead of `toSeq` to fix such issue in `def createDataFrame(data: java.util.List[_], beanClass: Class[_]): DataFrame`.
    
    ## How was the this patch tested?
    
    Jenkins tests.
    
    Author: Shixiong Zhu <shixiong@databricks.com>
    
    Closes #11334 from zsxwing/SPARK-13390.
    
    (cherry picked from commit 06f4fce29227f9763d9f9abff6e7459542dce261)

commit c9adc22a2436589ae0c1d280aa988227b01838bc
Author: Xiangrui Meng <meng@databricks.com>
Date:   Mon Feb 22 23:54:21 2016 -0800

    [SPARK-13355][MLLIB] replace GraphImpl.fromExistingRDDs by Graph.apply
    
    `GraphImpl.fromExistingRDDs` expects preprocessed vertex RDD as input. We call it in LDA without validating this requirement. So it might introduce errors. Replacing it by `Graph.apply` would be safer and more proper because it is a public API. The tests still pass. So maybe it is safe to use `fromExistingRDDs` here (though it doesn't seem so based on the implementation) or the test cases are special. jkbradley ankurdave
    
    Author: Xiangrui Meng <meng@databricks.com>
    
    Closes #11226 from mengxr/SPARK-13355.
    
    (cherry picked from commit 764ca18037b6b1884fbc4be9a011714a81495020)
    Signed-off-by: Xiangrui Meng <meng@databricks.com>
    (cherry picked from commit 0784e02fd438e5fa2e6639d6bba114fa647dad23)

commit 85b13d33bc86897f90df127727845f3e59006545
Author: Earthson Lu <Earthson.Lu@gmail.com>
Date:   Mon Feb 22 23:40:36 2016 -0800

    [SPARK-12746][ML] ArrayType(_, true) should also accept ArrayType(_, false) fix for branch-1.6
    
    https://issues.apache.org/jira/browse/SPARK-13359
    
    Author: Earthson Lu <Earthson.Lu@gmail.com>
    
    Closes #11237 from Earthson/SPARK-13359.
    
    (cherry picked from commit d31854da5155550f4e9c5e717c92dfec87d0ff6a)

commit 398540d36b0aeb311aecdbdc12b25c9ecbc06568
Author: Shixiong Zhu <shixiong@databricks.com>
Date:   Mon Feb 22 17:42:30 2016 -0800

    [SPARK-13298][CORE][UI] Escape "label" to avoid DAG being broken by some special character
    
    ## What changes were proposed in this pull request?
    
    When there are some special characters (e.g., `"`, `\`) in `label`, DAG will be broken. This patch just escapes `label` to avoid DAG being broken by some special characters
    
    ## How was the this patch tested?
    
    Jenkins tests
    
    Author: Shixiong Zhu <shixiong@databricks.com>
    
    Closes #11309 from zsxwing/SPARK-13298.
    
    (cherry picked from commit a11b3995190cb4a983adcc8667f7b316cce18d24)
    Signed-off-by: Andrew Or <andrew@databricks.com>
    (cherry picked from commit 85e6a2205d4549c81edbc2238fd15659120cee78)

commit 83d57bc2801144bf087aa880939c2307e3633db3
Author: Sean Owen <sowen@cloudera.com>
Date:   Thu Feb 18 12:14:30 2016 -0800

    [SPARK-13371][CORE][STRING] TaskSetManager.dequeueSpeculativeTask compares Option and String directly.
    
    ## What changes were proposed in this pull request?
    
    Fix some comparisons between unequal types that cause IJ warnings and in at least one case a likely bug (TaskSetManager)
    
    ## How was the this patch tested?
    
    Running Jenkins tests
    
    Author: Sean Owen <sowen@cloudera.com>
    
    Closes #11253 from srowen/SPARK-13371.
    
    (cherry picked from commit 78562535feb6e214520b29e0bbdd4b1302f01e93)
    Signed-off-by: Andrew Or <andrew@databricks.com>
    (cherry picked from commit 16f35c4c6e7e56bdb1402eab0877da6e8497cb3f)

commit 5e132060d96de8a3ad6e7979f08903359a3d847f
Author: Christopher C. Aycock <chris@chrisaycock.com>
Date:   Wed Feb 17 11:24:18 2016 -0800

    [SPARK-13350][DOCS] Config doc updated to state that PYSPARK_PYTHON's default is "python2.7"
    
    Author: Christopher C. Aycock <chris@chrisaycock.com>
    
    Closes #11239 from chrisaycock/master.
    
    (cherry picked from commit a7c74d7563926573c01baf613708a0f105a03e57)
    Signed-off-by: Josh Rosen <joshrosen@databricks.com>
    (cherry picked from commit 66106a660149607348b8e51994eb2ce29d67abc0)

commit 7cfb53ebc12143134133820e9867cf772ffe50f5
Author: Sital Kedia <skedia@fb.com>
Date:   Tue Feb 16 22:27:34 2016 -0800

    [SPARK-13279] Remove O(n^2) operation from scheduler.
    
    This commit removes an unnecessary duplicate check in addPendingTask that meant
    that scheduling a task set took time proportional to (# tasks)^2.
    
    Author: Sital Kedia <skedia@fb.com>
    
    Closes #11175 from sitalkedia/fix_stuck_driver.
    
    (cherry picked from commit 1e1e31e03df14f2e7a9654e640fb2796cf059fe0)
    Signed-off-by: Kay Ousterhout <kayousterhout@gmail.com>
    (cherry picked from commit 98354cae984e3719a49050e7a6aa75dae78b12bb)

commit 911fd8c765caff779322bca7ed5970d081658476
Author: JeremyNixon <jnixon2@gmail.com>
Date:   Mon Feb 15 09:25:13 2016 +0000

    [SPARK-13312][MLLIB] Update java train-validation-split example in ml-guide
    
    Response to JIRA https://issues.apache.org/jira/browse/SPARK-13312.
    
    This contribution is my original work and I license the work to this project.
    
    Author: JeremyNixon <jnixon2@gmail.com>
    
    Closes #11199 from JeremyNixon/update_train_val_split_example.
    
    (cherry picked from commit adb548365012552e991d51740bfd3c25abf0adec)
    Signed-off-by: Sean Owen <sowen@cloudera.com>
    (cherry picked from commit 71f53edc0e39bc907755153b9603be8c6fcc1d93)

commit 3d11b6add4b6ae6b66002b29bd83390fffff0992
Author: Amit Dev <amitdev@gmail.com>
Date:   Sun Feb 14 11:41:27 2016 +0000

    [SPARK-13300][DOCUMENTATION] Added pygments.rb dependancy
    
    Looks like pygments.rb gem is also required for jekyll build to work. At least on Ubuntu/RHEL I could not do build without this dependency. So added this to steps.
    
    Author: Amit Dev <amitdev@gmail.com>
    
    Closes #11180 from amitdev/master.
    
    (cherry picked from commit 331293c30242dc43e54a25171ca51a1c9330ae44)
    Signed-off-by: Sean Owen <sowen@cloudera.com>
    (cherry picked from commit ec40c5a59fe45e49496db6e0082ddc65c937a857)

commit d1276706191ee634809463b512b41f9f5d5594f3
Author: Liang-Chi Hsieh <viirya@gmail.com>
Date:   Sat Feb 13 15:56:20 2016 -0800

    [SPARK-12363][MLLIB] Remove setRun and fix PowerIterationClustering failed test
    
    JIRA: https://issues.apache.org/jira/browse/SPARK-12363
    
    This issue is pointed by yanboliang. When `setRuns` is removed from PowerIterationClustering, one of the tests will be failed. I found that some `dstAttr`s of the normalized graph are not correct values but 0.0. By setting `TripletFields.All` in `mapTriplets` it can work.
    
    Author: Liang-Chi Hsieh <viirya@gmail.com>
    Author: Xiangrui Meng <meng@databricks.com>
    
    Closes #10539 from viirya/fix-poweriter.
    
    (cherry picked from commit e3441e3f68923224d5b576e6112917cf1fe1f89a)
    Signed-off-by: Xiangrui Meng <meng@databricks.com>
    (cherry picked from commit 107290c94312524bfc4560ebe0de268be4ca56af)

commit 9f4014fe3569f91c9ed0aaf1d18e1a84c3db1206
Author: markpavey <mark.pavey@thefilter.com>
Date:   Sat Feb 13 08:39:43 2016 +0000

    [SPARK-13142][WEB UI] Problem accessing Web UI /logPage/ on Microsoft Windows
    
    Due to being on a Windows platform I have been unable to run the tests as described in the "Contributing to Spark" instructions. As the change is only to two lines of code in the Web UI, which I have manually built and tested, I am submitting this pull request anyway. I hope this is OK.
    
    Is it worth considering also including this fix in any future 1.5.x releases (if any)?
    
    I confirm this is my own original work and license it to the Spark project under its open source license.
    
    Author: markpavey <mark.pavey@thefilter.com>
    
    Closes #11135 from markpavey/JIRA_SPARK-13142_WindowsWebUILogFix.
    
    (cherry picked from commit 374c4b2869fc50570a68819cf0ece9b43ddeb34b)
    Signed-off-by: Sean Owen <sowen@cloudera.com>
    (cherry picked from commit 93a55f3df3c9527ecf4143cb40ac7212bc3a975a)

commit 5b874bd225f2fd359dbe546c52ccf391da792c86
Author: Tommy YU <tummyyu@163.com>
Date:   Thu Feb 11 18:38:49 2016 -0800

    [SPARK-13153][PYSPARK] ML persistence failed when handle no default value parameter
    
    Fix this defect by check default value exist or not.
    
    yanboliang Please help to review.
    
    Author: Tommy YU <tummyyu@163.com>
    
    Closes #11043 from Wenpei/spark-13153-handle-param-withnodefaultvalue.
    
    (cherry picked from commit d3e2e202994e063856c192e9fdd0541777b88e0e)
    Signed-off-by: Xiangrui Meng <meng@databricks.com>
    (cherry picked from commit 18661a2bb527adbd01e98158696a16f6d8162411)

commit f9a73a1daa79e17d6e9263747f45cb20c79ab3d5
Author: sethah <seth.hendrickson16@gmail.com>
Date:   Thu Feb 11 16:42:44 2016 -0800

    [SPARK-13047][PYSPARK][ML] Pyspark Params.hasParam should not throw an error
    
    Pyspark Params class has a method `hasParam(paramName)` which returns `True` if the class has a parameter by that name, but throws an `AttributeError` otherwise. There is not currently a way of getting a Boolean to indicate if a class has a parameter. With Spark 2.0 we could modify the existing behavior of `hasParam` or add an additional method with this functionality.
    
    In Python:
    ```python
    from pyspark.ml.classification import NaiveBayes
    nb = NaiveBayes()
    print nb.hasParam("smoothing")
    print nb.hasParam("notAParam")
    ```
    produces:
    > True
    > AttributeError: 'NaiveBayes' object has no attribute 'notAParam'
    
    However, in Scala:
    ```scala
    import org.apache.spark.ml.classification.NaiveBayes
    val nb  = new NaiveBayes()
    nb.hasParam("smoothing")
    nb.hasParam("notAParam")
    ```
    produces:
    > true
    > false
    
    cc holdenk
    
    Author: sethah <seth.hendrickson16@gmail.com>
    
    Closes #10962 from sethah/SPARK-13047.
    
    (cherry picked from commit b35467388612167f0bc3d17142c21a406f6c620d)
    Signed-off-by: Xiangrui Meng <meng@databricks.com>
    (cherry picked from commit 9d45ec466a4067bb2d0b59ff1174bec630daa7b1)

commit c53855fa3a48a1faeff13a9e0d6264222c3a8f7a
Author: Yu ISHIKAWA <yuu.ishikawa@gmail.com>
Date:   Thu Feb 11 15:00:23 2016 -0800

    [SPARK-13265][ML] Refactoring of basic ML import/export for other file system besides HDFS
    
    jkbradley I tried to improve the function to export a model. When I tried to export a model to S3 under Spark 1.6, we couldn't do that. So, it should offer S3 besides HDFS. Can you review it when you have time? Thanks!
    
    Author: Yu ISHIKAWA <yuu.ishikawa@gmail.com>
    
    Closes #11151 from yu-iskw/SPARK-13265.
    
    (cherry picked from commit efb65e09bcfa4542348f5cd37fe5c14047b862e5)
    Signed-off-by: Xiangrui Meng <meng@databricks.com>
    (cherry picked from commit 91a5ca5e84497c37de98c194566a568117332710)

commit 27a1f4b7945b772d662d3dd2b3e6f58b385efef3
Author: raela <raela@databricks.com>
Date:   Wed Feb 10 17:00:54 2016 -0800

    [SPARK-13274] Fix Aggregator Links on GroupedDataset Scala API
    
    Update Aggregator links to point to #org.apache.spark.sql.expressions.Aggregator
    
    Author: raela <raela@databricks.com>
    
    Closes #11158 from raelawang/master.
    
    (cherry picked from commit 719973b05ef6d6b9fbb83d76aebac6454ae84fad)
    Signed-off-by: Reynold Xin <rxin@databricks.com>
    (cherry picked from commit b57fac576f0033e8b43a89b4ada29901199aa29b)

commit 37222a4c453ea1f76db09fc01feb9d5c6239e3cb
Author: Josh Rosen <joshrosen@databricks.com>
Date:   Wed Feb 10 11:02:41 2016 -0800

    [SPARK-12921] Fix another non-reflective TaskAttemptContext access in SpecificParquetRecordReaderBase
    
    This is a minor followup to #10843 to fix one remaining place where we forgot to use reflective access of TaskAttemptContext methods.
    
    Author: Josh Rosen <joshrosen@databricks.com>
    
    Closes #11131 from JoshRosen/SPARK-12921-take-2.
    
    (cherry picked from commit 93f1d91755475a242456fe06e57bfca10f4d722f)

commit 0302717e34f9a4bb60bfc08f8caa778e5b55e2b1
Author: Liang-Chi Hsieh <viirya@gmail.com>
Date:   Tue Feb 9 17:10:55 2016 -0800

    [SPARK-10524][ML] Use the soft prediction to order categories' bins
    
    JIRA: https://issues.apache.org/jira/browse/SPARK-10524
    
    Currently we use the hard prediction (`ImpurityCalculator.predict`) to order categories' bins. But we should use the soft prediction.
    
    Author: Liang-Chi Hsieh <viirya@gmail.com>
    Author: Liang-Chi Hsieh <viirya@appier.com>
    Author: Joseph K. Bradley <joseph@databricks.com>
    
    Closes #8734 from viirya/dt-soft-centroids.
    
    (cherry picked from commit 9267bc68fab65c6a798e065a1dbe0f5171df3077)
    Signed-off-by: Joseph K. Bradley <joseph@databricks.com>
    (cherry picked from commit 89818cbf808137201d2558eaab312264d852cf00)

commit d1ad4f7191f633ed59d072ba342e16c0e3077ac9
Author: Davies Liu <davies@databricks.com>
Date:   Mon Feb 8 12:08:58 2016 -0800

    [SPARK-13210][SQL] catch OOM when allocate memory and expand array
    
    There is a bug when we try to grow the buffer, OOM is ignore wrongly (the assert also skipped by JVM), then we try grow the array again, this one will trigger spilling free the current page, the current record we inserted will be invalid.
    
    The root cause is that JVM has less free memory than MemoryManager thought, it will OOM when allocate a page without trigger spilling. We should catch the OOM, and acquire memory again to trigger spilling.
    
    And also, we could not grow the array in `insertRecord` of `InMemorySorter` (it was there just for easy testing).
    
    Author: Davies Liu <davies@databricks.com>
    
    Closes #11095 from davies/fix_expand.
    
    (cherry picked from commit 9b30096227263f77fc67ed8f12fb2911c3256774)

commit 1430d0938d72665bdc24c94fa0a53e1a3fa96c4a
Author: Bill Chambers <bill@databricks.com>
Date:   Fri Feb 5 14:35:39 2016 -0800

    [SPARK-13214][DOCS] update dynamicAllocation documentation
    
    Author: Bill Chambers <bill@databricks.com>
    
    Closes #11094 from anabranch/dynamic-docs.
    
    (cherry picked from commit 66e1383de2650a0f06929db8109a02e32c5eaf6b)
    Signed-off-by: Andrew Or <andrew@databricks.com>
    (cherry picked from commit 3ca5dc3072d0d96ba07d102e9104cbbb177c352b)

commit 646b449cbe55b24d9579d5f354d80911a133fab9
Author: Shixiong Zhu <shixiong@databricks.com>
Date:   Thu Feb 4 12:43:16 2016 -0800

    [SPARK-13195][STREAMING] Fix NoSuchElementException when a state is not set but timeoutThreshold is defined
    
    Check the state Existence before calling get.
    
    Author: Shixiong Zhu <shixiong@databricks.com>
    
    Closes #11081 from zsxwing/SPARK-13195.
    
    (cherry picked from commit 8e2f296306131e6c7c2f06d6672995d3ff8ab021)
    Signed-off-by: Shixiong Zhu <shixiong@databricks.com>
    (cherry picked from commit a907c7c64887833770cd593eecccf53620de59b7)

commit 1adc2c67cbecf9688336257d2bdea279f4980458
Author: Narine Kokhlikyan <narine.kokhlikyan@gmail.com>
Date:   Fri Jan 22 10:35:02 2016 -0800

    [SPARK-12629][SPARKR] Fixes for DataFrame saveAsTable method
    
    I've tried to solve some of the issues mentioned in: https://issues.apache.org/jira/browse/SPARK-12629
    Please, let me know what do you think.
    Thanks!
    
    Author: Narine Kokhlikyan <narine.kokhlikyan@gmail.com>
    
    Closes #10580 from NarineK/sparkrSavaAsRable.
    
    (cherry picked from commit 8a88e121283472c26e70563a4e04c109e9b183b3)
    Signed-off-by: Shivaram Venkataraman <shivaram@cs.berkeley.edu>
    (cherry picked from commit 53f518a6e2791cc4967793b6cc0d4a68d579cb33)

commit 4c6611e89f877a059d811fc558346e1434db72bd
Author: Holden Karau <holden@us.ibm.com>
Date:   Sun Jan 3 17:04:35 2016 -0800

    [SPARK-12611][SQL][PYSPARK][TESTS] Fix test_infer_schema_to_local
    
    Previously (when the PR was first created) not specifying b= explicitly was fine (and treated as default null) - instead be explicit about b being None in the test.
    
    Author: Holden Karau <holden@us.ibm.com>
    
    Closes #10564 from holdenk/SPARK-12611-fix-test-infer-schema-local.
    
    (cherry picked from commit 13dab9c3862cc454094cd9ba7b4504a2d095028f)
    Signed-off-by: Yin Huai <yhuai@databricks.com>
    (cherry picked from commit 85518eda459a48c72a629b4cb9994fc753f72a58)

commit a510eaf57b83a0fe1e7cbe92368fd8233d3edc4c
Author: Shixiong Zhu <shixiong@databricks.com>
Date:   Mon Jan 18 16:50:05 2016 -0800

    [SPARK-12894][DOCUMENT] Add deploy instructions for Python in Kinesis integration doc
    
    This PR added instructions to get Kinesis assembly jar for Python users in the Kinesis integration page like Kafka doc.
    
    Author: Shixiong Zhu <shixiong@databricks.com>
    
    Closes #10822 from zsxwing/kinesis-doc.
    
    (cherry picked from commit 721845c1b64fd6e3b911bd77c94e01dc4e5fd102)
    Signed-off-by: Tathagata Das <tathagata.das1565@gmail.com>
    (cherry picked from commit d43704d7fc6a5e9da4968b1dafa8d4b1c341ee8d)

commit 69710823f2d0be0f908f92fba9deb08b82714a17
Author: Shixiong Zhu <shixiong@databricks.com>
Date:   Mon Jan 18 15:38:03 2016 -0800

    [SPARK-12814][DOCUMENT] Add deploy instructions for Python in flume integration doc
    
    This PR added instructions to get flume assembly jar for Python users in the flume integration page like Kafka doc.
    
    Author: Shixiong Zhu <shixiong@databricks.com>
    
    Closes #10746 from zsxwing/flume-doc.
    
    (cherry picked from commit a973f483f6b819ed4ecac27ff5c064ea13a8dd71)
    Signed-off-by: Tathagata Das <tathagata.das1565@gmail.com>
    (cherry picked from commit 7482c7b5aba5b649510bbb8886bbf2b44f86f543)

commit b17f0da135ae3157cd010e20012f45294d0ab204
Author: Jeff Lam <sha0lin@alumni.carnegiemellon.edu>
Date:   Sat Jan 16 10:41:40 2016 +0000

    [SPARK-12722][DOCS] Fixed typo in Pipeline example
    
    http://spark.apache.org/docs/latest/ml-guide.html#example-pipeline
    ```
    val sameModel = Pipeline.load("/tmp/spark-logistic-regression-model")
    ```
    should be
    ```
    val sameModel = PipelineModel.load("/tmp/spark-logistic-regression-model")
    ```
    cc: jkbradley
    
    Author: Jeff Lam <sha0lin@alumni.carnegiemellon.edu>
    
    Closes #10769 from Agent007/SPARK-12722.
    
    (cherry picked from commit 86972fa52152d2149b88ba75be048a6986006285)
    Signed-off-by: Sean Owen <sowen@cloudera.com>
    (cherry picked from commit 5803fce900cf74508e9520370b8d777862a15a49)

commit d89898ab747ee5d42dc8ed5c5f71110920b75ffe
Author: Oscar D. Lara Yejas <odlaraye@oscars-mbp.usca.ibm.com>
Date:   Fri Jan 15 07:37:54 2016 -0800

    [SPARK-11031][SPARKR] Method str() on a DataFrame
    
    Author: Oscar D. Lara Yejas <odlaraye@oscars-mbp.usca.ibm.com>
    Author: Oscar D. Lara Yejas <olarayej@mail.usf.edu>
    Author: Oscar D. Lara Yejas <oscar.lara.yejas@us.ibm.com>
    Author: Oscar D. Lara Yejas <odlaraye@oscars-mbp.attlocal.net>
    
    Closes #9613 from olarayej/SPARK-11031.
    
    (cherry picked from commit ba4a641902f95c5a9b3a6bebcaa56039eca2720d)
    Signed-off-by: Shivaram Venkataraman <shivaram@cs.berkeley.edu>
    (cherry picked from commit 5a00528391b9b15dd6db15123578d78077f495d8)

commit 00be6e86a968ea55e5c99a9611bc85b9aaf84fda
Author: Yin Huai <yhuai@databricks.com>
Date:   Tue Jan 12 15:15:10 2016 -0800

    Revert "[SPARK-12645][SPARKR] SparkR support hash function"
    
    This reverts commit 8b5f23043322254c725c703c618ba3d3cc4a4240.
    
    (cherry picked from commit 03e523e520b4717a7932859e0bc7d43e5b08dd92)

commit 8f485a16cc5a639e93801f57d463b6c8273af744
Author: Brandon Bradley <bradleytastic@gmail.com>
Date:   Mon Jan 11 14:21:50 2016 -0800

    [SPARK-12758][SQL] add note to Spark SQL Migration guide about TimestampType casting
    
    Warning users about casting changes.
    
    Author: Brandon Bradley <bradleytastic@gmail.com>
    
    Closes #10708 from blbradley/spark-12758.
    
    (cherry picked from commit a767ee8a0599f5482717493a3298413c65d8ff89)
    Signed-off-by: Michael Armbrust <michael@databricks.com>
    (cherry picked from commit dd2cf64f300ec42802dbea38b95047842de81870)

commit 97c496727224e2c952489fc81d98d77fcd8f084c
Author: Yanbo Liang <ybliang8@gmail.com>
Date:   Sat Jan 9 12:29:51 2016 +0530

    [SPARK-12645][SPARKR] SparkR support hash function
    
    Add ```hash``` function for SparkR ```DataFrame```.
    
    Author: Yanbo Liang <ybliang8@gmail.com>
    
    Closes #10597 from yanboliang/spark-12645.
    
    (cherry picked from commit 3d77cffec093bed4d330969f1a996f3358b9a772)
    Signed-off-by: Shivaram Venkataraman <shivaram@cs.berkeley.edu>
    (cherry picked from commit 8b5f23043322254c725c703c618ba3d3cc4a4240)

commit 9acbbaf034764ceaaf5d0d111b311958c5c27a0b
Author: Yanbo Liang <ybliang8@gmail.com>
Date:   Wed Jan 6 12:05:41 2016 +0530

    [SPARK-12393][SPARKR] Add read.text and write.text for SparkR
    
    Add ```read.text``` and ```write.text``` for SparkR.
    cc sun-rui felixcheung shivaram
    
    Author: Yanbo Liang <ybliang8@gmail.com>
    
    Closes #10348 from yanboliang/spark-12393.
    
    (cherry picked from commit d1fea41363c175a67b97cb7b3fe89f9043708739)
    Signed-off-by: Shivaram Venkataraman <shivaram@cs.berkeley.edu>
    (cherry picked from commit c3135d02176cdd679b4a0e4883895b9e9f001a55)

commit b8b26809d170b4077dcce3b19f6c4cc72cb16e4b
Author: Michael Armbrust <michael@databricks.com>
Date:   Mon Jan 4 23:23:41 2016 -0800

    [SPARK-12568][SQL] Add BINARY to Encoders
    
    Author: Michael Armbrust <michael@databricks.com>
    
    Closes #10516 from marmbrus/datasetCleanup.
    
    (cherry picked from commit 53beddc5bf04a35ab73de99158919c2fdd5d4508)
    Signed-off-by: Michael Armbrust <michael@databricks.com>
    (cherry picked from commit d9e4438b5c7b3569662a50973164955332463d05)

commit 04382b0ff37043f8ceac2ee1b7227a8cdce18a76
Author: Daoyuan Wang <daoyuan.wang@intel.com>
Date:   Tue Dec 29 07:02:30 2015 +0900

    [SPARK-12222][CORE] Deserialize RoaringBitmap using Kryo serializer throw Buffer underflow exception
    
    Since we only need to implement `def skipBytes(n: Int)`,
    code in #10213 could be simplified.
    davies scwf
    
    Author: Daoyuan Wang <daoyuan.wang@intel.com>
    
    Closes #10253 from adrian-wang/kryo.
    
    (cherry picked from commit a6d385322e7dfaff600465fa5302010a5f122c6b)
    Signed-off-by: Kousuke Saruta <sarutak@oss.nttdata.co.jp>
    (cherry picked from commit a9c52d4954aa445ab751b38ddbfd8fb6f84d7c14)

commit bef6e549a8ff8e8bc4b4d35c57c1455ae050f3b6
Author: gatorsmile <gatorsmile@gmail.com>
Date:   Sun Dec 27 23:18:48 2015 -0800

    [SPARK-12520] [PYSPARK] Correct Descriptions and Add Use Cases in Equi-Join
    
    After reading the JIRA https://issues.apache.org/jira/browse/SPARK-12520, I double checked the code.
    
    For example, users can do the Equi-Join like
      ```df.join(df2, 'name', 'outer').select('name', 'height').collect()```
    - There exists a bug in 1.5 and 1.4. The code just ignores the third parameter (join type) users pass. However, the join type we called is `Inner`, even if the user-specified type is the other type (e.g., `Outer`).
    - After a PR: https://github.com/apache/spark/pull/8600, the 1.6 does not have such an issue, but the description has not been updated.
    
    Plan to submit another PR to fix 1.5 and issue an error message if users specify a non-inner join type when using Equi-Join.
    
    Author: gatorsmile <gatorsmile@gmail.com>
    
    Closes #10477 from gatorsmile/pyOuterJoin.
    
    (cherry picked from commit b8da77ef776ab9cdc130a70293d75e7bdcdf95b0)

commit 24c0b6e5d9813473c2d84bc6516853c8f861ac80
Author: Shixiong Zhu <shixiong@databricks.com>
Date:   Tue Dec 22 16:39:10 2015 -0800

    [SPARK-12429][STREAMING][DOC] Add Accumulator and Broadcast example for Streaming
    
    This PR adds Scala, Java and Python examples to show how to use Accumulator and Broadcast in Spark Streaming to support checkpointing.
    
    Author: Shixiong Zhu <shixiong@databricks.com>
    
    Closes #10385 from zsxwing/accumulator-broadcast-example.
    
    (cherry picked from commit 20591afd790799327f99485c5a969ed7412eca45)
    Signed-off-by: Tathagata Das <tathagata.das1565@gmail.com>

commit f2bf28b0fb9c9c650f45f717d8a17e66a80906ec
Author: Shixiong Zhu <shixiong@databricks.com>
Date:   Tue Dec 22 15:33:30 2015 -0800

    [SPARK-12487][STREAMING][DOCUMENT] Add docs for Kafka message handler
    
    Author: Shixiong Zhu <shixiong@databricks.com>
    
    Closes #10439 from zsxwing/kafka-message-handler-doc.
    
    (cherry picked from commit 93db50d1c2ff97e6eb9200a995e4601f752968ae)
    Signed-off-by: Tathagata Das <tathagata.das1565@gmail.com>
    (cherry picked from commit 94fb5e870403e19feca8faf7d98bba6d14f7a362)

commit 1068c3c4e0105b00aff9bbf707bbe84c89cca747
Author: Yin Huai <yhuai@databricks.com>
Date:   Tue Jan 26 08:34:10 2016 -0800

    [SPARK-12682][SQL][HOT-FIX] Fix test compilation
    
    Author: Yin Huai <yhuai@databricks.com>
    
    Closes #10925 from yhuai/branch-1.6-hot-fix.
    
    (cherry picked from commit 6ce3dd940def9257982d556cd3adf307fc2fe8a4)

commit 8e75c9b661563b76c913458d3afe1e403e7e26ee
Author: Sameer Agarwal <sameer@databricks.com>
Date:   Tue Jan 26 07:50:37 2016 -0800

    [SPARK-12682][SQL] Add support for (optionally) not storing tables in hive metadata format
    
    This PR adds a new table option (`skip_hive_metadata`) that'd allow the user to skip storing the table metadata in hive metadata format. While this could be useful in general, the specific use-case for this change is that Hive doesn't handle wide schemas well (see https://issues.apache.org/jira/browse/SPARK-12682 and https://issues.apache.org/jira/browse/SPARK-6024) which in turn prevents such tables from being queried in SparkSQL.
    
    Author: Sameer Agarwal <sameer@databricks.com>
    
    Closes #10826 from sameeragarwal/skip-hive-metadata.
    
    (cherry picked from commit 08c781ca672820be9ba32838bbe40d2643c4bde4)
    Signed-off-by: Yin Huai <yhuai@databricks.com>
    (cherry picked from commit f0c98a60f0b4982dc8e29b4a5d213fd8ce4abaf2)

commit 677fdfe2436794214b9c4610e82531c8040a8f55
Author: Michael Armbrust <michael@databricks.com>
Date:   Mon Feb 22 15:27:29 2016 -0800

    [SPARK-12546][SQL] Change default number of open parquet files
    
    A common problem that users encounter with Spark 1.6.0 is that writing to a partitioned parquet table OOMs.  The root cause is that parquet allocates a significant amount of memory that is not accounted for by our own mechanisms.  As a workaround, we can ensure that only a single file is open per task unless the user explicitly asks for more.
    
    Author: Michael Armbrust <michael@databricks.com>
    
    Closes #11308 from marmbrus/parquetWriteOOM.
    
    (cherry picked from commit 173aa949c309ff7a7a03e9d762b9108542219a95)

commit 20f52d3758f3b03a0b2310e468308916e894264e
Author: Shixiong Zhu <shixiong@databricks.com>
Date:   Wed Jan 6 12:03:01 2016 -0800

    [SPARK-12617][PYSPARK] Move Py4jCallbackConnectionCleaner to Streaming
    
    Move Py4jCallbackConnectionCleaner to Streaming because the callback server starts only in StreamingContext.
    
    Author: Shixiong Zhu <shixiong@databricks.com>
    
    Closes #10621 from zsxwing/SPARK-12617-2.
    
    (cherry picked from commit 1e6648d62fb82b708ea54c51cd23bfe4f542856e)
    Signed-off-by: Shixiong Zhu <shixiong@databricks.com>
    (cherry picked from commit d821fae0ecca6393d3632977797d72ba594d26a9)
    (cherry picked from commit bd56448e024bf62dd8b2b6cf991a032a029b55aa)

commit 04e18cddea35d2bea22ac9043724453c58473760
Author: Shixiong Zhu <shixiong@databricks.com>
Date:   Tue Jan 5 13:10:46 2016 -0800

    [SPARK-12617] [PYSPARK] Clean up the leak sockets of Py4J
    
    This patch added Py4jCallbackConnectionCleaner to clean the leak sockets of Py4J every 30 seconds. This is a workaround before Py4J fixes the leak issue https://github.com/bartdag/py4j/issues/187
    
    Author: Shixiong Zhu <shixiong@databricks.com>
    
    Closes #10579 from zsxwing/SPARK-12617.
    
    (cherry picked from commit 047a31bb1042867b20132b347b1e08feab4562eb)
    Signed-off-by: Davies Liu <davies.liu@gmail.com>
    (cherry picked from commit f31d0fd9ea12bfe94434671fbcfe3d0e06a4a97d)
    (cherry picked from commit 208d1a4017d8cb0f3b41fc170cd4e5b5343254cf)

commit 06ce9dae07f9dbd965c81b7de128eafcf7407e89
Author: Marcelo Vanzin <vanzin@cloudera.com>
Date:   Thu Feb 18 15:46:08 2016 -0800

    CLOUDERA-BUILD. CDH-37391. Fix CTE query parsing.
    
    The AST for CTE queries changed because of an internal Hive change
    (HIVE-10698 I believe), so the Spark parser needs to reflect that.

commit 1dc1e6e706d92a242f57ea199e42baefb94de9f3
Author: Marcelo Vanzin <vanzin@cloudera.com>
Date:   Tue Feb 16 11:25:43 2016 -0800

    [SPARK-13280][STREAMING] Use a better logger name for FileBasedWriteAheadLog.
    
    The new logger name is under the org.apache.spark namespace.
    The detection of the caller name was also enhanced a bit to ignore
    some common things that show up in the call stack.
    
    Author: Marcelo Vanzin <vanzin@cloudera.com>
    
    Closes #11165 from vanzin/SPARK-13280.
    
    (cherry picked from commit c7d00a24da317c9601a9239ac1cf185fb6647352)

commit 53b561d13bfae1092173498f2f1ce5c40f2ba0b9
Author: Imran Rashid <irashid@cloudera.com>
Date:   Fri Feb 12 16:48:22 2016 -0600

    CLOUDERA-BUILD Fix test failure in HistoryServerSuite by only using the rest api, no HtmlUnitDriver.
    
    SPARK-7889 introduced some additional tests that use HtmlUnitDriver -- there seems to be some
    version conflict just in the cdh build that leads to class not found exceptions.  The tests against
    the rest api are probably sufficient anyway.

commit 24c79cdde3069b9edf086b820d63eb2a4127b808
Author: Jenkins <dev-kitchen@cloudera.com>
Date:   Fri Feb 12 20:27:48 2016 -0800

    Updating Maven version to 5.8.0-SNAPSHOT

commit ecb4b3ed3bdfc4752f4454b17584b295402dd3f3
Author: Steve Loughran <stevel@hortonworks.com>
Date:   Thu Feb 11 21:37:53 2016 -0600

    [SPARK-7889][WEBUI] HistoryServer updates UI for incomplete apps
    
    When the HistoryServer is showing an incomplete app, it needs to check if there is a newer version of the app available.  It does this by checking if a version of the app has been loaded with a larger *filesize*.  If so, it detaches the current UI, attaches the new one, and redirects back to the same URL to show the new UI.
    
    https://issues.apache.org/jira/browse/SPARK-7889
    
    Author: Steve Loughran <stevel@hortonworks.com>
    Author: Imran Rashid <irashid@cloudera.com>
    
    Closes #11118 from squito/SPARK-7889-alternate.
    
    (cherry picked from commit a2c7dcf61f33fa1897c950d2d905651103c170ea)

commit 8f00d5fc78fec323f9f7146cc5de88c3ca6436e2
Author: Marcelo Vanzin <vanzin@cloudera.com>
Date:   Thu Feb 11 09:43:04 2016 -0800

    CLOUDERA-BUILD. Preview of "[SPARK-5682] Add encrypted shuffle in spark".
    
    This patch is using Chimera library to enable shuffle encryption support.

commit 575fa11fbbef8ee8b86236da58b6432a30516534
Author: Mark Grover <mark@apache.org>
Date:   Thu Feb 11 09:34:06 2016 -0800

    CLOUDERA-BUILD. Build spark against CDH Kafka 2.0.0

commit 9ce706880356e2a16d3f788d638107d923fc8009
Author: Nishkam Ravi <nishkamravi@gmail.com>
Date:   Tue Jan 26 21:14:39 2016 -0800

    [SPARK-12967][NETTY] Avoid NettyRpc error message during sparkContext shutdown
    
    If there's an RPC issue while sparkContext is alive but stopped (which would happen only when executing SparkContext.stop), log a warning instead. This is a common occurrence.
    
    vanzin
    
    Author: Nishkam Ravi <nishkamravi@gmail.com>
    Author: nishkamravi2 <nishkamravi@gmail.com>
    
    Closes #10881 from nishkamravi2/master_netty.
    
    (cherry picked from commit bae3c9a4eb0c320999e5dbafd62692c12823e07d)

commit b03c027efb662492d5add4819baf6c856bc42280
Author: Wenchen Fan <wenchen@databricks.com>
Date:   Wed Feb 3 16:13:23 2016 -0800

    [SPARK-13101][SQL][BRANCH-1.6] nullability of array type element should not fail analysis of encoder
    
    nullability should only be considered as an optimization rather than part of the type system, so instead of failing analysis for mismatch nullability, we should pass analysis and add runtime null check.
    
    backport https://github.com/apache/spark/pull/11035 to 1.6
    
    Author: Wenchen Fan <wenchen@databricks.com>
    
    Closes #11042 from cloud-fan/branch-1.6.
    
    (cherry picked from commit cdfb2a1410aa799596c8b751187dbac28b2cc678)

commit 78c657c76dab4622185c0b9023e1c8859153a58e
Author: Mario Briggs <mario.briggs@in.ibm.com>
Date:   Wed Feb 3 09:50:28 2016 -0800

    [SPARK-12739][STREAMING] Details of batch in Streaming tab uses two Duration columns
    
    I have clearly prefix the two 'Duration' columns in 'Details of Batch' Streaming tab as 'Output Op Duration' and 'Job Duration'
    
    Author: Mario Briggs <mario.briggs@in.ibm.com>
    Author: mariobriggs <mariobriggs@in.ibm.com>
    
    Closes #11022 from mariobriggs/spark-12739.
    
    (cherry picked from commit e9eb248edfa81d75f99c9afc2063e6b3d9ee7392)
    Signed-off-by: Shixiong Zhu <shixiong@databricks.com>
    (cherry picked from commit 5fe8796c2fa859e30cf5ba293bee8957e23163bc)

commit fc6bb69ca1b6139520c427553fbdc3fdb93986b8
Author: Adam Budde <budde@amazon.com>
Date:   Tue Feb 2 19:35:33 2016 -0800

    [SPARK-13122] Fix race condition in MemoryStore.unrollSafely()
    
    https://issues.apache.org/jira/browse/SPARK-13122
    
    A race condition can occur in MemoryStore's unrollSafely() method if two threads that
    return the same value for currentTaskAttemptId() execute this method concurrently. This
    change makes the operation of reading the initial amount of unroll memory used, performing
    the unroll, and updating the associated memory maps atomic in order to avoid this race
    condition.
    
    Initial proposed fix wraps all of unrollSafely() in a memoryManager.synchronized { } block. A cleaner approach might be introduce a mechanism that synchronizes based on task attempt ID. An alternative option might be to track unroll/pending unroll memory based on block ID rather than task attempt ID.
    
    Author: Adam Budde <budde@amazon.com>
    
    Closes #11012 from budde/master.
    
    (cherry picked from commit ff71261b651a7b289ea2312abd6075da8b838ed9)
    Signed-off-by: Andrew Or <andrew@databricks.com>
    
    Conflicts:
    	core/src/main/scala/org/apache/spark/storage/MemoryStore.scala
    
    (cherry picked from commit 2f8abb4afc08aa8dc4ed763bcb93ff6b1d6f0d78)

commit dad031ce067afc4f443d07f4c56147232daa5028
Author: Daoyuan Wang <daoyuan.wang@intel.com>
Date:   Tue Feb 2 11:09:40 2016 -0800

    [SPARK-13056][SQL] map column would throw NPE if value is null
    
    Jira:
    https://issues.apache.org/jira/browse/SPARK-13056
    
    Create a map like
    { "a": "somestring", "b": null}
    Query like
    SELECT col["b"] FROM t1;
    NPE would be thrown.
    
    Author: Daoyuan Wang <daoyuan.wang@intel.com>
    
    Closes #10964 from adrian-wang/npewriter.
    
    (cherry picked from commit 358300c795025735c3b2f96c5447b1b227d4abc1)
    Signed-off-by: Michael Armbrust <michael@databricks.com>
    
    Conflicts:
    	sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala
    
    (cherry picked from commit 3c92333ee78f249dae37070d3b6558b9c92ec7f4)

commit 38fbe6ed2170427bba4f88686dd83a784de8e5db
Author: Grzegorz Chilkiewicz <grzegorz.chilkiewicz@codilime.com>
Date:   Tue Feb 2 11:16:24 2016 -0800

    [SPARK-12711][ML] ML StopWordsRemover does not protect itself from column name duplication
    
    Fixes problem and verifies fix by test suite.
    Also - adds optional parameter: nullable (Boolean) to: SchemaUtils.appendColumn
    and deduplicates SchemaUtils.appendColumn functions.
    
    Author: Grzegorz Chilkiewicz <grzegorz.chilkiewicz@codilime.com>
    
    Closes #10741 from grzegorz-chilkiewicz/master.
    
    (cherry picked from commit b1835d727234fdff42aa8cadd17ddcf43b0bed15)
    Signed-off-by: Joseph K. Bradley <joseph@databricks.com>
    (cherry picked from commit 9c0cf22f7681ae05d894ae05f6a91a9467787519)

commit cf1ae7247bd70d19ddf278e0f70d9941082900b1
Author: Gabriele Nizzoli <mail@nizzoli.net>
Date:   Tue Feb 2 10:57:18 2016 -0800

    [SPARK-13121][STREAMING] java mapWithState mishandles scala Option
    
    java mapwithstate with Function3 has wrong conversion of java `Optional` to scala `Option`, fixed code uses same conversion used in the mapwithstate call that uses Function4 as an input. `Optional.fromNullable(v.get)` fails if v is `None`, better to use `JavaUtils.optionToOptional(v)` instead.
    
    Author: Gabriele Nizzoli <mail@nizzoli.net>
    
    Closes #11007 from gabrielenizzoli/branch-1.6.
    
    (cherry picked from commit 4c28b4c8f342fde937ff77ab30f898dfe3186c03)

commit 81811448ee7e633822d6c2e347cfbc17fe747b0c
Author: Xusen Yin <yinxusen@gmail.com>
Date:   Tue Feb 2 10:21:21 2016 -0800

    [SPARK-12780][ML][PYTHON][BACKPORT] Inconsistency returning value of ML python models' properties
    
    Backport of [SPARK-12780] for branch-1.6
    
    Original PR for master: https://github.com/apache/spark/pull/10724
    
    This fixes StringIndexerModel.labels in pyspark.
    
    Author: Xusen Yin <yinxusen@gmail.com>
    
    Closes #10950 from jkbradley/yinxusen-spark-12780-backport.
    
    (cherry picked from commit 9a3d1bd09cdf4a7c2992525c203d4dac764fddb8)

commit 70596af0e9d54f65fea786406fb41dca79b19d80
Author: Michael Armbrust <michael@databricks.com>
Date:   Tue Feb 2 10:15:40 2016 -0800

    [SPARK-13094][SQL] Add encoders for seq/array of primitives
    
    Author: Michael Armbrust <michael@databricks.com>
    
    Closes #11014 from marmbrus/seqEncoders.
    
    (cherry picked from commit 29d92181d0c49988c387d34e4a71b1afe02c29e2)
    Signed-off-by: Michael Armbrust <michael@databricks.com>
    (cherry picked from commit 99594b213c941cd3ffa3a034f007e44efebdb545)

commit 67ba036e8e681a3cf0f8371652c9309af01e1fd7
Author: Michael Armbrust <michael@databricks.com>
Date:   Tue Feb 2 16:51:07 2016 +0800

    [SPARK-13087][SQL] Fix group by function for sort based aggregation
    
    It is not valid to call `toAttribute` on a `NamedExpression` unless we know for sure that the child produced that `NamedExpression`.  The current code worked fine when the grouping expressions were simple, but when they were a derived value this blew up at execution time.
    
    Author: Michael Armbrust <michael@databricks.com>
    
    Closes #11011 from marmbrus/groupByFunction.
    
    (cherry picked from commit bd8efba8f2131d951829020b4c68309a174859cf)

commit 5ebc67abd86ed827d48ceb6703d6d82887cef010
Author: Takeshi YAMAMURO <linguin.m.s@gmail.com>
Date:   Mon Feb 1 12:13:17 2016 -0800

    [SPARK-11780][SQL] Add catalyst type aliases backwards compatibility
    
    Changed a target at branch-1.6 from #10635.
    
    Author: Takeshi YAMAMURO <linguin.m.s@gmail.com>
    
    Closes #10915 from maropu/pr9935-v3.
    
    (cherry picked from commit 70fcbf68e412f6549ba6c2db86f7ef4518d05fe1)

commit b1550680ce15e04420b885f04b99a35303823558
Author: gatorsmile <gatorsmile@gmail.com>
Date:   Mon Feb 1 11:22:02 2016 -0800

    [SPARK-12989][SQL] Delaying Alias Cleanup after ExtractWindowExpressions
    
    JIRA: https://issues.apache.org/jira/browse/SPARK-12989
    
    In the rule `ExtractWindowExpressions`, we simply replace alias by the corresponding attribute. However, this will cause an issue exposed by the following case:
    
    ```scala
    val data = Seq(("a", "b", "c", 3), ("c", "b", "a", 3)).toDF("A", "B", "C", "num")
      .withColumn("Data", struct("A", "B", "C"))
      .drop("A")
      .drop("B")
      .drop("C")
    
    val winSpec = Window.partitionBy("Data.A", "Data.B").orderBy($"num".desc)
    data.select($"*", max("num").over(winSpec) as "max").explain(true)
    ```
    In this case, both `Data.A` and `Data.B` are `alias` in `WindowSpecDefinition`. If we replace these alias expression by their alias names, we are unable to know what they are since they will not be put in `missingExpr` too.
    
    Author: gatorsmile <gatorsmile@gmail.com>
    Author: xiaoli <lixiao1983@gmail.com>
    Author: Xiao Li <xiaoli@Xiaos-MacBook-Pro.local>
    
    Closes #10963 from gatorsmile/seletStarAfterColDrop.
    
    (cherry picked from commit 33c8a490f7f64320c53530a57bd8d34916e3607c)
    Signed-off-by: Michael Armbrust <michael@databricks.com>
    (cherry picked from commit 9a5b25d0f8543e24b4d00497399790930c01246f)

commit c516fbac19e13904e3d4a6bf43c4fb410af987f4
Author: Kevin Yu <qyu@us.ibm.com>
Date:   Mon Dec 28 11:58:33 2015 -0800

    [SPARK-12231][SQL] create a combineFilters' projection when we call buildPartitionedTableScan
    
    Hello Michael & All:
    
    We have some issues to submit the new codes in the other PR(#10299), so we closed that PR and open this one with the fix.
    
    The reason for the previous failure is that the projection for the scan when there is a filter that is not pushed down (the "left-over" filter) could be different, in elements or ordering, from the original projection.
    
    With this new codes, the approach to solve this problem is:
    
    Insert a new Project if the "left-over" filter is nonempty and (the original projection is not empty and the projection for the scan has more than one elements which could otherwise cause different ordering in projection).
    
    We create 3 test cases to cover the otherwise failure cases.
    
    Author: Kevin Yu <qyu@us.ibm.com>
    
    Closes #10388 from kevinyu98/spark-12231.
    
    (cherry picked from commit fd50df413fbb3b7528cdff311cc040a6212340b9)
    Signed-off-by: Cheng Lian <lian@databricks.com>
    (cherry picked from commit ddb9633043e82fb2a34c7e0e29b487f635c3c744)

commit 42338ce04708df8f50172d3fad9b6f18826720e8
Author: Andrew Or <andrew@databricks.com>
Date:   Fri Jan 29 18:00:49 2016 -0800

    [SPARK-13088] Fix DAG viz in latest version of chrome
    
    Apparently chrome removed `SVGElement.prototype.getTransformToElement`, which is used by our JS library dagre-d3 when creating edges. The real diff can be found here: https://github.com/andrewor14/dagre-d3/commit/7d6c0002e4c74b82a02c5917876576f71e215590, which is taken from the fix in the main repo: https://github.com/cpettitt/dagre-d3/commit/1ef067f1c6ad2e0980f6f0ca471bce998784b7b2
    
    Upstream issue: https://github.com/cpettitt/dagre-d3/issues/202
    
    Author: Andrew Or <andrew@databricks.com>
    
    Closes #10986 from andrewor14/fix-dag-viz.
    
    (cherry picked from commit 70e69fc4dd619654f5d24b8b84f6a94f7705c59b)
    Signed-off-by: Andrew Or <andrew@databricks.com>
    (cherry picked from commit bb01cbe9b2c0f64eef34f6a59b5bf7be55c73012)

commit c110729eb7ed15c5e63664d6b6b92c6d8aaa3f50
Author: Shixiong Zhu <shixiong@databricks.com>
Date:   Fri Jan 29 13:53:11 2016 -0800

    [SPARK-13082][PYSPARK] Backport the fix of 'read.json(rdd)' in #10559 to branch-1.6
    
    SPARK-13082 actually fixed by  #10559. However, it's a big PR and not backported to 1.6. This PR just backported the fix of 'read.json(rdd)' to branch-1.6.
    
    Author: Shixiong Zhu <shixiong@databricks.com>
    
    Closes #10988 from zsxwing/json-rdd.
    
    (cherry picked from commit 84dab7260e9a33586ad4002cd826a5ae7c8c4141)

commit d6113a967f5b6898958b4ec7b857919381d04fb4
Author: Jason Lee <cjlee@us.ibm.com>
Date:   Wed Jan 27 09:55:10 2016 -0800

    [SPARK-10847][SQL][PYSPARK] Pyspark - DataFrame - Optional Metadata with `None` triggers cryptic failure
    
    The error message is now changed from "Do not support type class scala.Tuple2." to "Do not support type class org.json4s.JsonAST$JNull$" to be more informative about what is not supported. Also, StructType metadata now handles JNull correctly, i.e., {'a': None}. test_metadata_null is added to tests.py to show the fix works.
    
    Author: Jason Lee <cjlee@us.ibm.com>
    
    Closes #8969 from jasoncl/SPARK-10847.
    
    (cherry picked from commit edd473751b59b55fa3daede5ed7bc19ea8bd7170)
    Signed-off-by: Yin Huai <yhuai@databricks.com>
    (cherry picked from commit 96e32db5cbd1ef32f65206357bfb8d9f70a06d0a)

commit 1557e395bbd654694229be9c207f265ea80c5791
Author: Xusen Yin <yinxusen@gmail.com>
Date:   Wed Jan 27 00:32:52 2016 -0800

    [SPARK-12834][ML][PYTHON][BACKPORT] Change ser/de of JavaArray and JavaList
    
    Backport of SPARK-12834 for branch-1.6
    
    Original PR: https://github.com/apache/spark/pull/10772
    
    Original commit message:
    We use `SerDe.dumps()` to serialize `JavaArray` and `JavaList` in `PythonMLLibAPI`, then deserialize them with `PickleSerializer` in Python side. However, there is no need to transform them in such an inefficient way. Instead of it, we can use type conversion to convert them, e.g. `list(JavaArray)` or `list(JavaList)`. What's more, there is an issue to Ser/De Scala Array as I said in https://issues.apache.org/jira/browse/SPARK-12780
    
    Author: Xusen Yin <yinxusen@gmail.com>
    
    Closes #10941 from jkbradley/yinxusen-SPARK-12834-1.6.
    
    (cherry picked from commit 17d1071ce8945d056da145f64797d1d10529afc1)

commit d2696a09962e5ffeaa3ccdac6a21c57014316f27
Author: Shixiong Zhu <shixiong@databricks.com>
Date:   Thu Jan 7 17:37:46 2016 -0800

    [SPARK-12507][STREAMING][DOCUMENT] Expose closeFileAfterWrite and allowBatching configurations for Streaming
    
    /cc tdas brkyvz
    
    Author: Shixiong Zhu <shixiong@databricks.com>
    
    Closes #10453 from zsxwing/streaming-conf.
    
    (cherry picked from commit c94199e977279d9b4658297e8108b46bdf30157b)
    Signed-off-by: Tathagata Das <tathagata.das1565@gmail.com>
    (cherry picked from commit a7c36362fb9532183b7b6a0ad5020f02b816a9b3)

commit e79cb8fec8b81e13f8fc26cf60eab29f843537eb
Author: Nong Li <nong@databricks.com>
Date:   Mon Jan 4 10:37:56 2016 -0800

    [SPARK-12486] Worker should kill the executors more forcefully if possible.
    
    This patch updates the ExecutorRunner's terminate path to use the new java 8 API
    to terminate processes more forcefully if possible. If the executor is unhealthy,
    it would previously ignore the destroy() call. Presumably, the new java API was
    added to handle cases like this.
    
    We could update the termination path in the future to use OS specific commands
    for older java versions.
    
    Author: Nong Li <nong@databricks.com>
    
    Closes #10438 from nongli/spark-12486-executors.
    
    (cherry picked from commit 8f659393b270c46e940c4e98af2d996bd4fd6442)
    Signed-off-by: Andrew Or <andrew@databricks.com>
    (cherry picked from commit cd02038198fa57da816211d7bc65921ff9f1e9bb)

commit 5bf0bf3994f86d65c50be6db7966976b0eb44744
Author: Liang-Chi Hsieh <viirya@gmail.com>
Date:   Tue Jan 26 11:36:00 2016 +0000

    [SPARK-12961][CORE] Prevent snappy-java memory leak
    
    JIRA: https://issues.apache.org/jira/browse/SPARK-12961
    
    To prevent memory leak in snappy-java, just call the method once and cache the result. After the library releases new version, we can remove this object.
    
    JoshRosen
    
    Author: Liang-Chi Hsieh <viirya@gmail.com>
    
    Closes #10875 from viirya/prevent-snappy-memory-leak.
    
    (cherry picked from commit 5936bf9fa85ccf7f0216145356140161c2801682)
    Signed-off-by: Sean Owen <sowen@cloudera.com>
    (cherry picked from commit 572bc399952bae322ed6909290996b103688fd3a)

commit cbd908a8394c427980e93d3c7537011856100c6b
Author: jerryshao <sshao@hortonworks.com>
Date:   Wed Dec 9 09:50:43 2015 -0800

    [SPARK-10582][YARN][CORE] Fix AM failure situation for dynamic allocation
    
    Because of AM failure, the target executor number between driver and AM will be different, which will lead to unexpected behavior in dynamic allocation. So when AM is re-registered with driver, state in `ExecutorAllocationManager` and `CoarseGrainedSchedulerBacked` should be reset.
    
    This issue is originally addressed in #8737 , here re-opened again. Thanks a lot KaiXinXiaoLei for finding this issue.
    
    andrewor14 and vanzin would you please help to review this, thanks a lot.
    
    Author: jerryshao <sshao@hortonworks.com>
    
    Closes #9963 from jerryshao/SPARK-10582.
    
    (cherry picked from commit 6900f0173790ad2fa4c79a426bd2dec2d149daa2)

commit 27f9b2abefde5d241f6d83c0f2265fa3b9419669
Author: Andy Grove <andygrove73@gmail.com>
Date:   Mon Jan 25 09:22:10 2016 +0000

    [SPARK-12932][JAVA API] improved error message for java type inference failure
    
    Author: Andy Grove <andygrove73@gmail.com>
    
    Closes #10865 from andygrove/SPARK-12932.
    
    (cherry picked from commit d8e480521e362bc6bc5d8ebcea9b2d50f72a71b9)
    Signed-off-by: Sean Owen <sowen@cloudera.com>
    (cherry picked from commit 88114d3d87f41827ffa9f683edce5e85fdb724ff)

commit 9bf5d9abf4bbc5edfb664ff46b08a072a84e046b
Author: Bryan Cutler <cutlerb@gmail.com>
Date:   Fri Jan 8 11:08:45 2016 -0800

    [SPARK-12701][CORE] FileAppender should use join to ensure writing thread completion
    
    Changed Logging FileAppender to use join in `awaitTermination` to ensure that thread is properly finished before returning.
    
    Author: Bryan Cutler <cutlerb@gmail.com>
    
    Closes #10654 from BryanCutler/fileAppender-join-thread-SPARK-12701.
    
    (cherry picked from commit ea104b8f1ce8aa109d1b16b696a61a47df6283b2)
    Signed-off-by: Sean Owen <sowen@cloudera.com>
    (cherry picked from commit 773366818bbdd479fcb59b6fb7fccf28da13a303)

commit ddbf85c0bd924b90c0cd4aaad3554692b87e63fd
Author: RJ Nowling <rnowling@gmail.com>
Date:   Tue Jan 5 15:05:04 2016 -0800

    [SPARK-12450][MLLIB] Un-persist broadcasted variables in KMeans
    
    SPARK-12450 . Un-persist broadcasted variables in KMeans.
    
    Author: RJ Nowling <rnowling@gmail.com>
    
    Closes #10415 from rnowling/spark-12450.
    
    (cherry picked from commit 78015a8b7cc316343e302eeed6fe30af9f2961e8)
    Signed-off-by: Joseph K. Bradley <joseph@databricks.com>
    (cherry picked from commit 0afad6678431846a6eebda8d5891da9115884915)

commit 49da41f915bc9ef47fd6f1384264ae65a3a6be06
Author: Nong Li <nong@databricks.com>
Date:   Fri Dec 18 16:05:18 2015 -0800

    [SPARK-12411][CORE] Decrease executor heartbeat timeout to match heartbeat interval
    
    Previously, the rpc timeout was the default network timeout, which is the same value
    the driver uses to determine dead executors. This means if there is a network issue,
    the executor is determined dead after one heartbeat attempt. There is a separate config
    for the heartbeat interval which is a better value to use for the heartbeat RPC. With
    this change, the executor will make multiple heartbeat attempts even with RPC issues.
    
    Author: Nong Li <nong@databricks.com>
    
    Closes #10365 from nongli/spark-12411.
    
    (cherry picked from commit b49856ae5983aca8ed7df2f478fc5f399ec34ce8)

commit 6e4fb656ac7f7d6a6586eb6a4057ff18116ee758
Author: Jeff Zhang <zjffdu@apache.org>
Date:   Sun Jan 24 12:29:26 2016 -0800

    [SPARK-12120][PYSPARK] Improve exception message when failing to init…
    
    …ialize HiveContext in PySpark
    
    davies Mind to review ?
    
    This is the error message after this PR
    
    ```
    15/12/03 16:59:53 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
    /Users/jzhang/github/spark/python/pyspark/sql/context.py:689: UserWarning: You must build Spark with Hive. Export 'SPARK_HIVE=true' and run build/sbt assembly
      warnings.warn("You must build Spark with Hive. "
    Traceback (most recent call last):
      File "<stdin>", line 1, in <module>
      File "/Users/jzhang/github/spark/python/pyspark/sql/context.py", line 663, in read
        return DataFrameReader(self)
      File "/Users/jzhang/github/spark/python/pyspark/sql/readwriter.py", line 56, in __init__
        self._jreader = sqlContext._ssql_ctx.read()
      File "/Users/jzhang/github/spark/python/pyspark/sql/context.py", line 692, in _ssql_ctx
        raise e
    py4j.protocol.Py4JJavaError: An error occurred while calling None.org.apache.spark.sql.hive.HiveContext.
    : java.lang.RuntimeException: java.net.ConnectException: Call From jzhangMBPr.local/127.0.0.1 to 0.0.0.0:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
    	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:522)
    	at org.apache.spark.sql.hive.client.ClientWrapper.<init>(ClientWrapper.scala:194)
    	at org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:238)
    	at org.apache.spark.sql.hive.HiveContext.executionHive$lzycompute(HiveContext.scala:218)
    	at org.apache.spark.sql.hive.HiveContext.executionHive(HiveContext.scala:208)
    	at org.apache.spark.sql.hive.HiveContext.functionRegistry$lzycompute(HiveContext.scala:462)
    	at org.apache.spark.sql.hive.HiveContext.functionRegistry(HiveContext.scala:461)
    	at org.apache.spark.sql.UDFRegistration.<init>(UDFRegistration.scala:40)
    	at org.apache.spark.sql.SQLContext.<init>(SQLContext.scala:330)
    	at org.apache.spark.sql.hive.HiveContext.<init>(HiveContext.scala:90)
    	at org.apache.spark.sql.hive.HiveContext.<init>(HiveContext.scala:101)
    	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
    	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
    	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
    	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:234)
    	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)
    	at py4j.Gateway.invoke(Gateway.java:214)
    	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:79)
    	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:68)
    	at py4j.GatewayConnection.run(GatewayConnection.java:209)
    	at java.lang.Thread.run(Thread.java:745)
    ```
    
    Author: Jeff Zhang <zjffdu@apache.org>
    
    Closes #10126 from zjffdu/SPARK-12120.
    
    (cherry picked from commit e789b1d2c1eab6187f54424ed92697ca200c3101)
    Signed-off-by: Josh Rosen <joshrosen@databricks.com>
    (cherry picked from commit f913f7ea080bc90bd967724e583f42b0a48075d9)

commit c8c35b8df7edb32eea43354bf2af390f46cbd10f
Author: Sean Owen <sowen@cloudera.com>
Date:   Tue Jan 12 12:13:32 2016 +0000

    [SPARK-5273][MLLIB][DOCS] Improve documentation examples for LinearRegression
    
    Use a much smaller step size in LinearRegressionWithSGD MLlib examples to achieve a reasonable RMSE.
    
    Our training folks hit this exact same issue when concocting an example and had the same solution.
    
    Author: Sean Owen <sowen@cloudera.com>
    
    Closes #10675 from srowen/SPARK-5273.
    
    (cherry picked from commit 9c7f34af37ef328149c1d66b4689d80a1589e1cc)
    Signed-off-by: Sean Owen <sowen@cloudera.com>
    (cherry picked from commit 4c67d55c0ccf086e91d1755b62c8526f2ff51f21)

commit bf3123c9b92429d04c1bffd3338f9a0632192442
Author: Xin Ren <iamshrek@126.com>
Date:   Tue Dec 8 11:44:51 2015 -0600

    [SPARK-11155][WEB UI] Stage summary json should include stage duration
    
    The json endpoint for stages doesn't include information on the stage duration that is present in the UI. This looks like a simple oversight, they should be included. eg., the metrics should be included at api/v1/applications/<appId>/stages.
    
    Metrics I've added are: submissionTime, firstTaskLaunchedTime and completionTime
    
    Author: Xin Ren <iamshrek@126.com>
    
    Closes #10107 from keypointt/SPARK-11155.
    
    (cherry picked from commit 6cb06e8711fd6ac10c57faeb94bc323cae1cef27)

commit 23a35bdbddcfff479a47310ffd9b9831f8340dd7
Author: CodingCat <zhunansjtu@gmail.com>
Date:   Tue Dec 15 18:21:00 2015 -0800

    [SPARK-9516][UI] Improvement of Thread Dump Page
    
    https://issues.apache.org/jira/browse/SPARK-9516
    
    - [x] new look of Thread Dump Page
    
    - [x] click column title to sort
    
    - [x] grep
    
    - [x] search as you type
    
    squito JoshRosen It's ready for the review now
    
    Author: CodingCat <zhunansjtu@gmail.com>
    
    Closes #7910 from CodingCat/SPARK-9516.
    
    (cherry picked from commit a63d9edcfb8a714a17492517927aa114dea8fea0)

commit 29e79113a96a524169680f2903810444388e63e9
Author: Nong Li <nong@databricks.com>
Date:   Tue Dec 22 13:27:28 2015 -0800

    [SPARK-12471][CORE] Spark daemons will log their pid on start up.
    
    Author: Nong Li <nong@databricks.com>
    
    Closes #10422 from nongli/12471-pids.
    
    (cherry picked from commit 575a1327976202614a6d3268918ae8dad49fcd72)

commit efe46e5cab3eefa6af76d73f020d5c37f83e1c41
Author: Mark Grover <mark@apache.org>
Date:   Tue Feb 9 13:44:24 2016 -0800

    CLOUDERA-BUILD. Revert Build spark against CDH Kafka 2.0.0"
    
    This reverts commit 47e752dea3d499bcd97194c6b48a928f011e2ede.

commit 47e752dea3d499bcd97194c6b48a928f011e2ede
Author: Mark Grover <mark@apache.org>
Date:   Tue Feb 9 13:12:04 2016 -0800

    CLOUDERA-BUILD. Build spark against CDH Kafka 2.0.0

commit c3a3512dc9a86e0581f796dce46f6ec2d281eb17
Author: Josh Rosen <joshrosen@databricks.com>
Date:   Sat Dec 5 08:15:30 2015 +0800

    [SPARK-12112][BUILD] Upgrade to SBT 0.13.9
    
    We should upgrade to SBT 0.13.9, since this is a requirement in order to use SBT's new Maven-style resolution features (which will be done in a separate patch, because it's blocked by some binary compatibility issues in the POM reader plugin).
    
    I also upgraded Scalastyle to version 0.8.0, which was necessary in order to fix a Scala 2.10.5 compatibility issue (see https://github.com/scalastyle/scalastyle/issues/156). The newer Scalastyle is slightly stricter about whitespace surrounding tokens, so I fixed the new style violations.
    
    Author: Josh Rosen <joshrosen@databricks.com>
    
    Closes #10112 from JoshRosen/upgrade-to-sbt-0.13.9.
    
    (cherry picked from commit b7204e1d41271d2e8443484371770936664350b1)

commit f0832fd1f36aa7fc4f22d5e069ca489131630bcc
Author: Wenchen Fan <wenchen@databricks.com>
Date:   Mon Dec 28 11:45:44 2015 -0800

    [SPARK-12321][SPARK-12696][HOT-FIX] bypass hive test when parse logical plan to json
    
    https://github.com/apache/spark/pull/10311 introduces some rare, non-deterministic flakiness for hive udf tests, see https://github.com/apache/spark/pull/10311#issuecomment-166548851
    
    I can't reproduce it locally, and may need more time to investigate, a quick solution is: bypass hive tests for json serialization.
    
    Author: Wenchen Fan <wenchen@databricks.com>
    
    Closes #10430 from cloud-fan/hot-fix.
    
    (cherry picked from commit 8543997f2daa60dfa0509f149fab207de98145a0)
    Signed-off-by: Michael Armbrust <michael@databricks.com>
    (cherry picked from commit f71e5cc123ea84341de25e5d07f838ca2d9fe3c0)

commit 46796c71306b3314bc963ad66f702b48aecb9e34
Author: Michael Allman <michael@videoamp.com>
Date:   Mon Jan 25 09:51:41 2016 +0000

    [SPARK-12755][CORE] Stop the event logger before the DAG scheduler
    
    [SPARK-12755][CORE] Stop the event logger before the DAG scheduler to avoid a race condition where the standalone master attempts to build the app's history UI before the event log is stopped.
    
    This contribution is my original work, and I license this work to the Spark project under the project's open source license.
    
    Author: Michael Allman <michael@videoamp.com>
    
    Closes #10700 from mallman/stop_event_logger_first.
    
    (cherry picked from commit 4ee8191e57cb823a23ceca17908af86e70354554)
    Signed-off-by: Sean Owen <sowen@cloudera.com>
    (cherry picked from commit b40e58cf251c22c6b0ba383cc7e67ef6b07d8ec5)

commit 9370ef66bdfd3a04d38b4a27428ef2fd2254b7aa
Author: Cheng Lian <lian@databricks.com>
Date:   Sun Jan 24 19:40:34 2016 -0800

    [SPARK-12624][PYSPARK] Checks row length when converting Java arrays to Python rows
    
    When actual row length doesn't conform to specified schema field length, we should give a better error message instead of throwing an unintuitive `ArrayOutOfBoundsException`.
    
    Author: Cheng Lian <lian@databricks.com>
    
    Closes #10886 from liancheng/spark-12624.
    
    (cherry picked from commit 3327fd28170b549516fee1972dc6f4c32541591b)
    Signed-off-by: Yin Huai <yhuai@databricks.com>
    (cherry picked from commit 88614dd0f9f25ec2045940b030d757079913ac26)

commit c1fb4fbdeb79e7b4a13856219e7f1b58d281bc5c
Author: Sean Owen <sowen@cloudera.com>
Date:   Sat Jan 23 11:45:12 2016 +0000

    [SPARK-12760][DOCS] inaccurate description for difference between local vs cluster mode in closure handling
    
    Clarify that modifying a driver local variable won't have the desired effect in cluster modes, and may or may not work as intended in local mode
    
    Author: Sean Owen <sowen@cloudera.com>
    
    Closes #10866 from srowen/SPARK-12760.
    
    (cherry picked from commit aca2a0165405b9eba27ac5e4739e36a618b96676)
    Signed-off-by: Sean Owen <sowen@cloudera.com>
    (cherry picked from commit f13a3d1f73d01bf167f3736b66222b1cb8f7a01b)

commit 6e3f1c60d97f95ba1c34eae385b42cb0db649d8c
Author: Mortada Mehyar <mortada.mehyar@gmail.com>
Date:   Sat Jan 23 11:36:33 2016 +0000

    [SPARK-12760][DOCS] invalid lambda expression in python example for …
    
    …local vs cluster
    
    srowen thanks for the PR at https://github.com/apache/spark/pull/10866! sorry it took me a while.
    
    This is related to https://github.com/apache/spark/pull/10866, basically the assignment in the lambda expression in the python example is actually invalid
    
    ```
    In [1]: data = [1, 2, 3, 4, 5]
    In [2]: counter = 0
    In [3]: rdd = sc.parallelize(data)
    In [4]: rdd.foreach(lambda x: counter += x)
      File "<ipython-input-4-fcb86c182bad>", line 1
        rdd.foreach(lambda x: counter += x)
                                       ^
    SyntaxError: invalid syntax
    ```
    
    Author: Mortada Mehyar <mortada.mehyar@gmail.com>
    
    Closes #10867 from mortada/doc_python_fix.
    
    (cherry picked from commit 56f57f894eafeda48ce118eec16ecb88dbd1b9dc)
    Signed-off-by: Sean Owen <sowen@cloudera.com>
    (cherry picked from commit e8ae242f925ab747aa5a7bba581da66195e31110)

commit 23e9876c3ca57a1570f00feb1ddd92caa30f742e
Author: Alex Bozarth <ajbozart@us.ibm.com>
Date:   Sat Jan 23 20:19:58 2016 +0900

    [SPARK-12859][STREAMING][WEB UI] Names of input streams with receivers don't fit in Streaming page
    
    Added CSS style to force names of input streams with receivers to wrap
    
    Author: Alex Bozarth <ajbozart@us.ibm.com>
    
    Closes #10873 from ajbozarth/spark12859.
    
    (cherry picked from commit 358a33bbff549826b2336c317afc7274bdd30fdb)
    Signed-off-by: Kousuke Saruta <sarutak@oss.nttdata.co.jp>
    (cherry picked from commit dca238af7ef39e0d1951b72819f12092eae1964a)

commit 1a7fce2fc7fa08aaf2d05ac7931868d5bdf96a3c
Author: Liang-Chi Hsieh <viirya@gmail.com>
Date:   Thu Jan 21 18:55:28 2016 -0800

    [SPARK-12747][SQL] Use correct type name for Postgres JDBC's real array
    
    https://issues.apache.org/jira/browse/SPARK-12747
    
    Postgres JDBC driver uses "FLOAT4" or "FLOAT8" not "real".
    
    Author: Liang-Chi Hsieh <viirya@gmail.com>
    
    Closes #10695 from viirya/fix-postgres-jdbc.
    
    (cherry picked from commit 55c7dd031b8a58976922e469626469aa4aff1391)
    Signed-off-by: Reynold Xin <rxin@databricks.com>
    (cherry picked from commit b5d7dbeb3110a11716f6642829f4ea14868ccc8a)

commit e1a7d01d47eaa1e8bff20fbfa0d9ce1c38bce7f9
Author: Josh Rosen <joshrosen@databricks.com>
Date:   Wed Jan 20 16:10:28 2016 -0800

    [SPARK-12921] Use SparkHadoopUtil reflection in SpecificParquetRecordReaderBase
    
    It looks like there's one place left in the codebase, SpecificParquetRecordReaderBase, where we didn't use SparkHadoopUtil's reflective accesses of TaskAttemptContext methods, which could create problems when using a single Spark artifact with both Hadoop 1.x and 2.x.
    
    Author: Josh Rosen <joshrosen@databricks.com>
    
    Closes #10843 from JoshRosen/SPARK-12921.
    
    (cherry picked from commit 40fa21856aded0e8b0852cdc2d8f8bc577891908)

commit 29ea39c246118c19d4d75476df532430fa8e2274
Author: Wenchen Fan <wenchen@databricks.com>
Date:   Mon Jan 18 21:20:19 2016 -0800

    [SPARK-12841][SQL][BRANCH-1.6] fix cast in filter
    
    In SPARK-10743 we wrap cast with `UnresolvedAlias` to give `Cast` a better alias if possible. However, for cases like filter, the `UnresolvedAlias` can't be resolved and actually we don't need a better alias for this case. This PR move the cast wrapping logic to `Column.named` so that we will only do it when we need a alias name.
    
    backport https://github.com/apache/spark/pull/10781 to 1.6
    
    Author: Wenchen Fan <wenchen@databricks.com>
    
    Closes #10819 from cloud-fan/bug.
    
    (cherry picked from commit 68265ac23e20305474daef14bbcf874308ca8f5a)

commit 6e58725ef8057b1524f1bff513f883f8e20e71de
Author: Eric Liang <ekl@databricks.com>
Date:   Mon Jan 18 12:50:58 2016 -0800

    [SPARK-12346][ML] Missing attribute names in GLM for vector-type features
    
    Currently `summary()` fails on a GLM model fitted over a vector feature missing ML attrs, since the output feature attrs will also have no name. We can avoid this situation by forcing `VectorAssembler` to make up suitable names when inputs are missing names.
    
    cc mengxr
    
    Author: Eric Liang <ekl@databricks.com>
    
    Closes #10323 from ericl/spark-12346.
    
    (cherry picked from commit 5e492e9d5bc0992cbcffe64a9aaf3b334b173d2c)
    Signed-off-by: Xiangrui Meng <meng@databricks.com>
    (cherry picked from commit 8c2b67f55416562a0f1fafeefb073f79701c9cc9)

commit eab39f29e3405dcb35d28d3ddead150d14e9ecff
Author: Dilip Biswal <dbiswal@us.ibm.com>
Date:   Mon Jan 18 10:28:01 2016 -0800

    [SPARK-12558][FOLLOW-UP] AnalysisException when multiple functions applied in GROUP BY clause
    
    Addresses the comments from Yin.
    https://github.com/apache/spark/pull/10520
    
    Author: Dilip Biswal <dbiswal@us.ibm.com>
    
    Closes #10758 from dilipbiswal/spark-12558-followup.
    
    (cherry picked from commit db9a860589bfc4f80d6cdf174a577ca538b82e6d)
    Signed-off-by: Yin Huai <yhuai@databricks.com>
    
    Conflicts:
    	sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/HiveUDFSuite.scala
    
    (cherry picked from commit 53184ce779d85022e49651e631df907dc75af045)

commit e1abd2867f6d0a2f6eae3d56e2ea656f1abac2ff
Author: Koyo Yoshida <yoshidakuy@oss.nttdata.co.jp>
Date:   Fri Jan 15 13:32:47 2016 +0900

    [SPARK-12708][UI] Sorting task error in Stages Page when yarn mode.
    
    If sort column contains slash(e.g. "Executor ID / Host") when yarn mode,sort fail with following message.
    
    ![spark-12708](https://cloud.githubusercontent.com/assets/6679275/12193320/80814f8c-b62a-11e5-9914-7bf3907029df.png)
    
    Ｉt's similar to SPARK-4313 .
    
    Author: root <root@R520T1.(none)>
    Author: Koyo Yoshida <koyo0615@gmail.com>
    
    Closes #10663 from yoshidakuy/SPARK-12708.
    
    (cherry picked from commit 32cca933546b4aaf0fc040b9cfd1a5968171b423)
    Signed-off-by: Kousuke Saruta <sarutak@oss.nttdata.co.jp>
    (cherry picked from commit d23e57d02977f83c099ef24dda52c7673dcc9ad7)

commit 43b86042c63dc3da2a825d3026088d0a30ef6773
Author: Shixiong Zhu <shixiong@databricks.com>
Date:   Thu Jan 14 09:50:57 2016 -0800

    [SPARK-12784][UI] Fix Spark UI IndexOutOfBoundsException with dynamic allocation
    
    Add `listener.synchronized` to get `storageStatusList` and `execInfo` atomically.
    
    Author: Shixiong Zhu <shixiong@databricks.com>
    
    Closes #10728 from zsxwing/SPARK-12784.
    
    (cherry picked from commit 501e99ef0fbd2f2165095548fe67a3447ccbfc91)
    Signed-off-by: Shixiong Zhu <shixiong@databricks.com>
    (cherry picked from commit d1855adb5eab7bf42604e949fa6c9687e91bade1)

commit 00362b56c46986300c7800d854e155928eff0b52
Author: Bryan Cutler <cutlerb@gmail.com>
Date:   Thu Jan 14 10:59:02 2016 +0000

    [SPARK-9844][CORE] File appender race condition during shutdown
    
    When an Executor process is destroyed, the FileAppender that is asynchronously reading the stderr stream of the process can throw an IOException during read because the stream is closed.  Before the ExecutorRunner destroys the process, the FileAppender thread is flagged to stop.  This PR wraps the inputStream.read call of the FileAppender in a try/catch block so that if an IOException is thrown and the thread has been flagged to stop, it will safely ignore the exception.  Additionally, the FileAppender thread was changed to use Utils.tryWithSafeFinally to better log any exception that do occur.  Added unit tests to verify a IOException is thrown and logged if FileAppender is not flagged to stop, and that no IOException when the flag is set.
    
    Author: Bryan Cutler <cutlerb@gmail.com>
    
    Closes #10714 from BryanCutler/file-appender-read-ioexception-SPARK-9844.
    
    (cherry picked from commit 56cdbd654d54bf07a063a03a5c34c4165818eeb2)
    Signed-off-by: Sean Owen <sowen@cloudera.com>
    (cherry picked from commit 0c67993cf25c681611c55fd056808beee048129b)

commit 82597d7cf068656954a634712e1a4ccbac232b8b
Author: Yuhao Yang <hhbyyh@gmail.com>
Date:   Wed Jan 13 17:43:27 2016 -0800

    [SPARK-12026][MLLIB] ChiSqTest gets slower and slower over time when number of features is large
    
    jira: https://issues.apache.org/jira/browse/SPARK-12026
    
    The issue is valid as features.toArray.view.zipWithIndex.slice(startCol, endCol) becomes slower as startCol gets larger.
    
    I tested on local and the change can improve the performance and the running time was stable.
    
    Author: Yuhao Yang <hhbyyh@gmail.com>
    
    Closes #10146 from hhbyyh/chiSq.
    
    (cherry picked from commit 021dafc6a05a31dc22c9f9110dedb47a1f913087)
    Signed-off-by: Joseph K. Bradley <joseph@databricks.com>
    (cherry picked from commit a490787daa5ec11a5e30bc0df31f81edd54ccc6a)

commit 58316d9818dcdd956c7ff8a63c58fc962f6ed32d
Author: Carson Wang <carson.wang@intel.com>
Date:   Wed Jan 13 13:28:39 2016 -0800

    [SPARK-12690][CORE] Fix NPE in UnsafeInMemorySorter.free()
    
    I hit the exception below. The `UnsafeKVExternalSorter` does pass `null` as the consumer when creating an `UnsafeInMemorySorter`. Normally the NPE doesn't occur because the `inMemSorter` is set to null later and the `free()` method is not called. It happens when there is another exception like OOM thrown before setting `inMemSorter` to null. Anyway, we can add the null check to avoid it.
    
    ```
    ERROR spark.TaskContextImpl: Error in TaskCompletionListener
    java.lang.NullPointerException
            at org.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorter.free(UnsafeInMemorySorter.java:110)
            at org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.cleanupResources(UnsafeExternalSorter.java:288)
            at org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter$1.onTaskCompletion(UnsafeExternalSorter.java:141)
            at org.apache.spark.TaskContextImpl$$anonfun$markTaskCompleted$1.apply(TaskContextImpl.scala:79)
            at org.apache.spark.TaskContextImpl$$anonfun$markTaskCompleted$1.apply(TaskContextImpl.scala:77)
            at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
            at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
            at org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:77)
            at org.apache.spark.scheduler.Task.run(Task.scala:91)
            at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)
            at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
            at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
            at java.lang.Thread.run(Thread.java:722)
    ```
    
    Author: Carson Wang <carson.wang@intel.com>
    
    Closes #10637 from carsonwang/FixNPE.
    
    (cherry picked from commit eabc7b8ee7e809bab05361ed154f87bff467bd88)
    Signed-off-by: Josh Rosen <joshrosen@databricks.com>
    (cherry picked from commit 26f13faa981a51046ed1f16b9c3ee42ac5f6b6da)

commit 27c086e365938555207d87ce2b01bef578371be9
Author: Erik Selin <erik.selin@gmail.com>
Date:   Wed Jan 13 12:21:45 2016 -0800

    [SPARK-12268][PYSPARK] Make pyspark shell pythonstartup work under python3
    
    This replaces the `execfile` used for running custom python shell scripts
    with explicit open, compile and exec (as recommended by 2to3). The reason
    for this change is to make the pythonstartup option compatible with python3.
    
    Author: Erik Selin <erik.selin@gmail.com>
    
    Closes #10255 from tyro89/pythonstartup-python3.
    
    (cherry picked from commit e4e0b3f7b2945aae5ec7c3d68296010bbc5160cf)
    Signed-off-by: Josh Rosen <joshrosen@databricks.com>
    (cherry picked from commit cf6d506c7426dbcd19d4c9d7c2d673aa52d00d4e)

commit 166d94fa6623388b0936b422299bb9bfe0e101b4
Author: Yuhao Yang <hhbyyh@gmail.com>
Date:   Wed Jan 13 11:53:25 2016 -0800

    [SPARK-12685][MLLIB][BACKPORT TO 1.4] word2vec trainWordsCount gets overflow
    
    jira: https://issues.apache.org/jira/browse/SPARK-12685
    
    master PR: https://github.com/apache/spark/pull/10627
    
    the log of word2vec reports
    trainWordsCount = -785727483
    during computation over a large dataset.
    
    Update the priority as it will affect the computation process.
    alpha = learningRate * (1 - numPartitions * wordCount.toDouble / (trainWordsCount + 1))
    
    Author: Yuhao Yang <hhbyyh@gmail.com>
    
    Closes #10721 from hhbyyh/branch-1.4.
    
    (cherry picked from commit 7bd2564192f51f6229cf759a2bafc22134479955)
    Signed-off-by: Joseph K. Bradley <joseph@databricks.com>
    (cherry picked from commit 364f799cf6e23d084d7e9adb8c33f923f4130aa9)

commit 792fd37fef20f1fd21e1d6ea85cd766fbb090d45
Author: Luc Bourlier <luc.bourlier@typesafe.com>
Date:   Wed Jan 13 11:45:13 2016 -0800

    [SPARK-12805][MESOS] Fixes documentation on Mesos run modes
    
    The default run has changed, but the documentation didn't fully reflect the change.
    
    Author: Luc Bourlier <luc.bourlier@typesafe.com>
    
    Closes #10740 from skyluc/issue/mesos-modes-doc.
    
    (cherry picked from commit cc91e21879e031bcd05316eabb856e67a51b191d)
    Signed-off-by: Reynold Xin <rxin@databricks.com>
    (cherry picked from commit f9ecd3a3942e3acc2e0f8f8082186f1aca71d40f)

commit 74b91e649cc5537856a2ed2ff4a53ac740e0501d
Author: Dilip Biswal <dbiswal@us.ibm.com>
Date:   Tue Jan 12 21:41:38 2016 -0800

    [SPARK-12558][SQL] AnalysisException when multiple functions applied in GROUP BY clause
    
    cloud-fan Can you please take a look ?
    
    In this case, we are failing during check analysis while validating the aggregation expression. I have added a semanticEquals for HiveGenericUDF to fix this. Please let me know if this is the right way to address this issue.
    
    Author: Dilip Biswal <dbiswal@us.ibm.com>
    
    Closes #10520 from dilipbiswal/spark-12558.
    
    (cherry picked from commit dc7b3870fcfc2723319dbb8c53d721211a8116be)
    Signed-off-by: Yin Huai <yhuai@databricks.com>
    
    Conflicts:
    	sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveShim.scala
    
    (cherry picked from commit dcdc864cf4e40a9d65e6e066c30355addc75c3b2)

commit 3eb0cf2f98ac1dd9ecb54a14dda67fda398b5949
Author: Sean Owen <sowen@cloudera.com>
Date:   Tue Jan 12 11:50:33 2016 +0000

    [SPARK-7615][MLLIB] MLLIB Word2Vec wordVectors divided by Euclidean Norm equals to zero
    
    Cosine similarity with 0 vector should be 0
    
    Related to https://github.com/apache/spark/pull/10152
    
    Author: Sean Owen <sowen@cloudera.com>
    
    Closes #10696 from srowen/SPARK-7615.
    
    (cherry picked from commit c48f2a3a5fd714ad2ff19b29337e55583988431e)
    Signed-off-by: Sean Owen <sowen@cloudera.com>
    (cherry picked from commit 94b39f7777ecff3794727c186bd681fa4c6af4fd)

commit fe4caae2bfd217fadfd1eba5e94641aaf5343c3e
Author: Yucai Yu <yucai.yu@intel.com>
Date:   Tue Jan 12 13:23:23 2016 +0000

    [SPARK-12582][TEST] IndexShuffleBlockResolverSuite fails in windows
    
    [SPARK-12582][Test] IndexShuffleBlockResolverSuite fails in windows
    
    * IndexShuffleBlockResolverSuite fails in windows due to file is not closed.
    * mv IndexShuffleBlockResolverSuite.scala from "test/java" to "test/scala".
    
    https://issues.apache.org/jira/browse/SPARK-12582
    
    Author: Yucai Yu <yucai.yu@intel.com>
    
    Closes #10526 from yucai/master.
    
    (cherry picked from commit 7e15044d9d9f9839c8d422bae71f27e855d559b4)
    Signed-off-by: Sean Owen <sowen@cloudera.com>
    (cherry picked from commit 3221a7d912bdc5a1ce5992501e1a2e6a8248c668)

commit 7487ac51699b4142a8b5980be05b0060d5eae278
Author: Tommy YU <tummyyu@163.com>
Date:   Tue Jan 12 13:20:04 2016 +0000

    [SPARK-12638][API DOC] Parameter explanation not very accurate for rdd function "aggregate"
    
    Currently, RDD function aggregate's parameter doesn't explain well, especially parameter "zeroValue".
    It's helpful to let junior scala user know that "zeroValue" attend both "seqOp" and "combOp" phase.
    
    Author: Tommy YU <tummyyu@163.com>
    
    Closes #10587 from Wenpei/rdd_aggregate_doc.
    
    (cherry picked from commit 9f0995bb0d0bbe5d9b15a1ca9fa18e246ff90d66)
    Signed-off-by: Sean Owen <sowen@cloudera.com>
    (cherry picked from commit 46fc7a12a30b82cf1bcaab0e987a98b4dace37fe)

commit ba2ef25ee03f0d9c67259b2bf27efa24da7cbfe4
Author: Josh Rosen <joshrosen@databricks.com>
Date:   Mon Jan 11 12:56:43 2016 -0800

    [SPARK-12734][HOTFIX] Build changes must trigger all tests; clean after install in dep tests
    
    This patch fixes a build/test issue caused by the combination of #10672 and a latent issue in the original `dev/test-dependencies` script.
    
    First, changes which _only_ touched build files were not triggering full Jenkins runs, making it possible for a build change to be merged even though it could cause failures in other tests. The `root` build module now depends on `build`, so all tests will now be run whenever a build-related file is changed.
    
    I also added a `clean` step to the Maven install step in `dev/test-dependencies` in order to address an issue where the dummy JARs stuck around and caused "multiple assembly JARs found" errors in tests.
    
    /cc zsxwing
    
    Author: Josh Rosen <joshrosen@databricks.com>
    
    Closes #10704 from JoshRosen/fix-build-test-problems.
    
    (cherry picked from commit a44991453a43615028083ba9546f5cd93112f6bd)
    Signed-off-by: Josh Rosen <joshrosen@databricks.com>
    (cherry picked from commit 3b32aa9e29506606d4ca2407aa65a1aab8794805)

commit da1e45dbc1e826c8c94c9d4d9b5b8b35acc010b8
Author: Josh Rosen <joshrosen@databricks.com>
Date:   Mon Jan 11 00:36:52 2016 -0800

    [SPARK-12734][BUILD] Backport Netty exclusion + Maven enforcer fixes to branch-1.6
    
    This patch backports the Netty exclusion fixes from #10672 to branch-1.6.
    
    Author: Josh Rosen <joshrosen@databricks.com>
    
    Closes #10691 from JoshRosen/netty-exclude-16-backport.
    
    (cherry picked from commit 43b72d83e1d0c426d00d29e54ab7d14579700330)

commit acfad05822240f900873500d5265bc9036b952aa
Author: Josh Rosen <joshrosen@databricks.com>
Date:   Sun Jan 10 14:49:45 2016 -0800

    [SPARK-10359][PROJECT-INFRA] Backport dev/test-dependencies script to branch-1.6
    
    This patch backports the `dev/test-dependencies` script (from #10461) to branch-1.6.
    
    Author: Josh Rosen <joshrosen@databricks.com>
    
    Closes #10680 from JoshRosen/test-deps-16-backport.

commit 1343cd91e73501aad3bfe0497640dfb68233ad8c
Author: Michael Armbrust <michael@databricks.com>
Date:   Fri Jan 8 15:43:11 2016 -0800

    [SPARK-12696] Backport Dataset Bug fixes to 1.6
    
    We've fixed a lot of bugs in master, and since this is experimental in 1.6 we should consider back porting the fixes.  The only thing that is obviously risky to me is 0e07ed3, we might try to remove that.
    
    Author: Wenchen Fan <wenchen@databricks.com>
    Author: gatorsmile <gatorsmile@gmail.com>
    Author: Liang-Chi Hsieh <viirya@gmail.com>
    Author: Cheng Lian <lian@databricks.com>
    Author: Nong Li <nong@databricks.com>
    
    Closes #10650 from marmbrus/dataset-backports.
    
    (cherry picked from commit a6190508b20673952303eff32b3a559f0a264d03)

commit 00a2971aca9a2797b67171413ce8a2c656b6cbce
Author: Thomas Graves <tgraves@apache.org>
Date:   Fri Jan 8 14:38:19 2016 -0600

    [SPARK-12654] sc.wholeTextFiles with spark.hadoop.cloneConf=true fail…
    
    …s on secure Hadoop
    
    https://issues.apache.org/jira/browse/SPARK-12654
    
    So the bug here is that WholeTextFileRDD.getPartitions has:
    val conf = getConf
    in getConf if the cloneConf=true it creates a new Hadoop Configuration. Then it uses that to create a new newJobContext.
    The newJobContext will copy credentials around, but credentials are only present in a JobConf not in a Hadoop Configuration. So basically when it is cloning the hadoop configuration its changing it from a JobConf to Configuration and dropping the credentials that were there. NewHadoopRDD just uses the conf passed in for the getPartitions (not getConf) which is why it works.
    
    Author: Thomas Graves <tgraves@staydecay.corp.gq1.yahoo.com>
    
    Closes #10651 from tgravescs/SPARK-12654.
    
    (cherry picked from commit 553fd7b912a32476b481fd3f80c1d0664b6c6484)
    Signed-off-by: Tom Graves <tgraves@yahoo-inc.com>
    (cherry picked from commit faf094c7c35baf0e73290596d4ca66b7d083ed5b)

commit af29bb1a88c401b918f458ab40d869e0ab1afdcd
Author: Shixiong Zhu <shixiong@databricks.com>
Date:   Fri Jan 8 02:02:06 2016 -0800

    [SPARK-12591][STREAMING] Register OpenHashMapBasedStateMap for Kryo (branch 1.6)
    
    backport #10609 to branch 1.6
    
    Author: Shixiong Zhu <shixiong@databricks.com>
    
    Closes #10656 from zsxwing/SPARK-12591-branch-1.6.
    
    (cherry picked from commit 0d96c54534d8bfca191c892b98397a176bc46152)

commit 75a018e7480ca3f2a2525f0e70f1b6091989ee01
Author: Darek Blasiak <darek.blasiak@640labs.com>
Date:   Thu Jan 7 21:15:40 2016 +0000

    [SPARK-12598][CORE] bug in setMinPartitions
    
    There is a bug in the calculation of ```maxSplitSize```.  The ```totalLen``` should be divided by ```minPartitions``` and not by ```files.size```.
    
    Author: Darek Blasiak <darek.blasiak@640labs.com>
    
    Closes #10546 from datafarmer/setminpartitionsbug.
    
    (cherry picked from commit 8346518357f4a3565ae41e9a5ccd7e2c3ed6c468)
    Signed-off-by: Sean Owen <sowen@cloudera.com>
    (cherry picked from commit 6ef823544dfbc8c9843bdedccfda06147a1a74fe)

commit 497c14f684fae14d3cd62b49aae9eb8913851cc1
Author: Sameer Agarwal <sameer@databricks.com>
Date:   Thu Jan 7 10:37:15 2016 -0800

    [SPARK-12662][SQL] Fix DataFrame.randomSplit to avoid creating overlapping splits
    
    https://issues.apache.org/jira/browse/SPARK-12662
    
    cc yhuai
    
    Author: Sameer Agarwal <sameer@databricks.com>
    
    Closes #10626 from sameeragarwal/randomsplit.
    
    (cherry picked from commit f194d9911a93fc3a78be820096d4836f22d09976)
    Signed-off-by: Reynold Xin <rxin@databricks.com>
    (cherry picked from commit 017b73e69693cd151516f92640a95a4a66e02dff)

commit b22656931d0779ac10e720f5df212d570ce2ccb0
Author: zero323 <matthew.szymkiewicz@gmail.com>
Date:   Thu Jan 7 10:32:56 2016 -0800

    [SPARK-12006][ML][PYTHON] Fix GMM failure if initialModel is not None
    
    If initial model passed to GMM is not empty it causes net.razorvine.pickle.PickleException. It can be fixed by converting initialModel.weights to list.
    
    Author: zero323 <matthew.szymkiewicz@gmail.com>
    
    Closes #10644 from zero323/SPARK-12006.
    
    (cherry picked from commit 592f64985d0d58b4f6a0366bf975e04ca496bdbe)
    Signed-off-by: Joseph K. Bradley <joseph@databricks.com>
    (cherry picked from commit 69a885a71cfe7c62179e784e7d9eee023d3bb6eb)

commit dd0a0ddb0ced9bd3f1bac094ce18621895bb0be0
Author: Guillaume Poulin <poulin.guillaume@gmail.com>
Date:   Wed Jan 6 21:34:46 2016 -0800

    [SPARK-12678][CORE] MapPartitionsRDD clearDependencies
    
    MapPartitionsRDD was keeping a reference to `prev` after a call to
    `clearDependencies` which could lead to memory leak.
    
    Author: Guillaume Poulin <poulin.guillaume@gmail.com>
    
    Closes #10623 from gpoulin/map_partition_deps.
    
    (cherry picked from commit b6738520374637347ab5ae6c801730cdb6b35daa)
    Signed-off-by: Reynold Xin <rxin@databricks.com>
    (cherry picked from commit d061b852274c12784f3feb96c0cdcab39989f8e7)

commit eaf84599a7c3b75b2ded104821101d652c1881ad
Author: jerryshao <sshao@hortonworks.com>
Date:   Wed Jan 6 21:28:29 2016 -0800

    [SPARK-12673][UI] Add missing uri prepending for job description
    
    Otherwise the url will be failed to proxy to the right one if in YARN mode. Here is the screenshot:
    
    ![screen shot 2016-01-06 at 5 28 26 pm](https://cloud.githubusercontent.com/assets/850797/12139632/bbe78ecc-b49c-11e5-8932-94e8b3622a09.png)
    
    Author: jerryshao <sshao@hortonworks.com>
    
    Closes #10618 from jerryshao/SPARK-12673.
    
    (cherry picked from commit 174e72ceca41a6ac17ad05d50832ee9c561918c0)
    Signed-off-by: Shixiong Zhu <shixiong@databricks.com>
    (cherry picked from commit 94af69c9be70b9d2cd95c26288e2af9599d61e5c)

commit e8f7037755debef08a8bbdd02d62df589efdec6b
Author: Liang-Chi Hsieh <viirya@appier.com>
Date:   Mon Dec 14 09:59:42 2015 -0800

    [SPARK-12016] [MLLIB] [PYSPARK] Wrap Word2VecModel when loading it in pyspark
    
    JIRA: https://issues.apache.org/jira/browse/SPARK-12016
    
    We should not directly use Word2VecModel in pyspark. We need to wrap it in a Word2VecModelWrapper when loading it in pyspark.
    
    Author: Liang-Chi Hsieh <viirya@appier.com>
    
    Closes #10100 from viirya/fix-load-py-wordvecmodel.
    
    (cherry picked from commit b51a4cdff3a7e640a8a66f7a9c17021f3056fd34)
    Signed-off-by: Joseph K. Bradley <joseph@databricks.com>
    (cherry picked from commit 11b901b22b1cdaa6d19b1b73885627ac601be275)

commit bd56448e024bf62dd8b2b6cf991a032a029b55aa
Author: Shixiong Zhu <shixiong@databricks.com>
Date:   Wed Jan 6 12:03:01 2016 -0800

    [SPARK-12617][PYSPARK] Move Py4jCallbackConnectionCleaner to Streaming
    
    Move Py4jCallbackConnectionCleaner to Streaming because the callback server starts only in StreamingContext.
    
    Author: Shixiong Zhu <shixiong@databricks.com>
    
    Closes #10621 from zsxwing/SPARK-12617-2.
    
    (cherry picked from commit 1e6648d62fb82b708ea54c51cd23bfe4f542856e)
    Signed-off-by: Shixiong Zhu <shixiong@databricks.com>
    (cherry picked from commit d821fae0ecca6393d3632977797d72ba594d26a9)

commit b0b30378b31756e1b5296ce7bbe6786d21f1987e
Author: BrianLondon <brian@seatgeek.com>
Date:   Tue Jan 5 23:15:07 2016 +0000

    [SPARK-12453][STREAMING] Remove explicit dependency on aws-java-sdk
    
    Successfully ran kinesis demo on a live, aws hosted kinesis stream against master and 1.6 branches.  For reasons I don't entirely understand it required a manual merge to 1.5 which I did as shown here: https://github.com/BrianLondon/spark/commit/075c22e89bc99d5e99be21f40e0d72154a1e23a2
    
    The demo ran successfully on the 1.5 branch as well.
    
    According to `mvn dependency:tree` it is still pulling a fairly old version of the aws-java-sdk (1.9.37), but this appears to have fixed the kinesis regression in 1.5.2.
    
    Author: BrianLondon <brian@seatgeek.com>
    
    Closes #10492 from BrianLondon/remove-only.
    
    (cherry picked from commit ff89975543b153d0d235c0cac615d45b34aa8fe7)
    Signed-off-by: Sean Owen <sowen@cloudera.com>
    (cherry picked from commit bf3dca2df4dd3be264691be1321e0c700d4f4e32)

commit 06941779ad9ebb53aa96ec7e2b0455269db3d84c
Author: Shixiong Zhu <shixiong@databricks.com>
Date:   Tue Jan 5 13:48:47 2016 -0800

    [SPARK-12511] [PYSPARK] [STREAMING] Make sure PythonDStream.registerSerializer is called only once
    
    There is an issue that Py4J's PythonProxyHandler.finalize blocks forever. (https://github.com/bartdag/py4j/pull/184)
    
    Py4j will create a PythonProxyHandler in Java for "transformer_serializer" when calling "registerSerializer". If we call "registerSerializer" twice, the second PythonProxyHandler will override the first one, then the first one will be GCed and trigger "PythonProxyHandler.finalize". To avoid that, we should not call"registerSerializer" more than once, so that "PythonProxyHandler" in Java side won't be GCed.
    
    Author: Shixiong Zhu <shixiong@databricks.com>
    
    Closes #10514 from zsxwing/SPARK-12511.
    
    (cherry picked from commit 6cfe341ee89baa952929e91d33b9ecbca73a3ea0)
    Signed-off-by: Davies Liu <davies.liu@gmail.com>
    (cherry picked from commit 83fe5cf9a2621d7e53b5792a7c7549c9da7f130a)

commit 208d1a4017d8cb0f3b41fc170cd4e5b5343254cf
Author: Shixiong Zhu <shixiong@databricks.com>
Date:   Tue Jan 5 13:10:46 2016 -0800

    [SPARK-12617] [PYSPARK] Clean up the leak sockets of Py4J
    
    This patch added Py4jCallbackConnectionCleaner to clean the leak sockets of Py4J every 30 seconds. This is a workaround before Py4J fixes the leak issue https://github.com/bartdag/py4j/issues/187
    
    Author: Shixiong Zhu <shixiong@databricks.com>
    
    Closes #10579 from zsxwing/SPARK-12617.
    
    (cherry picked from commit 047a31bb1042867b20132b347b1e08feab4562eb)
    Signed-off-by: Davies Liu <davies.liu@gmail.com>
    (cherry picked from commit f31d0fd9ea12bfe94434671fbcfe3d0e06a4a97d)

commit 051fad748c27787f69541e62fac3c7716cd5d526
Author: Pete Robbins <robbinspg@gmail.com>
Date:   Tue Jan 5 13:10:21 2016 -0800

    [SPARK-12647][SQL] Fix o.a.s.sqlexecution.ExchangeCoordinatorSuite.determining the number of reducers: aggregate operator
    
    change expected partition sizes
    
    Author: Pete Robbins <robbinspg@gmail.com>
    
    Closes #10599 from robbinspg/branch-1.6.
    
    (cherry picked from commit 5afa62b20090e763ba10d9939ec214a11466087b)

commit bdece7d594c52a1150266edd82ff761ce08844b7
Author: Nong Li <nong@databricks.com>
Date:   Mon Jan 4 14:58:24 2016 -0800

    [SPARK-12589][SQL] Fix UnsafeRowParquetRecordReader to properly set the row length.
    
    The reader was previously not setting the row length meaning it was wrong if there were variable
    length columns. This problem does not manifest usually, since the value in the column is correct and
    projecting the row fixes the issue.
    
    Author: Nong Li <nong@databricks.com>
    
    Closes #10576 from nongli/spark-12589.
    
    (cherry picked from commit 34de24abb518e95c4312b77aa107d061ce02c835)
    Signed-off-by: Yin Huai <yhuai@databricks.com>
    
    Conflicts:
    	sql/catalyst/src/main/java/org/apache/spark/sql/catalyst/expressions/UnsafeRow.java
    
    (cherry picked from commit 8ac9198096d1cef9fbc062df8b8bd94fb9e96829)

commit 3af05e641953e09994b972dd91a9ee90d8b77b00
Author: Josh Rosen <joshrosen@databricks.com>
Date:   Mon Jan 4 10:39:42 2016 -0800

    [SPARK-12579][SQL] Force user-specified JDBC driver to take precedence
    
    Spark SQL's JDBC data source allows users to specify an explicit JDBC driver to load (using the `driver` argument), but in the current code it's possible that the user-specified driver will not be used when it comes time to actually create a JDBC connection.
    
    In a nutshell, the problem is that you might have multiple JDBC drivers on the classpath that claim to be able to handle the same subprotocol, so simply registering the user-provided driver class with the our `DriverRegistry` and JDBC's `DriverManager` is not sufficient to ensure that it's actually used when creating the JDBC connection.
    
    This patch addresses this issue by first registering the user-specified driver with the DriverManager, then iterating over the driver manager's loaded drivers in order to obtain the correct driver and use it to create a connection (previously, we just called `DriverManager.getConnection()` directly).
    
    If a user did not specify a JDBC driver to use, then we call `DriverManager.getDriver` to figure out the class of the driver to use, then pass that class's name to executors; this guards against corner-case bugs in situations where the driver and executor JVMs might have different sets of JDBC drivers on their classpaths (previously, there was the (rare) potential for `DriverManager.getConnection()` to use different drivers on the driver and executors if the user had not explicitly specified a JDBC driver class and the classpaths were different).
    
    This patch is inspired by a similar patch that I made to the `spark-redshift` library (https://github.com/databricks/spark-redshift/pull/143), which contains its own modified fork of some of Spark's JDBC data source code (for cross-Spark-version compatibility reasons).
    
    Author: Josh Rosen <joshrosen@databricks.com>
    
    Closes #10519 from JoshRosen/jdbc-driver-precedence.
    
    (cherry picked from commit 6c83d938cc61bd5fabaf2157fcc3936364a83f02)
    Signed-off-by: Yin Huai <yhuai@databricks.com>
    (cherry picked from commit 7f37c1e45d52b7823d566349e2be21366d73651f)

commit e5d145309e9e0e10880be7854bcf0c53e1bb8425
Author: Pete Robbins <robbinspg@gmail.com>
Date:   Mon Jan 4 10:43:21 2016 -0800

    [SPARK-12470] [SQL] Fix size reduction calculation
    
    also only allocate required buffer size
    
    Author: Pete Robbins <robbinspg@gmail.com>
    
    Closes #10421 from robbinspg/master.
    
    (cherry picked from commit b504b6a90a95a723210beb0031ed41a75d702f66)
    Signed-off-by: Davies Liu <davies.liu@gmail.com>
    
    Conflicts:
    	sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/GenerateUnsafeRowJoiner.scala
    
    (cherry picked from commit b5a1f564a3c099ef0b674599f0b012d9346115a3)

commit 96beaa25272316f1b93ac3e6a43d43ac7d1c45a5
Author: Xiu Guo <xguo27@gmail.com>
Date:   Sun Jan 3 20:48:56 2016 -0800

    [SPARK-12562][SQL] DataFrame.write.format(text) requires the column name to be called value
    
    Author: Xiu Guo <xguo27@gmail.com>
    
    Closes #10515 from xguo27/SPARK-12562.
    
    (cherry picked from commit 84f8492c1555bf8ab44c9818752278f61768eb16)
    Signed-off-by: Reynold Xin <rxin@databricks.com>
    (cherry picked from commit f7a322382a3c1eed7088541add55a7813813a958)

commit 2706e55e76cdf40ed4fe30cf9f447cdf992c7a13
Author: felixcheung <felixcheung_m@hotmail.com>
Date:   Sun Jan 3 20:53:35 2016 +0530

    [SPARK-12327][SPARKR] fix code for lintr warning for commented code
    
    shivaram
    
    Author: felixcheung <felixcheung_m@hotmail.com>
    
    Closes #10408 from felixcheung/rcodecomment.
    
    (cherry picked from commit c3d505602de2fd2361633f90e4fff7e041849e28)
    Signed-off-by: Shivaram Venkataraman <shivaram@cs.berkeley.edu>
    (cherry picked from commit 4e9dd16987b3cba19dcf6437f3b6c8aeb59e2e39)

commit 378e29115628b6ff74d0da3602712e374245c671
Author: Carson Wang <carson.wang@intel.com>
Date:   Wed Dec 30 13:49:10 2015 -0800

    [SPARK-12399] Display correct error message when accessing REST API with an unknown app Id
    
    I got an exception when accessing the below REST API with an unknown application Id.
    `http://<server-url>:18080/api/v1/applications/xxx/jobs`
    Instead of an exception, I expect an error message "no such app: xxx" which is a similar error message when I access `/api/v1/applications/xxx`
    ```
    org.spark-project.guava.util.concurrent.UncheckedExecutionException: java.util.NoSuchElementException: no app with key xxx
    	at org.spark-project.guava.cache.LocalCache$Segment.get(LocalCache.java:2263)
    	at org.spark-project.guava.cache.LocalCache.get(LocalCache.java:4000)
    	at org.spark-project.guava.cache.LocalCache.getOrLoad(LocalCache.java:4004)
    	at org.spark-project.guava.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4874)
    	at org.apache.spark.deploy.history.HistoryServer.getSparkUI(HistoryServer.scala:116)
    	at org.apache.spark.status.api.v1.UIRoot$class.withSparkUI(ApiRootResource.scala:226)
    	at org.apache.spark.deploy.history.HistoryServer.withSparkUI(HistoryServer.scala:46)
    	at org.apache.spark.status.api.v1.ApiRootResource.getJobs(ApiRootResource.scala:66)
    ```
    
    Author: Carson Wang <carson.wang@intel.com>
    
    Closes #10352 from carsonwang/unknownAppFix.
    
    (cherry picked from commit b244297966be1d09f8e861cfe2d8e69f7bed84da)
    Signed-off-by: Marcelo Vanzin <vanzin@cloudera.com>
    (cherry picked from commit cd86075b52d6363f674dffc3eb71d90449563879)

commit bf53d92d22980b117151ace95c1bb78dc3fa6415
Author: Holden Karau <holden@us.ibm.com>
Date:   Wed Dec 30 11:14:47 2015 -0800

    [SPARK-12300] [SQL] [PYSPARK] fix schema inferance on local collections
    
    Current schema inference for local python collections halts as soon as there are no NullTypes. This is different than when we specify a sampling ratio of 1.0 on a distributed collection. This could result in incomplete schema information.
    
    Author: Holden Karau <holden@us.ibm.com>
    
    Closes #10275 from holdenk/SPARK-12300-fix-schmea-inferance-on-local-collections.
    
    (cherry picked from commit d1ca634db4ca9db7f0ba7ca38a0e03bcbfec23c9)
    Signed-off-by: Davies Liu <davies.liu@gmail.com>
    (cherry picked from commit 8dc65497152f2c8949b08fddad853d31c4bd9ae5)

commit e223493ca584f2c2d57ce5e5b67a6fa64ee51c47
Author: Forest Fang <forest.fang@outlook.com>
Date:   Tue Dec 29 12:45:24 2015 +0530

    [SPARK-12526][SPARKR] ifelse`, `when`, `otherwise` unable to take Column as value
    
    `ifelse`, `when`, `otherwise` is unable to take `Column` typed S4 object as values.
    
    For example:
    ```r
    ifelse(lit(1) == lit(1), lit(2), lit(3))
    ifelse(df$mpg > 0, df$mpg, 0)
    ```
    will both fail with
    ```r
    attempt to replicate an object of type 'environment'
    ```
    
    The PR replaces `ifelse` calls with `if ... else ...` inside the function implementations to avoid attempt to vectorize(i.e. `rep()`). It remains to be discussed whether we should instead support vectorization in these functions for consistency because `ifelse` in base R is vectorized but I cannot foresee any scenarios these functions will want to be vectorized in SparkR.
    
    For reference, added test cases which trigger failures:
    ```r
    . Error: when(), otherwise() and ifelse() with column on a DataFrame ----------
    error in evaluating the argument 'x' in selecting a method for function 'collect':
      error in evaluating the argument 'col' in selecting a method for function 'select':
      attempt to replicate an object of type 'environment'
    Calls: when -> when -> ifelse -> ifelse
    
    1: withCallingHandlers(eval(code, new_test_environment), error = capture_calls, message = function(c) invokeRestart("muffleMessage"))
    2: eval(code, new_test_environment)
    3: eval(expr, envir, enclos)
    4: expect_equal(collect(select(df, when(df$a > 1 & df$b > 2, lit(1))))[, 1], c(NA, 1)) at test_sparkSQL.R:1126
    5: expect_that(object, equals(expected, label = expected.label, ...), info = info, label = label)
    6: condition(object)
    7: compare(actual, expected, ...)
    8: collect(select(df, when(df$a > 1 & df$b > 2, lit(1))))
    Error: Test failures
    Execution halted
    ```
    
    Author: Forest Fang <forest.fang@outlook.com>
    
    Closes #10481 from saurfang/spark-12526.
    
    (cherry picked from commit d80cc90b5545cff82cd9b340f12d01eafc9ca524)
    Signed-off-by: Shivaram Venkataraman <shivaram@cs.berkeley.edu>
    (cherry picked from commit c069ffc2b13879f471e6d888116f45f6a8902236)

commit 24c49a9e015d8f761f0f02ad1b9e39be8f1d8e79
Author: Takeshi YAMAMURO <linguin.m.s@gmail.com>
Date:   Mon Dec 28 21:28:32 2015 -0800

    [SPARK-11394][SQL] Throw IllegalArgumentException for unsupported types in postgresql
    
    If DataFrame has BYTE types, throws an exception:
    org.postgresql.util.PSQLException: ERROR: type "byte" does not exist
    
    Author: Takeshi YAMAMURO <linguin.m.s@gmail.com>
    
    Closes #9350 from maropu/FixBugInPostgreJdbc.
    
    (cherry picked from commit 73862a1eb9744c3c32458c9c6f6431c23783786a)
    Signed-off-by: Yin Huai <yhuai@databricks.com>
    (cherry picked from commit 85a871818ee1134deb29387c78c6ce21eb6d2acb)

commit 96f024ba083165fa2ca227254826274aad14db20
Author: Shixiong Zhu <shixiong@databricks.com>
Date:   Mon Dec 28 15:01:51 2015 -0800

    [SPARK-12489][CORE][SQL][MLIB] Fix minor issues found by FindBugs
    
    Include the following changes:
    
    1. Close `java.sql.Statement`
    2. Fix incorrect `asInstanceOf`.
    3. Remove unnecessary `synchronized` and `ReentrantLock`.
    
    Author: Shixiong Zhu <shixiong@databricks.com>
    
    Closes #10440 from zsxwing/findbugs.
    
    (cherry picked from commit 710b41172958a0b3a2b70c48821aefc81893731b)
    Signed-off-by: Shixiong Zhu <shixiong@databricks.com>
    (cherry picked from commit fd202485ace613d9930d0ede48ba8a65920004db)

commit f9c6f4c2104b78fc5fb48f547f5a5d2599027f75
Author: Kousuke Saruta <sarutak@oss.nttdata.co.jp>
Date:   Tue Dec 29 05:33:19 2015 +0900

    [SPARK-12424][ML] The implementation of ParamMap#filter is wrong.
    
    ParamMap#filter uses `mutable.Map#filterKeys`. The return type of `filterKey` is collection.Map, not mutable.Map but the result is casted to mutable.Map using `asInstanceOf` so we get `ClassCastException`.
    Also, the return type of Map#filterKeys is not Serializable. It's the issue of Scala (https://issues.scala-lang.org/browse/SI-6654).
    
    Author: Kousuke Saruta <sarutak@oss.nttdata.co.jp>
    
    Closes #10381 from sarutak/SPARK-12424.
    
    (cherry picked from commit 07165ca06fe0866677525f85fec25e4dbd336674)
    Signed-off-by: Kousuke Saruta <sarutak@oss.nttdata.co.jp>
    (cherry picked from commit 7c7d76f34c0e09aae12f03e7c2922d4eb50d1830)

commit b2e77ad065557c73890fe28402bd4d7cfa298f4a
Author: Yaron Weinsberg <wyaron@gmail.com>
Date:   Tue Dec 29 05:19:11 2015 +0900

    [SPARK-12517] add default RDD name for one created via sc.textFile
    
    The feature was first added at commit: 7b877b27053bfb7092e250e01a3b887e1b50a109 but was later removed (probably by mistake) at commit: fc8b58195afa67fbb75b4c8303e022f703cbf007.
    This change sets the default path of RDDs created via sc.textFile(...) to the path argument.
    
    Here is the symptom:
    
    * Using spark-1.5.2-bin-hadoop2.6:
    
    scala> sc.textFile("/home/root/.bashrc").name
    res5: String = null
    
    scala> sc.binaryFiles("/home/root/.bashrc").name
    res6: String = /home/root/.bashrc
    
    * while using Spark 1.3.1:
    
    scala> sc.textFile("/home/root/.bashrc").name
    res0: String = /home/root/.bashrc
    
    scala> sc.binaryFiles("/home/root/.bashrc").name
    res1: String = /home/root/.bashrc
    
    Author: Yaron Weinsberg <wyaron@gmail.com>
    Author: yaron <yaron@il.ibm.com>
    
    Closes #10456 from wyaron/master.
    
    (cherry picked from commit 73b70f076d4e22396b7e145f2ce5974fbf788048)
    Signed-off-by: Kousuke Saruta <sarutak@oss.nttdata.co.jp>
    (cherry picked from commit 1fbcb6e7be9cd9fa5255837cfc5358f2283f4aaf)

commit c49e10d08a5f301984c96281b7aff06af2509472
Author: CK50 <christian.kurz@oracle.com>
Date:   Thu Dec 24 13:39:11 2015 +0000

    [SPARK-12010][SQL] Spark JDBC requires support for column-name-free INSERT syntax
    
    In the past Spark JDBC write only worked with technologies which support the following INSERT statement syntax (JdbcUtils.scala: insertStatement()):
    
    INSERT INTO $table VALUES ( ?, ?, ..., ? )
    
    But some technologies require a list of column names:
    
    INSERT INTO $table ( $colNameList ) VALUES ( ?, ?, ..., ? )
    
    This was blocking the use of e.g. the Progress JDBC Driver for Cassandra.
    
    Another limitation is that syntax 1 relies no the dataframe field ordering match that of the target table. This works fine, as long as the target table has been created by writer.jdbc().
    
    If the target table contains more columns (not created by writer.jdbc()), then the insert fails due mismatch of number of columns or their data types.
    
    This PR switches to the recommended second INSERT syntax. Column names are taken from datafram field names.
    
    Author: CK50 <christian.kurz@oracle.com>
    
    Closes #10380 from CK50/master-SPARK-12010-2.
    
    (cherry picked from commit 502476e45c314a1229b3bce1c61f5cb94a9fc04b)
    Signed-off-by: Sean Owen <sowen@cloudera.com>
    (cherry picked from commit 865dd8bccfc994310ad6664151d469043706ef3b)

commit 7f9375780b9066ec26ee9ae6ffaae24f867baccd
Author: Kazuaki Ishizaki <ishizaki@jp.ibm.com>
Date:   Thu Dec 24 21:27:55 2015 +0900

    [SPARK-12502][BUILD][PYTHON] Script /dev/run-tests fails when IBM Java is used
    
    fix an exception with IBM JDK by removing update field from a JavaVersion tuple. This is because IBM JDK does not have information on update '_xx'
    
    Author: Kazuaki Ishizaki <ishizaki@jp.ibm.com>
    
    Closes #10463 from kiszk/SPARK-12502.
    
    (cherry picked from commit 9e85bb71ad2d7d3a9da0cb8853f3216d37e6ff47)
    Signed-off-by: Kousuke Saruta <sarutak@oss.nttdata.co.jp>
    (cherry picked from commit 4dd8712c1b64a64da0fa0413e2c9be68ad0ddc17)

commit b4a365d30999629938c7728829a4f5bafb254a47
Author: Adrian Bridgett <adrian@smop.co.uk>
Date:   Wed Dec 23 16:00:03 2015 -0800

    [SPARK-12499][BUILD] don't force MAVEN_OPTS
    
    allow the user to override MAVEN_OPTS (2GB wasn't sufficient for me)
    
    Author: Adrian Bridgett <adrian@smop.co.uk>
    
    Closes #10448 from abridgett/feature/do_not_force_maven_opts.
    
    (cherry picked from commit ead6abf7e7fc14b451214951d4991d497aa65e63)
    Signed-off-by: Josh Rosen <joshrosen@databricks.com>
    (cherry picked from commit 5987b1658b837400691160c38ba6eedc47274ee4)

commit 5d798162aa68f2d9976bee973ab36665e705196c
Author: pierre-borckmans <pierre.borckmans@realimpactanalytics.com>
Date:   Tue Dec 22 23:00:42 2015 -0800

    [SPARK-12477][SQL] - Tungsten projection fails for null values in array fields
    
    Accessing null elements in an array field fails when tungsten is enabled.
    It works in Spark 1.3.1, and in Spark > 1.5 with Tungsten disabled.
    
    This PR solves this by checking if the accessed element in the array field is null, in the generated code.
    
    Example:
    ```
    // Array of String
    case class AS( as: Seq[String] )
    val dfAS = sc.parallelize( Seq( AS ( Seq("a",null,"b") ) ) ).toDF
    dfAS.registerTempTable("T_AS")
    for (i <- 0 to 2) { println(i + " = " + sqlContext.sql(s"select as[$i] from T_AS").collect.mkString(","))}
    ```
    
    With Tungsten disabled:
    ```
    0 = [a]
    1 = [null]
    2 = [b]
    ```
    
    With Tungsten enabled:
    ```
    0 = [a]
    15/12/22 09:32:50 ERROR Executor: Exception in task 7.0 in stage 1.0 (TID 15)
    java.lang.NullPointerException
    	at org.apache.spark.sql.catalyst.expressions.UnsafeRowWriters$UTF8StringWriter.getSize(UnsafeRowWriters.java:90)
    	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)
    	at org.apache.spark.sql.execution.TungstenProject$$anonfun$3$$anonfun$apply$3.apply(basicOperators.scala:90)
    	at org.apache.spark.sql.execution.TungstenProject$$anonfun$3$$anonfun$apply$3.apply(basicOperators.scala:88)
    	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
    	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
    	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
    	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
    ```
    
    Author: pierre-borckmans <pierre.borckmans@realimpactanalytics.com>
    
    Closes #10429 from pierre-borckmans/SPARK-12477_Tungsten-Projection-Null-Element-In-Array.
    
    (cherry picked from commit 43b2a6390087b7ce262a54dc8ab8dd825db62e21)
    Signed-off-by: Reynold Xin <rxin@databricks.com>
    (cherry picked from commit c6c9bf99af0ee0559248ad772460e9b2efde5861)

commit ee0428726145f276752c39dd5de9957221ea80e7
Author: Cheng Lian <lian@databricks.com>
Date:   Wed Dec 9 23:30:42 2015 +0800

    [SPARK-12012][SQL] Show more comprehensive PhysicalRDD metadata when visualizing SQL query plan
    
    This PR adds a `private[sql]` method `metadata` to `SparkPlan`, which can be used to describe detail information about a physical plan during visualization. Specifically, this PR uses this method to provide details of `PhysicalRDD`s translated from a data source relation. For example, a `ParquetRelation` converted from Hive metastore table `default.psrc` is now shown as the following screenshot:
    
    ![image](https://cloud.githubusercontent.com/assets/230655/11526657/e10cb7e6-9916-11e5-9afa-f108932ec890.png)
    
    And here is the screenshot for a regular `ParquetRelation` (not converted from Hive metastore table) loaded from a really long path:
    
    ![output](https://cloud.githubusercontent.com/assets/230655/11680582/37c66460-9e94-11e5-8f50-842db5309d5a.png)
    
    Author: Cheng Lian <lian@databricks.com>
    
    Closes #10004 from liancheng/spark-12012.physical-rdd-metadata.
    
    (cherry picked from commit 6e1c55eac4849669e119ce0d51f6d051830deb9f)

commit 43846d5133dcd03b1e78ef7f52563c65f8d2910f
Author: Carson Wang <carson.wang@intel.com>
Date:   Thu Dec 3 16:39:12 2015 -0800

    [SPARK-11206] Support SQL UI on the history server (resubmit)
    
    Resubmit #9297 and #9991
    On the live web UI, there is a SQL tab which provides valuable information for the SQL query. But once the workload is finished, we won't see the SQL tab on the history server. It will be helpful if we support SQL UI on the history server so we can analyze it even after its execution.
    
    To support SQL UI on the history server:
    1. I added an onOtherEvent method to the SparkListener trait and post all SQL related events to the same event bus.
    2. Two SQL events SparkListenerSQLExecutionStart and SparkListenerSQLExecutionEnd are defined in the sql module.
    3. The new SQL events are written to event log using Jackson.
    4. A new trait SparkHistoryListenerFactory is added to allow the history server to feed events to the SQL history listener. The SQL implementation is loaded at runtime using java.util.ServiceLoader.
    
    Author: Carson Wang <carson.wang@intel.com>
    
    Closes #10061 from carsonwang/SqlHistoryUI.
    
    (cherry picked from commit b6e9963ee4bf0ffb62c8e9829a551bcdc31e12e3)

commit 53383ac43dae197e8206d5b5b2d98407568f9c83
Author: Imran Rashid <irashid@cloudera.com>
Date:   Mon Jan 25 12:58:55 2016 -0800

    CLOUDERA-BUILD [CDH-36259] Disable the Unified Memory Manager by default in CDH5.7

commit d7759bf009ef6d2d3e8b2828f45043c0d39192d0
Author: scwf <wangfei1@huawei.com>
Date:   Tue Jan 19 14:49:55 2016 -0800

    [SPARK-2750][WEB UI] Add https support to the Web UI
    
    Author: scwf <wangfei1@huawei.com>
    Author: Marcelo Vanzin <vanzin@cloudera.com>
    Author: WangTaoTheTonic <wangtao111@huawei.com>
    Author: w00228970 <wangfei1@huawei.com>
    
    Closes #10238 from vanzin/SPARK-2750.
    
    (cherry picked from commit 43f1d59e17d89d19b322d639c5069a3fc0c8e2ed)

commit 01462646de8e46e93d78622520bac3834aa26520
Author: Marcelo Vanzin <vanzin@cloudera.com>
Date:   Fri Jan 22 16:29:57 2016 -0800

    CLOUDERA-BUILD. Fix Spark's use of Hive's VariableSubtitution.
    
    This (internal) Hive API has changed and adjustments are needed in
    Spark for things to compile.

commit f07cd3551919fdc5140f34fbf88711fd2009f5f9
Author: Marcelo Vanzin <vanzin@cloudera.com>
Date:   Fri Dec 18 09:49:08 2015 -0800

    [SPARK-12350][CORE] Don't log errors when requested stream is not found.
    
    If a client requests a non-existent stream, just send a failure message
    back, without logging any error on the server side (since it's not a
    server error).
    
    On the executor side, avoid error logs by translating any errors during
    transfer to a `ClassNotFoundException`, so that loading the class is
    retried on a the parent class loader. This can mask IO errors during
    transmission, but the most common cause is that the class is not
    served by the remote end.
    
    Author: Marcelo Vanzin <vanzin@cloudera.com>
    
    Closes #10337 from vanzin/SPARK-12350.
    
    (cherry picked from commit 2782818287a71925523c1320291db6cb25221e9f)

commit e0d03eb30e03f589407c3cf37317a64f18db8257
Author: Marcelo Vanzin <vanzin@cloudera.com>
Date:   Thu Dec 10 13:26:30 2015 -0800

    [SPARK-11563][CORE][REPL] Use RpcEnv to transfer REPL-generated classes.
    
    This avoids bringing up yet another HTTP server on the driver, and
    instead reuses the file server already managed by the driver's
    RpcEnv. As a bonus, the repl now inherits the security features of
    the network library.
    
    There's also a small change to create the directory for storing classes
    under the root temp dir for the application (instead of directly
    under java.io.tmpdir).
    
    Author: Marcelo Vanzin <vanzin@cloudera.com>
    
    Closes #9923 from vanzin/SPARK-11563.
    
    (cherry picked from commit 4a46b8859d3314b5b45a67cdc5c81fecb6e9e78c)

commit 124a71fe7eaee16d0d3c25088730759f74644f12
Author: Marcelo Vanzin <vanzin@cloudera.com>
Date:   Wed Jan 13 12:39:32 2016 -0800

    CLOUDERA-BUILD. CDH-28174. Enable netty-based file server.
    
    SPARK-11140 was actually backported to Spark 1.6, but in a disabled
    state; this change just undoes the disabling so that the Netty code
    is used to transfer files, instead of the HTTP server.

commit 5e5eff8a516de68ff1f79f0492b292ec013a3b15
Author: Mark Grover <mark@apache.org>
Date:   Mon Jan 11 16:56:16 2016 -0800

    CLOUDERA-BUILD. Use Cloudera's Kafka version to build against, instead of upstream Kafka

commit 0da1341ab199734145ea8616c5ff2a9ca28cc32d
Author: Marcelo Vanzin <vanzin@cloudera.com>
Date:   Wed Sep 30 12:24:03 2015 -0700

    CLOUDERA-BUILD. CDH-32176. Add option to not localize configuration.
    
    Oozie does not have control over env variables that define where the
    hadoop config lives, and sometimes Spark may fail to localize that
    configuration; this change adds a config option to disable localization
    so that Oozie can launch Spark jobs on secure clusters, at the expense
    of relying on the Hadoop config propagated from the YARN node manager
    launching the container.
    
    (cherry picked from commit eff2156aa181b53882bfc59c15bf2c0dc99901be)

commit 7ec0d7b3f02797ef80bd3e83a68627bb61d6e053
Author: Marcelo Vanzin <vanzin@cloudera.com>
Date:   Tue Nov 24 15:08:02 2015 -0600

    [SPARK-11929][CORE] Make the repl log4j configuration override the root logger.
    
    In the default Spark distribution, there are currently two separate
    log4j config files, with different default values for the root logger,
    so that when running the shell you have a different default log level.
    This makes the shell more usable, since the logs don't overwhelm the
    output.
    
    But if you install a custom log4j.properties, you lose that, because
    then it's going to be used no matter whether you're running a regular
    app or the shell.
    
    With this change, the overriding of the log level is done differently;
    the log level repl's main class (org.apache.spark.repl.Main) is used
    to define the root logger's level when running the shell, defaulting
    to WARN if it's not set explicitly.
    
    On a somewhat related change, the shell output about the "sc" variable
    was changed a bit to contain a little more useful information about
    the application, since when the root logger's log level is WARN, that
    information is never shown to the user.
    
    Author: Marcelo Vanzin <vanzin@cloudera.com>
    
    Closes #9816 from vanzin/shell-logging.
    
    (cherry picked from commit e6dd237463d2de8c506f0735dfdb3f43e8122513)

commit c312e63943957ff0dccffe77f3f17163ac82a037
Author: Imran Rashid <irashid@cloudera.com>
Date:   Tue Jan 5 17:14:02 2016 -0600

    CLOUDERA-BUILD update spark version

commit 4050606d6b90f13b1aa1e4619a019de3e3f2a94b
Author: Marcelo Vanzin <vanzin@cloudera.com>
Date:   Fri Dec 4 16:05:51 2015 -0800

    CLOUDERA-BUILD. Revert "[SPARK-11783][SQL] Fixes execution Hive client when using remote Hive metastore"
    
    This reverts commit c7f95df5c6d8eb2e6f11cf58b704fea34326a5f2. CDH uses the
    same version for execution and metadata; connecting to other metastores is
    not supported. And removing this change speeds up the startup of apps that
    use HiveContext.
    
    (cherry picked from commit fa708bcd592e76875d2a8077c924e748c5b9cea5)

commit 10a981707ed5f5fe495aa0ba438fc1db1c01145d
Author: Marcelo Vanzin <vanzin@cloudera.com>
Date:   Mon Nov 30 16:53:31 2015 -0800

    CLOUDERA-BUILD. CDH-35052. Write parquet legacy format by default.
    
    Otherwise Hive cannot read decimal fields.
    
    (cherry picked from commit 7086bcf8b48d05cf1c3a6f27e0df9714a5b0f0f8)

commit cc2ea58c992f1335e780571c80d2558e990527e0
Author: Mark Grover <mgrover@cloudera.com>
Date:   Tue Oct 6 13:40:28 2015 -0700

    CLOUDERA-BUILD. CDH-29819: Add spark-avro to Spark
    
    (cherry picked from commit 68137f06aea99009f64965245c7b27c3a5ec01bf)

commit f10f45f6fc50f664f126155cd322f1b1abbae839
Author: Mark Grover <mgrover@cloudera.com>
Date:   Fri Nov 20 09:51:26 2015 -0800

    CLOUDERA-BUILD. Revert "[SPARK-7743] [SQL] Parquet 1.7"
    
    (cherry picked from commit 69e6895092649b96cf322ef66dc42cd0ec1cd42c)

commit 47367efac429368970e62a2906db048909def02d
Author: Marcelo Vanzin <vanzin@cloudera.com>
Date:   Thu Nov 12 15:57:18 2015 -0800

    CLOUDERA-BUILD. CDH-33655. Use version of Spark defined in user configuration.
    
    During rolling upgrades, the activated parcel and the active client
    configuration in /etc/spark may not match. This may cause jobs to fail to
    launch because the config is tailored for a particular Spark version.
    
    This change detects when the versions do not match and runs Spark from the
    version defined in the configuration, warning the user that this is
    happening.
    
    (cherry picked from commit 5ce1d7c599364a07ce0b98793b56e302e0025c23)

commit ff4215307139a9e81d6dc89536dc62184618ff8a
Author: Marcelo Vanzin <vanzin@cloudera.com>
Date:   Thu Nov 5 17:03:18 2015 -0800

    CLOUDERA-BUILD. Remove Snappy from codecs that support concatenation.
    
    CDH ships an older version of Snappy that doesn't support the feature.
    
    (cherry picked from commit 75db083a7e0ef62e520636919e7d9682f5c1f3aa)

commit 7940fb0894c6647bf7f78bcb52e72d211c7725b5
Author: Marcelo Vanzin <vanzin@cloudera.com>
Date:   Fri Oct 23 13:35:07 2015 -0700

    CLOUDERA-BUILD. Revert "[SPARK-11153][SQL] Disables Parquet filter push-down for string and binary columns"
    
    This reverts commit 89e6db6150704deab46232352d1986bc1449883b.
    
    (cherry picked from commit 5b602eee13705bdd7d946bb17dd9135b918c00bd)

commit f3e5b902431a89c210005b20dafe909d180dd9c3
Author: Marcelo Vanzin <vanzin@cloudera.com>
Date:   Fri Oct 9 11:33:04 2015 -0700

    CLOUDERA-BUILD. CDH-31874. Remove scripts that don't work from package.
    
    CDH doesn't include the code that these scripts need, so they just don't
    work.
    
    (cherry picked from commit 52aff1dea7e3e6fef3793a78a68fbe3a50e47a6f)

commit 9b42a59736b3a9181aee80b984d7b16ca2ec1be1
Author: Marcelo Vanzin <vanzin@cloudera.com>
Date:   Thu Aug 13 17:47:56 2015 -0700

    CLOUDERA-BUILD. More dependency tweaks.
    
    Remove a few more things from the Spark assemblies, since they're
    already provided in CDH.
    
    (cherry picked from commit ec20f3dcb66ff93bed482e46f8716e7c729f852c)

commit 3e1e89208c6fe6007ae24e0cd4d45737594fecc6
Author: Marcelo Vanzin <vanzin@cloudera.com>
Date:   Wed Aug 26 11:14:32 2015 -0700

    CLOUDERA-BUILD. CDH-30545. Allow overriding the mvn command to run.
    
    (cherry picked from commit cf4cec5e562a498d32031bce4126bea189e3d2ee)

commit 6c5fa0339e01332ba4b16f878a74f293e5191fdc
Author: Marcelo Vanzin <vanzin@cloudera.com>
Date:   Wed Aug 19 14:25:02 2015 -0700

    CLOUDERA-BUILD. Partial revert of 5e6fdc6 (SPARK-9407).
    
    PARQUET-201 is fixed in CDH. Keep the refactoring, but remove the hack.
    
    (cherry picked from commit 2fc45c0c30433e42e4c8df9692951ace22c2e499)

commit b546491b0deeffbbe6a68ec30b6b499100f7ac3a
Author: Marcelo Vanzin <vanzin@cloudera.com>
Date:   Thu Aug 13 15:47:28 2015 -0700

    CLOUDERA-BUILD. CDH-29312. Package kafka backend with Spark.
    
    This makes it easy to deploy streaming applications that use Kafka
    in CDH, by removing the requirement to package the integration bits
    with the application.
    
    (cherry picked from commit 8c4dd4b9be8cf4841816fe4953934705e693c194)

commit a816dd1b4859b0064019daa4d48f85e69c9c11d9
Author: Marcelo Vanzin <vanzin@cloudera.com>
Date:   Mon Aug 3 17:44:23 2015 -0700

    CLOUDERA-BUILD. Use Hive 1.1 for HiveQL execution.
    
    A few changes:
    - Revert to TOK_UNION (instead of TOK_UNIONALL added in 1.2)
    - Set SCRATCHDIR in configuration appropriately so that HiveContext
      doesn't pollute /tmp
    - Fix some dependencies so that all datanucleus artifacts are compatible.
    - Lock execution and metastore versions since they're the same
      in CDH. Update VersionsSuite to only test that case (to speed it up).
    - Disable tests that fail on CDH (because it has different deps).
    - Disable code that uses new grammar added in Hive 1.2.
    - Reduce parallelism of a test to avoid shuffle memory tracker issues.
    - Disable the thrift server (not supported in CDH).
    - Disable tests that are incompatible with Hive 1.1 due to incompatible
      DecimalType support.
    
    (cherry picked from commit 89ac1c547858748b8995cf22d7d9aa298264f6cb)

commit cb265791ac14b87e47461d3e08373a3b1d0ad0f4
Author: Marcelo Vanzin <vanzin@cloudera.com>
Date:   Tue Jul 21 11:10:01 2015 -0700

    CLOUDERA-BUILD. Allow multiple paths in HADOOP_CONF_DIR.
    
    This allows us to append Hive configuration to the YARN configuration
    CM generates for Spark.
    
    (cherry picked from commit fadb73fa384a5239b95dc88f0659c38d21eb1cb1)

commit 42a401412517cadb288092cd88bed09a2f420231
Author: Marcelo Vanzin <vanzin@cloudera.com>
Date:   Mon Jul 13 10:45:38 2015 -0700

    CLOUDERA-BUILD. CDH-26527. Package flume backend in Spark assembly.
    
    This makes it easier for applications that use the flume backend to
    be deployed in CDH, since they don't need to worry about how to
    distribute / package that dependency.
    
    (cherry picked from commit bfcd035dc3cc59ad83ad7cc8f60553a6b468b314)

commit 2736be7d6a9f0a4b55c9b63de12088c379dc61b0
Author: Marcelo Vanzin <vanzin@cloudera.com>
Date:   Wed Jul 1 16:20:06 2015 -0700

    CLOUDERA-BUILD. Disable slow tests.
    
    These tests consistently fail in our build machines due to timeouts.
    SparkListenerWithClusterSuite also seems racy, on top of the timeout issue.
    
    (cherry picked from commit 7e5dbb9b795bc54f305cdf2e17c60f4679887a87)

commit 6abe381430ed3a306be19de71e9515c58a10caaa
Author: Marcelo Vanzin <vanzin@cloudera.com>
Date:   Mon Jun 8 14:06:11 2015 -0700

    CLOUDERA-BUILD. Fix AkkaRpcEnv for old akka version.
    
    (cherry picked from commit e9016361b76fac9fabf61d3fd05316dd82fc6f0e)

commit 5711a21c401065d518132d832bca0ed006bb047f
Author: Marcelo Vanzin <vanzin@cloudera.com>
Date:   Wed Apr 29 13:46:54 2015 -0700

    CLOUDERA-BUILD. Disable InputStreamsSuite.
    
    These tests tend to hang and cause builds to time out.
    
    (cherry picked from commit 4c277e53dc4fa17133ab06f2329e4e69da3dc049)

commit 31fba10a7454000e27459ebcb9fc2b0db0bb3109
Author: Marcelo Vanzin <vanzin@cloudera.com>
Date:   Tue Feb 24 16:31:15 2015 -0800

    CLOUDERA-BUILD. Deploy the assembly artifact.
    
    This is needed for Oozie to consume it.
    
    (cherry picked from commit b3263cab3e12d0d72c58b0214966e3d035b58791)

commit c1bdf49b3968e04ddfa2f49ce19123e204211aca
Author: Marcelo Vanzin <vanzin@cloudera.com>
Date:   Wed Feb 18 13:26:05 2015 -0800

    CLOUDERA-BUILD. CDH-25336. Disable Yarn classpath tests.
    
    The Snappy library used in CDH does not play well with the class
    loaders used when enabling userClassPathFirst.
    
    (cherry picked from commit 45dc8e1427ea06e45c0e31f8b4afba58e970a084)

commit 49a9e61130abffa93997e31f7a1eff63b982aa2d
Author: Andrew Bayer <andrew.bayer@gmail.com>
Date:   Fri Jan 16 13:02:36 2015 -0800

    CLOUDERA-BUILD. Add profile for faster package builds
    
    (cherry picked from commit de79f1b48437031e0a7493bbabae37925bd1553f)

commit f23ec2c4e1d8ce4b638adb1ac221f0609b5cc8d2
Author: Marcelo Vanzin <vanzin@cloudera.com>
Date:   Wed Nov 12 17:00:49 2014 -0800

    CLOUDERA-BUILD. Exclude hadoop-aws dependency.
    
    It's not used by Spark and it brings in an older com.fasterxml.jackson
    dependency which conflicts with the version used by other Spark
    dependencies.
    
    (cherry picked from commit a9532a345490fb34cd1ee31a67aea3a4f94a37c4)
    (cherry picked from commit 81aeb7a96f6f02c9bfbdcea90bd299aa9dba1452)

commit e28ea44efdf0108fe9101484522c505d0dba1aea
Author: Marcelo Vanzin <vanzin@cloudera.com>
Date:   Thu Nov 6 12:55:59 2014 -0800

    CLOUDERA-BUILD. Revert "[SPARK-2805] Upgrade Akka to 2.3.4"
    
    This reverts commit 411cf29fff011561f0093bb6101af87842828369.
    
    (cherry picked from commit b36742cf56ae6c4f0b2d433674c2ee3e519c15e8)
    
    Conflicts:
    	pom.xml
    
    (cherry picked from commit 747a41cea926ae5630c8a74ad7c797b4311c02c7)

commit 0f5eee614ba0a4bb2775fd5482401c6777db97eb
Author: Marcelo Vanzin <vanzin@cloudera.com>
Date:   Wed Nov 5 11:45:46 2014 -0800

    CLOUDERA-BUILD. Changes for CDH build.
    
    Adjusts dependency versions, adds Cloudera repos, and triggers
    all needed profiles based on the "cdh.build" property.
    
    (cherry picked from commit 018bcc32602c05ebb8b59d7bed8158cf25b00ca4)
