From 4e0ef9ee20937e964ad6b5de56db4b95703894b0 Mon Sep 17 00:00:00 2001
From: Salil Surendran <salilsurendran@cloudera.com>
Date: Tue, 21 Feb 2017 14:05:12 -0800
Subject: [PATCH 492/517] CDH-50366. Lineage should output lineage data even
 when there are no inputs

If a dataframe is created out of a sequence of numbers rather than from a hive table or parquet file and written to a table or file then lineage data should contain the output data. Joins with existing tables of such dataframes should only give that table as input.
---
 .../spark/sql/query/analysis/QueryAnalysis.scala   |   11 +-
 .../query/analysis/HiveQueryAnalysisSuite.scala    |  125 +++++++++++++++-----
 .../spark/sql/query/analysis/TestUtils.scala       |    5 +-
 3 files changed, 104 insertions(+), 37 deletions(-)

diff --git a/lineage/src/main/scala/org/apache/spark/sql/query/analysis/QueryAnalysis.scala b/lineage/src/main/scala/org/apache/spark/sql/query/analysis/QueryAnalysis.scala
index e484f7f..8c9acdf 100644
--- a/lineage/src/main/scala/org/apache/spark/sql/query/analysis/QueryAnalysis.scala
+++ b/lineage/src/main/scala/org/apache/spark/sql/query/analysis/QueryAnalysis.scala
@@ -46,7 +46,7 @@ import scala.collection.mutable.ListBuffer
 object QueryAnalysis {
 
   def getOutputMetaData(qe: QueryExecution): Option[QueryDetails] = {
-    getTopLevelNamedExpressions(qe.optimizedPlan) match {
+    getTopLevelNamedExpressions(qe.optimizedPlan, true) match {
       case Some(s) => getSource(qe, qe.outputParams, s.map(_.name))
       case None => None
     }
@@ -155,14 +155,17 @@ object QueryAnalysis {
     getTopLevelNamedExpressions(plan).map(getAttributesReferences)
   }
 
-  private def getTopLevelNamedExpressions(plan: LogicalPlan): Option[List[NamedExpression]] = {
+  private def getTopLevelNamedExpressions(
+      plan: LogicalPlan,
+      output: Boolean = false): Option[List[NamedExpression]] = {
     plan.collectFirst {
       case p @ Project(_, _) => p.projectList.toList
       case Join(left, right, _, _) => {
-        val leftTop = getTopLevelNamedExpressions(left).getOrElse(List.empty)
-        val rightTop = getTopLevelNamedExpressions(right).getOrElse(List.empty)
+        val leftTop = getTopLevelNamedExpressions(left, output).getOrElse(List.empty)
+        val rightTop = getTopLevelNamedExpressions(right, output).getOrElse(List.empty)
         leftTop ::: rightTop
       }
+      case l: LocalRelation if output => l.output.toList
       case l: LogicalRelation => l.output.toList
       case m: MetastoreRelation => m.attributes.toList
     }
diff --git a/lineage/src/test/scala/org/apache/spark/sql/query/analysis/HiveQueryAnalysisSuite.scala b/lineage/src/test/scala/org/apache/spark/sql/query/analysis/HiveQueryAnalysisSuite.scala
index f3854ae..adfaa4f 100644
--- a/lineage/src/test/scala/org/apache/spark/sql/query/analysis/HiveQueryAnalysisSuite.scala
+++ b/lineage/src/test/scala/org/apache/spark/sql/query/analysis/HiveQueryAnalysisSuite.scala
@@ -25,12 +25,14 @@ import org.apache.spark.sql.test.SQLTestUtils
 import org.apache.spark.sql.hive.test.TestHive._
 import org.apache.spark.sql.query.analysis.TestUtils._
 
-
 /**
  * Tests that check that reading and writing to Hive tables produce the desired lineage data
  */
-class HiveQueryAnalysisSuite extends SparkFunSuite with TestHiveSingleton with SQLTestUtils
-  with ParquetHDFSTest {
+class HiveQueryAnalysisSuite
+    extends SparkFunSuite
+    with TestHiveSingleton
+    with SQLTestUtils
+    with ParquetHDFSTest {
 
   protected override def beforeAll(): Unit = {
     super.beforeAll()
@@ -51,11 +53,7 @@ class HiveQueryAnalysisSuite extends SparkFunSuite with TestHiveSingleton with S
 
   test("QueryAnalysis.getInputMetadata returns back InputMetadata for simple queries") {
     val df = hiveContext.sql("select code, description, salary from test_table_1")
-    val inputMetadata = QueryAnalysis.getInputMetadata(df.queryExecution)
-    assert(inputMetadata.length === 3)
-    assertHiveFieldExists(inputMetadata, "test_table_1", "code")
-    assertHiveFieldExists(inputMetadata, "test_table_1", "description")
-    assertHiveFieldExists(inputMetadata, "test_table_1", "salary")
+    assertHiveInputs(df.queryExecution, "test_table_1", Seq("code", "description", "salary"))
   }
 
   test("QueryAnalysis.getInputMetadata return back InputMetadata for complex joins") {
@@ -70,21 +68,13 @@ class HiveQueryAnalysisSuite extends SparkFunSuite with TestHiveSingleton with S
     assert(inputMetadata.length === 2)
     assertHiveFieldExists(inputMetadata, "test_table_1", "salary")
     assertHiveFieldExists(inputMetadata, "test_table_2", "code")
-    val outputMetadata = QueryAnalysis.getOutputMetaData(qe)
-    assert(outputMetadata.isDefined)
-    assert(outputMetadata.get.fields.forall(Seq("code", "sal").contains(_)))
-    assert(outputMetadata.get.dataSourceType === DataSourceType.HIVE)
-    assert(outputMetadata.get.source === "default.mytable")
+    assertHiveOutputs(qe, "mytable", Seq("code", "sal"))
   }
 
   test("QueryAnalysis.getInputMetadata returns back InputMetadata for * queries") {
     val df = hiveContext.sql("select * from test_table_1")
-    val inputMetadata = QueryAnalysis.getInputMetadata(df.queryExecution)
-    assert(inputMetadata.length === 4)
-    assertHiveFieldExists(inputMetadata, "test_table_1", "code")
-    assertHiveFieldExists(inputMetadata, "test_table_1", "description")
-    assertHiveFieldExists(inputMetadata, "test_table_1", "salary")
-    assertHiveFieldExists(inputMetadata, "test_table_1", "total_emp")
+    assertHiveInputs(df.queryExecution, "test_table_1", Seq("code", "description", "salary",
+      "total_emp"))
   }
 
   test("There is fully qualified table name in OutputMetadata") {
@@ -93,18 +83,8 @@ class HiveQueryAnalysisSuite extends SparkFunSuite with TestHiveSingleton with S
       activateDatabase(db) {
         df.write.saveAsTable("mytable")
         val qe = TestQeListener.getAndClear()
-        val inputMetadata = QueryAnalysis.getInputMetadata(df.queryExecution)
-        assert(inputMetadata.length === 4)
-        assertHiveFieldExists(inputMetadata, "test_table_1", "code")
-        assertHiveFieldExists(inputMetadata, "test_table_1", "description")
-        assertHiveFieldExists(inputMetadata, "test_table_1", "salary")
-        assertHiveFieldExists(inputMetadata, "test_table_1", "total_emp")
-        val outputMetadata = QueryAnalysis.getOutputMetaData(qe)
-        assert(outputMetadata.isDefined)
-        assert(outputMetadata.get.fields.forall(Seq("code", "description", "salary",
-              "total_emp").contains(_)))
-        assert(outputMetadata.get.dataSourceType === DataSourceType.HIVE)
-        assert(outputMetadata.get.source === db + ".mytable")
+        assertHiveInputs(qe, "test_table_1", Seq("total_emp", "salary", "description", "code"))
+        assertHiveOutputs(qe, "mytable", Seq("total_emp", "salary", "description", "code"), db)
       }
     }
   }
@@ -128,6 +108,87 @@ class HiveQueryAnalysisSuite extends SparkFunSuite with TestHiveSingleton with S
     }
   }
 
+  test("CDH-50366: Lineage should output data when there is no inputs") {
+    import hiveContext.implicits._
+    val nonInputTable: String = "MyNonInputTable"
+
+    val nonInputDF = (1 to 4).map { i =>
+      (i % 2 == 0, i, i.toLong, i.toFloat, i.toDouble)
+    }.toDF
+    nonInputDF.write.saveAsTable(nonInputTable)
+    var qe = TestQeListener.getAndClear()
+    assert(QueryAnalysis.getInputMetadata(qe).length === 0)
+    assertHiveOutputs(qe, nonInputTable, Seq( "_1", "_2", "_3", "_4", "_5"))
+    withTempPath { f =>
+      nonInputDF.write.json(f.getCanonicalPath)
+      val qe = TestQeListener.getAndClear()
+      assert(QueryAnalysis.getInputMetadata(qe).length === 0)
+      val outputMetadata = QueryAnalysis.getOutputMetaData(qe)
+      assert(outputMetadata.isDefined)
+      assert(outputMetadata.get.fields.size === 5)
+      assert(outputMetadata.get.source === "file:" + f.getCanonicalPath)
+      assert(outputMetadata.get.dataSourceType === DataSourceType.LOCAL)
+    }
+
+    var anotherDF = hiveContext.sql("select code, description, salary from test_table_1")
+    anotherDF = anotherDF.join(nonInputDF, nonInputDF.col("_3") === anotherDF.col("salary"))
+    anotherDF.write.saveAsTable(nonInputTable + 2)
+    qe = TestQeListener.getAndClear()
+    assertHiveInputs(qe, "test_table_1", Seq("salary", "code", "description"))
+    assertHiveOutputs(qe, nonInputTable + 2, Seq("code", "description", "salary", "_1", "_2", "_3",
+      "_4", "_5"))
+
+    anotherDF.select("_2", "_5", "salary", "code").write.saveAsTable(nonInputTable + 3)
+    qe = TestQeListener.getAndClear()
+    assertHiveInputs(qe, "test_table_1", Seq("salary", "code"))
+    assertHiveOutputs(qe, nonInputTable + 3, Seq("code", "salary", "_2", "_5"))
+
+    var personDF = (1 to 4).map(i => Person(i.toString, i)).toDF
+    personDF.write.saveAsTable(nonInputTable + 4)
+    qe = TestQeListener.getAndClear()
+    val inputMetadata = QueryAnalysis.getInputMetadata(qe)
+    assert(inputMetadata.length === 0)
+    assertHiveOutputs(qe, nonInputTable + 4, Seq("name", "age"))
+
+    personDF = personDF.join(nonInputDF, nonInputDF.col("_3") === personDF.col("age"))
+    personDF.write.saveAsTable(nonInputTable + 5)
+    qe = TestQeListener.getAndClear()
+    assert(QueryAnalysis.getInputMetadata(qe).length === 0)
+    assertHiveOutputs(qe, nonInputTable + 5, Seq("name", "age" , "_1", "_2", "_3",
+      "_4", "_5"))
+
+    val testTableDF = hiveContext.sql("select code, description, salary from test_table_1")
+    personDF = (1 to 4).map(i => Person(i.toString, i)).toDF
+    personDF = personDF.join(testTableDF, testTableDF.col("salary") === personDF.col("age"))
+    personDF.select("code", "description", "name", "age").write.saveAsTable(nonInputTable + 6)
+    qe = TestQeListener.getAndClear()
+    assertHiveInputs(qe, "test_table_1", Seq("description", "code"))
+    assertHiveOutputs(qe, nonInputTable + 6, Seq("code", "description", "name", "age"))
+  }
+
+  def assertHiveInputs(
+      qe: org.apache.spark.sql.execution.QueryExecution,
+      table: String,
+      columns: Seq[String],
+      db: String = "default") {
+    val inputMetadata = QueryAnalysis.getInputMetadata(qe)
+    assert(inputMetadata.length === columns.size)
+    columns.foreach(assertHiveFieldExists(inputMetadata, table, _, db))
+  }
+
+  def assertHiveOutputs(
+      qe: org.apache.spark.sql.execution.QueryExecution,
+      table: String,
+      columns: Seq[String],
+      db: String = "default") {
+    val outputMetadata = QueryAnalysis.getOutputMetaData(qe)
+    assert(outputMetadata.isDefined)
+    assert(outputMetadata.get.fields.size === columns.size)
+    assert(outputMetadata.get.source === db + "." + table)
+    assert(outputMetadata.get.dataSourceType === DataSourceType.HIVE)
+    assert(outputMetadata.get.fields.forall(columns.contains(_)))
+  }
+
   implicit class SqlCmd(sql: String) {
     def cmd: () => Unit = { () =>
       new QueryExecution(sql).stringResult(): Unit
@@ -139,3 +200,5 @@ class HiveQueryAnalysisSuite extends SparkFunSuite with TestHiveSingleton with S
     hiveContext.listenerManager.clear()
   }
 }
+
+case class Person(name: String, age: Long)
diff --git a/lineage/src/test/scala/org/apache/spark/sql/query/analysis/TestUtils.scala b/lineage/src/test/scala/org/apache/spark/sql/query/analysis/TestUtils.scala
index 37aa937..6672c77 100644
--- a/lineage/src/test/scala/org/apache/spark/sql/query/analysis/TestUtils.scala
+++ b/lineage/src/test/scala/org/apache/spark/sql/query/analysis/TestUtils.scala
@@ -41,8 +41,9 @@ object TestUtils {
   def assertHiveFieldExists(
       inputMetadata: List[FieldDetails],
       table: String,
-      column: String): Unit = {
-    assert(inputMetadata.contains(FieldDetails(Array("default." + table), column,
+      column: String,
+      db: String = "default"): Unit = {
+    assert(inputMetadata.contains(FieldDetails(Array(db + "." + table), column,
       DataSourceType.HIVE, DataSourceFormat.UNKNOWN)))
   }
 
-- 
1.7.9.5

