From 636449f4da07f5048c7a24c293d2ed04604ac8a1 Mon Sep 17 00:00:00 2001
From: Imran Rashid <irashid@cloudera.com>
Date: Mon, 22 May 2017 14:31:08 -0500
Subject: [PATCH 511/517] Revert "[SPARK-12297][SQL] Hive compatibility for
 Parquet Timestamps"

This reverts commit e7e4f8ca5603316729da81c29ce203c4f088732d.

This is being reverted because the feature will not be included in
CDH5.12 for any component, because of continued upstream discussion
around the design.
---
 project/SparkBuild.scala                           |   20 +-
 .../datasources/parquet/CatalystReadSupport.scala  |    3 +-
 .../parquet/CatalystRecordMaterializer.scala       |    8 +-
 .../datasources/parquet/CatalystRowConverter.scala |   58 +---
 .../datasources/parquet/CatalystWriteSupport.scala |   20 +-
 .../datasources/parquet/ParquetFileFormat.scala    |   21 --
 .../datasources/parquet/ParquetRelation.scala      |   11 +-
 .../parquet/ParquetTimestampSuite.scala            |   97 -------
 .../spark/sql/hive/HiveMetastoreCatalog.scala      |    7 +-
 .../sql/hive/ParquetHiveCompatibilitySuite.scala   |  302 +-------------------
 10 files changed, 25 insertions(+), 522 deletions(-)
 delete mode 100644 sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat.scala
 delete mode 100644 sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetTimestampSuite.scala

diff --git a/project/SparkBuild.scala b/project/SparkBuild.scala
index bb3e5527..0e32315 100644
--- a/project/SparkBuild.scala
+++ b/project/SparkBuild.scala
@@ -129,7 +129,6 @@ object SparkBuild extends PomBuild {
   override val userPropertiesMap = System.getProperties.asScala.toMap
 
   lazy val MavenCompile = config("m2r") extend(Compile)
-  lazy val TestDebug = config("testDebug") extend(Test)
   lazy val publishLocalBoth = TaskKey[Unit]("publish-local", "publish local for m2 and ivy")
 
   lazy val sparkGenjavadocSettings: Seq[sbt.Def.Setting[_]] = Seq(
@@ -298,23 +297,8 @@ object SparkBuild extends PomBuild {
   // TODO: move this to its upstream project.
   override def projectDefinitions(baseDirectory: File): Seq[Project] = {
     super.projectDefinitions(baseDirectory).map { x =>
-      val baseProject =
-        if (projectsMap.exists(_._1 == x.id)) {
-          x.settings(projectsMap(x.id): _*)
-        } else {
-          x.settings(Seq[Setting[_]](): _*)
-        }
-      // enable running tests with a debugger like "testDebug:testOnly ..."
-      // (somewhat explained http://www.scala-sbt.org/0.13/docs/Testing.html)
-      // The test will pause until you attach a debugger to it (b/c of suspend=y).
-      // This is a standard remote debugging configuration, eg. in IntelliJ
-      // setup a debug configuration for "Remote".
-      baseProject
-       .configs(TestDebug)
-       .settings(inConfig(TestDebug)(Defaults.testTasks): _*)
-       .settings(Seq(
-         javaOptions in TestDebug ++= "-agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=5005"
-           .split(" ").toSeq))
+      if (projectsMap.exists(_._1 == x.id)) x.settings(projectsMap(x.id): _*)
+      else x.settings(Seq[Setting[_]](): _*)
     } ++ Seq[Project](OldDeps.project)
   }
 
diff --git a/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/CatalystReadSupport.scala b/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/CatalystReadSupport.scala
index 05386cb..82f68ee 100644
--- a/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/CatalystReadSupport.scala
+++ b/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/CatalystReadSupport.scala
@@ -97,8 +97,7 @@ private[parquet] class CatalystReadSupport extends ReadSupport[InternalRow] with
 
     new CatalystRecordMaterializer(
       parquetRequestedSchema,
-      CatalystReadSupport.expandUDT(catalystRequestedSchema),
-      conf)
+      CatalystReadSupport.expandUDT(catalystRequestedSchema))
   }
 }
 
diff --git a/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/CatalystRecordMaterializer.scala b/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/CatalystRecordMaterializer.scala
index 96789a8..407c48c 100644
--- a/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/CatalystRecordMaterializer.scala
+++ b/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/CatalystRecordMaterializer.scala
@@ -17,7 +17,6 @@
 
 package org.apache.spark.sql.execution.datasources.parquet
 
-import org.apache.hadoop.conf.Configuration
 import parquet.io.api.{GroupConverter, RecordMaterializer}
 import parquet.schema.MessageType
 
@@ -31,13 +30,10 @@ import org.apache.spark.sql.types.StructType
  * @param catalystSchema Catalyst schema of the rows to be constructed
  */
 private[parquet] class CatalystRecordMaterializer(
-    parquetSchema: MessageType,
-    catalystSchema: StructType,
-    conf: Configuration)
+    parquetSchema: MessageType, catalystSchema: StructType)
   extends RecordMaterializer[InternalRow] {
 
-  private val rootConverter = new CatalystRowConverter(
-    parquetSchema, catalystSchema, NoopUpdater, conf)
+  private val rootConverter = new CatalystRowConverter(parquetSchema, catalystSchema, NoopUpdater)
 
   override def getCurrentRecord: InternalRow = rootConverter.currentRecord
 
diff --git a/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/CatalystRowConverter.scala b/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/CatalystRowConverter.scala
index 9047dd3..1d18c51 100644
--- a/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/CatalystRowConverter.scala
+++ b/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/CatalystRowConverter.scala
@@ -19,12 +19,10 @@ package org.apache.spark.sql.execution.datasources.parquet
 
 import java.math.{BigDecimal, BigInteger}
 import java.nio.ByteOrder
-import java.util.TimeZone
 
 import scala.collection.JavaConverters._
 import scala.collection.mutable.ArrayBuffer
 
-import org.apache.hadoop.conf.Configuration
 import parquet.column.Dictionary
 import parquet.io.api.{Binary, Converter, GroupConverter, PrimitiveConverter}
 import parquet.schema.OriginalType.{INT_32, LIST, UTF8}
@@ -34,8 +32,7 @@ import parquet.schema.{GroupType, MessageType, PrimitiveType, Type}
 import org.apache.spark.Logging
 import org.apache.spark.sql.catalyst.InternalRow
 import org.apache.spark.sql.catalyst.expressions._
-import org.apache.spark.sql.catalyst.util.{DateTimeUtils, GenericArrayData, ArrayBasedMapData}
-import org.apache.spark.sql.catalyst.util.DateTimeUtils._
+import org.apache.spark.sql.catalyst.util.{GenericArrayData, ArrayBasedMapData, DateTimeUtils}
 import org.apache.spark.sql.types._
 import org.apache.spark.unsafe.types.UTF8String
 
@@ -123,8 +120,7 @@ private[parquet] class CatalystPrimitiveConverter(val updater: ParentContainerUp
 private[parquet] class CatalystRowConverter(
     parquetType: GroupType,
     catalystType: StructType,
-    updater: ParentContainerUpdater,
-    hadoopConf: Configuration)
+    updater: ParentContainerUpdater)
   extends CatalystGroupConverter(updater) with Logging {
 
   assert(
@@ -255,21 +251,18 @@ private[parquet] class CatalystRowConverter(
 
       case TimestampType =>
         // TODO Implements `TIMESTAMP_MICROS` once parquet-mr has that.
-        val localTz = TimeZone.getDefault()
-        val tzString = hadoopConf.get(ParquetFileFormat.PARQUET_TIMEZONE_TABLE_PROPERTY)
-        val storageTz = if (tzString == null) {
-          localTz
-        } else {
-          TimeZone.getTimeZone(tzString)
-        }
-        logInfo(s"Building timestamp reader with localTz = ${localTz.getID()}; " +
-          s"storageTz = ${storageTz.getID()}; tzString = $tzString")
         new CatalystPrimitiveConverter(updater) {
           // Converts nanosecond timestamps stored as INT96
           override def addBinary(value: Binary): Unit = {
-            val timestamp = CatalystRowConverter.binaryToSQLTimestamp(
-              value, fromTz = localTz, toTz = storageTz)
-            updater.setLong(timestamp)
+            assert(
+              value.length() == 12,
+              "Timestamps (with nanoseconds) are expected to be stored in 12-byte long binaries, " +
+              s"but got a ${value.length()}-byte binary.")
+
+            val buf = value.toByteBuffer.order(ByteOrder.LITTLE_ENDIAN)
+            val timeOfDayNanos = buf.getLong
+            val julianDay = buf.getInt
+            updater.setLong(DateTimeUtils.fromJulianDay(julianDay, timeOfDayNanos))
           }
         }
 
@@ -300,7 +293,7 @@ private[parquet] class CatalystRowConverter(
       case t: StructType =>
         new CatalystRowConverter(parquetType.asGroupType(), t, new ParentContainerUpdater {
           override def set(value: Any): Unit = updater.set(value.asInstanceOf[InternalRow].copy())
-        }, hadoopConf)
+        })
 
       case t =>
         throw new RuntimeException(
@@ -644,7 +637,7 @@ private[parquet] class CatalystRowConverter(
   }
 }
 
-private[parquet] object CatalystRowConverter extends Logging {
+private[parquet] object CatalystRowConverter {
   def binaryToUnscaledLong(binary: Binary): Long = {
     // The underlying `ByteBuffer` implementation is guaranteed to be `HeapByteBuffer`, so here
     // we are using `Binary.toByteBuffer.array()` to steal the underlying byte array without
@@ -666,29 +659,4 @@ private[parquet] object CatalystRowConverter extends Logging {
     unscaled = (unscaled << (64 - bits)) >> (64 - bits)
     unscaled
   }
-
-  /**
-   * Converts an int96 to a SQLTimestamp, given both the storage timezone and the local timezone.
-   * The timestamp is really meant to be interpreted as a "floating time", but since we
-   * actually store it as micros since epoch, why we have to apply a conversion when timezones
-   * change.
-   * @param binary
-   * @return
-   */
-  def binaryToSQLTimestamp(binary: Binary, fromTz: TimeZone, toTz: TimeZone): SQLTimestamp = {
-    // Note that this method is copied from ParquetRowConverter present in Spark 2.2, and has been
-    // placed here as part of the backport of SPARK-12297 / CDH-35305
-    assert(binary.length() == 12, s"Timestamps (with nanoseconds) are expected to be stored in" +
-      s" 12-byte long binaries. Found a ${binary.length()}-byte binary instead.")
-    val buffer = binary.toByteBuffer.order(ByteOrder.LITTLE_ENDIAN)
-    val timeOfDayNanos = buffer.getLong
-    val julianDay = buffer.getInt
-    val storageTzMicros = DateTimeUtils.fromJulianDay(julianDay, timeOfDayNanos)
-    // avoid expensive time logic if possible.
-    if (fromTz.getID() != toTz.getID()) {
-      DateTimeUtils.convertTz(storageTzMicros, fromTz, toTz)
-    } else {
-      storageTzMicros
-    }
-  }
 }
diff --git a/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/CatalystWriteSupport.scala b/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/CatalystWriteSupport.scala
index 1a2fb9d..0ae401a 100644
--- a/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/CatalystWriteSupport.scala
+++ b/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/CatalystWriteSupport.scala
@@ -19,7 +19,6 @@ package org.apache.spark.sql.execution.datasources.parquet
 
 import java.nio.{ByteBuffer, ByteOrder}
 import java.util
-import java.util.TimeZone
 
 import scala.collection.JavaConverters.mapAsJavaMapConverter
 
@@ -67,9 +66,6 @@ private[parquet] class CatalystWriteSupport extends WriteSupport[InternalRow] wi
   // Whether to write data in legacy Parquet format compatible with Spark 1.4 and prior versions
   private var writeLegacyParquetFormat: Boolean = _
 
-  private var storageTz: TimeZone = _
-  private var localTz: TimeZone = _
-
   // Reusable byte array used to write timestamps as Parquet INT96 values
   private val timestampBuffer = new Array[Byte](12)
 
@@ -85,14 +81,6 @@ private[parquet] class CatalystWriteSupport extends WriteSupport[InternalRow] wi
       configuration.get(SQLConf.PARQUET_WRITE_LEGACY_FORMAT.key).toBoolean
     }
     this.rootFieldWriters = schema.map(_.dataType).map(makeWriter)
-    localTz = TimeZone.getDefault()
-    // If the table has a timezone property, apply the correct conversions.  See SPARK-12297.
-    val tzString = configuration.get(ParquetFileFormat.PARQUET_TIMEZONE_TABLE_PROPERTY)
-    storageTz = if (tzString == null) {
-      localTz
-    } else {
-      TimeZone.getTimeZone(tzString)
-    }
 
     val messageType = new CatalystSchemaConverter(configuration).convert(schema)
     val metadata = Map(CatalystReadSupport.SPARK_METADATA_KEY -> schemaString).asJava
@@ -174,13 +162,7 @@ private[parquet] class CatalystWriteSupport extends WriteSupport[InternalRow] wi
 
           // NOTE: Starting from Spark 1.5, Spark SQL `TimestampType` only has microsecond
           // precision.  Nanosecond parts of timestamp values read from INT96 are simply stripped.
-          val rawMicros = row.getLong(ordinal)
-          val adjustedMicros = if (localTz.getID() == storageTz.getID()) {
-            rawMicros
-          } else {
-            DateTimeUtils.convertTz(rawMicros, storageTz, localTz)
-          }
-          val (julianDay, timeOfDayNanos) = DateTimeUtils.toJulianDay(adjustedMicros)
+          val (julianDay, timeOfDayNanos) = DateTimeUtils.toJulianDay(row.getLong(ordinal))
           val buf = ByteBuffer.wrap(timestampBuffer)
           buf.order(ByteOrder.LITTLE_ENDIAN).putLong(timeOfDayNanos).putInt(julianDay)
           recordConsumer.addBinary(Binary.fromByteArray(timestampBuffer))
diff --git a/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat.scala b/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat.scala
deleted file mode 100644
index 23dba59..0000000
--- a/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat.scala
+++ /dev/null
@@ -1,21 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.spark.sql.execution.datasources.parquet
-
-object ParquetFileFormat {
-  val PARQUET_TIMEZONE_TABLE_PROPERTY = "parquet.mr.int96.write.zone"
-}
diff --git a/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRelation.scala b/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRelation.scala
index 855710d..36778ab 100644
--- a/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRelation.scala
+++ b/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRelation.scala
@@ -279,9 +279,6 @@ private[sql] class ParquetRelation(
         .getOrElse(
           sqlContext.conf.parquetCompressionCodec.toUpperCase,
           CompressionCodecName.UNCOMPRESSED).name())
-    parameters.get(ParquetFileFormat.PARQUET_TIMEZONE_TABLE_PROPERTY).foreach (
-      conf.set(ParquetFileFormat.PARQUET_TIMEZONE_TABLE_PROPERTY, _)
-    )
 
     new OutputWriterFactory {
       override def newInstance(
@@ -311,7 +308,6 @@ private[sql] class ParquetRelation(
     val parquetBlockSize = ParquetOutputFormat.getLongBlockSize(broadcastedConf.value.value)
 
     // Create the function to set variable Parquet confs at both driver and executor side.
-    val tz = parameters.get(ParquetFileFormat.PARQUET_TIMEZONE_TABLE_PROPERTY)
     val initLocalJobFuncOpt =
       ParquetRelation.initializeLocalJobFunc(
         requiredColumns,
@@ -321,8 +317,7 @@ private[sql] class ParquetRelation(
         useMetadataCache,
         safeParquetFilterPushDown,
         assumeBinaryIsString,
-        assumeInt96IsTimestamp,
-        tz) _
+        assumeInt96IsTimestamp) _
 
     // Create the function to set input paths at the driver side.
     val setInputPaths =
@@ -568,8 +563,7 @@ private[sql] object ParquetRelation extends Logging {
       useMetadataCache: Boolean,
       parquetFilterPushDown: Boolean,
       assumeBinaryIsString: Boolean,
-      assumeInt96IsTimestamp: Boolean,
-      timestampTimezone: Option[String])(job: Job): Unit = {
+      assumeInt96IsTimestamp: Boolean)(job: Job): Unit = {
     val conf = SparkHadoopUtil.get.getConfigurationFromJobContext(job)
     conf.set(ParquetInputFormat.READ_SUPPORT_CLASS, classOf[CatalystReadSupport].getName)
 
@@ -599,7 +593,6 @@ private[sql] object ParquetRelation extends Logging {
     // Sets flags for `CatalystSchemaConverter`
     conf.setBoolean(SQLConf.PARQUET_BINARY_AS_STRING.key, assumeBinaryIsString)
     conf.setBoolean(SQLConf.PARQUET_INT96_AS_TIMESTAMP.key, assumeInt96IsTimestamp)
-    timestampTimezone.foreach(conf.set(ParquetFileFormat.PARQUET_TIMEZONE_TABLE_PROPERTY, _))
 
     overrideMinSplitSize(parquetBlockSize, conf)
   }
diff --git a/sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetTimestampSuite.scala b/sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetTimestampSuite.scala
deleted file mode 100644
index c2ef481..0000000
--- a/sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetTimestampSuite.scala
+++ /dev/null
@@ -1,97 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.spark.sql.execution.datasources.parquet
-
-import java.nio.{ByteOrder, ByteBuffer}
-import java.util.TimeZone
-import java.util.concurrent.TimeUnit
-
-import org.apache.hadoop.conf.Configuration
-import org.apache.spark.sql.SQLConf
-import org.apache.spark.sql.catalyst.util.DateTimeUtils
-import org.scalatest.mock.MockitoSugar
-import org.mockito.ArgumentCaptor
-import org.mockito.Mockito.verify
-import parquet.io.api.{Binary, RecordConsumer}
-
-import org.apache.spark.SparkFunSuite
-import org.apache.spark.sql.catalyst.CatalystTypeConverters
-import org.apache.spark.sql.catalyst.expressions.GenericInternalRow
-import org.apache.spark.sql.test.SharedSQLContext
-import org.apache.spark.sql.types.{TimestampType, StructField, StructType}
-
-class ParquetTimestampSuite extends SparkFunSuite with MockitoSugar with SharedSQLContext {
-
-  test("roundtrip via timezone combos") {
-    /*
-     *  Worked Example
-     *  2015-12-31 23:50:59.123 in floating (aka wall-clock aka timestamp without timezone) time
-     *  in a few different time zones
-     */
-    val MILLIS_PER_HOUR = TimeUnit.HOURS.toMillis(1)
-    val TIME_IN_LA = 1451634659123L
-    val TIME_IN_CHICAGO = TIME_IN_LA + (-2 * MILLIS_PER_HOUR)
-    val TIME_IN_BERLIN = TIME_IN_LA + (-9 * MILLIS_PER_HOUR)
-    val initialTz = TimeZone.getDefault()
-    try {
-      TimeZone.setDefault(TimeZone.getTimeZone("America/Los_Angeles"))
-      val conf = new Configuration()
-      val catalystSchema = StructType(Array(StructField("ts", TimestampType, true)))
-      CatalystWriteSupport.setSchema(catalystSchema, conf)
-      conf.set(ParquetFileFormat.PARQUET_TIMEZONE_TABLE_PROPERTY, "America/Chicago")
-      conf.set(SQLConf.PARQUET_WRITE_LEGACY_FORMAT.key, "true")
-      conf.set(SQLConf.PARQUET_BINARY_AS_STRING.key, "false")
-      conf.set(SQLConf.PARQUET_INT96_AS_TIMESTAMP.key, "true")
-      val writeSupport = new CatalystWriteSupport()
-      val writer = writeSupport.init(conf)
-
-      val recordConsumer = mock[RecordConsumer]
-      writeSupport.prepareForWrite(recordConsumer)
-      val rowData = new Array[Any](1)
-      val row = new GenericInternalRow(rowData)
-      val timestampInLa = new java.sql.Timestamp(TIME_IN_LA)
-      rowData(0) = CatalystTypeConverters.convertToCatalyst(timestampInLa)
-      writeSupport.write(row)
-      // Since the test is run in LA tz, we should should convert to the equivalent time in
-      // chicago timezone.
-      val binaryCaptor = ArgumentCaptor.forClass(classOf[Binary])
-      verify(recordConsumer).addBinary(binaryCaptor.capture())
-      assert(binaryCaptor.getValue.compareTo(millisToBinary(TIME_IN_CHICAGO)) === 0)
-
-      // If we read that time out again in Berlin timezone, we should convert it to the
-      // equivalent time in Berlin.
-      TimeZone.setDefault(TimeZone.getTimeZone("Europe/Berlin"))
-      val parquetSchema = new CatalystSchemaConverter(conf).convert(catalystSchema)
-      val updater = mock[ParentContainerUpdater]
-      val reader = new CatalystRowConverter(parquetSchema, catalystSchema, updater, conf)
-      reader.getConverter(0).asPrimitiveConverter().addBinary(millisToBinary(TIME_IN_CHICAGO))
-      // We read back time in microseconds, so we need to take millis * 1000
-      assert(reader.currentRecord.getLong(0) === (TIME_IN_BERLIN * 1000))
-    } finally {
-      TimeZone.setDefault(initialTz)
-    }
-  }
-
-  private def millisToBinary(millis: Long): Binary = {
-    val micros = millis * 1000
-    val timestampBuffer = new Array[Byte](12)
-    val (julianDay, timeOfDayNanos) = DateTimeUtils.toJulianDay(micros)
-    val buf = ByteBuffer.wrap(timestampBuffer)
-    buf.order(ByteOrder.LITTLE_ENDIAN).putLong(timeOfDayNanos).putInt(julianDay)
-    Binary.fromByteArray(timestampBuffer)
-  }
-}
diff --git a/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveMetastoreCatalog.scala b/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveMetastoreCatalog.scala
index 1830111..3da1892 100644
--- a/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveMetastoreCatalog.scala
+++ b/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveMetastoreCatalog.scala
@@ -38,7 +38,7 @@ import org.apache.spark.sql.catalyst.plans.logical
 import org.apache.spark.sql.catalyst.plans.logical._
 import org.apache.spark.sql.catalyst.rules._
 import org.apache.spark.sql.catalyst.util.DataTypeParser
-import org.apache.spark.sql.execution.datasources.parquet.{ParquetFileFormat, ParquetRelation}
+import org.apache.spark.sql.execution.datasources.parquet.ParquetRelation
 import org.apache.spark.sql.execution.datasources.{CreateTableUsingAsSelect, LogicalRelation, Partition => ParquetPartition, PartitionSpec, ResolvedDataSource}
 import org.apache.spark.sql.execution.{FileRelation, datasources}
 import org.apache.spark.sql.hive.client._
@@ -439,9 +439,6 @@ private[hive] class HiveMetastoreCatalog(val client: ClientInterface, hive: Hive
     // NOTE: Instead of passing Metastore schema directly to `ParquetRelation`, we have to
     // serialize the Metastore schema to JSON and pass it as a data source option because of the
     // evil case insensitivity issue, which is reconciled within `ParquetRelation`.
-    val tzKey = ParquetFileFormat.PARQUET_TIMEZONE_TABLE_PROPERTY
-    val tzProps = metastoreRelation.table.properties.get(tzKey)
-      .map { tz => Map(tzKey -> tz) }.getOrElse(Map())
     val parquetOptions = Map(
       ParquetRelation.METASTORE_SCHEMA -> metastoreSchema.json,
       ParquetRelation.MERGE_SCHEMA -> mergeSchema.toString,
@@ -449,7 +446,7 @@ private[hive] class HiveMetastoreCatalog(val client: ClientInterface, hive: Hive
         metastoreRelation.tableName,
         Some(metastoreRelation.databaseName)
       ).unquotedString
-    ) ++ tzProps
+    )
     val tableIdentifier =
       QualifiedTableName(metastoreRelation.databaseName, metastoreRelation.tableName)
 
diff --git a/sql/hive/src/test/scala/org/apache/spark/sql/hive/ParquetHiveCompatibilitySuite.scala b/sql/hive/src/test/scala/org/apache/spark/sql/hive/ParquetHiveCompatibilitySuite.scala
index 14e7f68..49aab85 100644
--- a/sql/hive/src/test/scala/org/apache/spark/sql/hive/ParquetHiveCompatibilitySuite.scala
+++ b/sql/hive/src/test/scala/org/apache/spark/sql/hive/ParquetHiveCompatibilitySuite.scala
@@ -18,19 +18,12 @@
 package org.apache.spark.sql.hive
 
 import java.sql.Timestamp
-import java.text.SimpleDateFormat
-import java.util.{Calendar, TimeZone}
 
 import org.apache.hadoop.hive.conf.HiveConf
 
-import org.apache.spark.SparkFunSuite
-import org.apache.spark.sql._
-import org.apache.spark.sql.catalyst.TableIdentifier
-import org.apache.spark.sql.catalyst.util.DateTimeUtils
-import org.apache.spark.sql.execution.datasources.parquet.{ParquetCompatibilityTest, ParquetFileFormat}
-import org.apache.spark.sql.hive.client.HiveTable
+import org.apache.spark.sql.execution.datasources.parquet.ParquetCompatibilityTest
+import org.apache.spark.sql.{Row, SQLConf}
 import org.apache.spark.sql.hive.test.TestHiveSingleton
-import org.apache.spark.sql.types.{StringType, StructField, StructType, TimestampType}
 
 class ParquetHiveCompatibilitySuite extends ParquetCompatibilityTest with TestHiveSingleton {
   /**
@@ -39,14 +32,6 @@ class ParquetHiveCompatibilitySuite extends ParquetCompatibilityTest with TestHi
    */
   private val stagingDir = new HiveConf().getVar(HiveConf.ConfVars.STAGINGDIR)
 
-  // This is needed just for backporting to the spark 1.6 line, to access the table properties.
-  // spark 2.x exposes the table metadata directly via catalog.
-  private lazy val hiveCatalog = sqlContext.catalog.asInstanceOf[HiveMetastoreCatalog]
-  private def hiveTable(tableId: TableIdentifier): HiveTable = {
-    val database = tableId.database.getOrElse(hiveCatalog.client.currentDatabase).toLowerCase
-    hiveCatalog.client.getTable(database, tableId.table)
-  }
-
   override protected def logParquetSchema(path: String): Unit = {
     val schema = readParquetSchema(path, { path =>
       !path.getName.startsWith("_") && !path.getName.startsWith(stagingDir)
@@ -151,287 +136,4 @@ class ParquetHiveCompatibilitySuite extends ParquetCompatibilityTest with TestHi
       Row(Row(1, Seq("foo", "bar", null))),
       "STRUCT<f0: INT, f1: ARRAY<STRING>>")
   }
-
-  // Check creating parquet tables, writing data into them, and reading it back out under a
-  // variety of conditions:
-  // * tables with explicit tz and those without
-  // * altering table properties directly
-  // * variety of timezones, local & non-local
-  testCreateWriteRead("no_tz", None)
-  val localTz = TimeZone.getDefault.getID()
-  testCreateWriteRead("local", Some(localTz))
-  // check with a variety of timezones.  The unit tests currently are configured to always use
-  // America/Los_Angeles, but even if they didn't, we'd be sure to cover a non-local timezone.
-  Seq(
-    "UTC" -> "UTC",
-    "LA" -> "America/Los_Angeles",
-    "Berlin" -> "Europe/Berlin"
-  ).foreach { case (tableName, zone) =>
-    if (zone != localTz) {
-      testCreateWriteRead(tableName, Some(zone))
-    }
-  }
-
-  private def testCreateWriteRead(
-      baseTable: String,
-      explicitTz: Option[String]): Unit = {
-    testCreateAlterTablesWithTimezone(baseTable, explicitTz)
-    testWriteTablesWithTimezone(baseTable, explicitTz)
-    testReadTablesWithTimezone(baseTable, explicitTz)
-  }
-
-  private def checkHasTz(table: String, tz: Option[String]): Unit = {
-    val tableMetadata = hiveTable(TableIdentifier(table))
-    val actualTz = tableMetadata.properties.get(ParquetFileFormat.PARQUET_TIMEZONE_TABLE_PROPERTY)
-    assert( actualTz === tz, s"table $table timezone was supposed to be $tz, was $actualTz")
-  }
-
-  private def testCreateAlterTablesWithTimezone(
-      baseTable: String,
-      explicitTz: Option[String]): Unit = {
-    test(s"SPARK-12297: Create and Alter Parquet tables and timezones; explicitTz = $explicitTz") {
-      // we're cheating a bit here, in general SparkConf isn't meant to be set at runtime,
-      // but its OK in this case, and lets us run this test, because these tests don't like
-      // creating multiple HiveContexts in the same jvm
-      val key = ParquetFileFormat.PARQUET_TIMEZONE_TABLE_PROPERTY
-      withTable(baseTable, s"like_$baseTable", s"select_$baseTable") {
-        val localTz = TimeZone.getDefault()
-        val localTzId = localTz.getID()
-        val defaultTz = None
-        // check that created tables have correct TBLPROPERTIES
-        val tblProperties = explicitTz.map {
-          tz => raw"""TBLPROPERTIES ("$key"="$tz")"""
-        }.getOrElse("")
-        sqlContext.sql(
-          raw"""CREATE TABLE $baseTable (
-                |  x int
-                | )
-                | STORED AS PARQUET
-                | $tblProperties
-            """.stripMargin)
-        val expectedTableTz = explicitTz.orElse(defaultTz)
-        checkHasTz(baseTable, expectedTableTz)
-        sqlContext.sql(s"CREATE TABLE like_$baseTable LIKE $baseTable")
-        checkHasTz(s"like_$baseTable", expectedTableTz)
-        sqlContext.sql(
-          raw"""CREATE TABLE select_$baseTable
-                | STORED AS PARQUET
-                | AS
-                | SELECT * from $baseTable
-            """.stripMargin)
-        checkHasTz(s"select_$baseTable", defaultTz)
-
-        // check alter table, setting, unsetting, resetting the property
-        sqlContext.sql(
-          raw"""ALTER TABLE $baseTable SET TBLPROPERTIES ("$key"="America/Los_Angeles")""")
-        checkHasTz(baseTable, Some("America/Los_Angeles"))
-        sqlContext.sql( raw"""ALTER TABLE $baseTable SET TBLPROPERTIES ("$key"="UTC")""")
-        checkHasTz(baseTable, Some("UTC"))
-        sqlContext.sql( raw"""ALTER TABLE $baseTable UNSET TBLPROPERTIES ("$key")""")
-        checkHasTz(baseTable, None)
-        explicitTz.foreach { tz =>
-          sqlContext.sql( raw"""ALTER TABLE $baseTable SET TBLPROPERTIES ("$key"="$tz")""")
-          checkHasTz(baseTable, expectedTableTz)
-        }
-      }
-    }
-  }
-
-  val desiredTimestampStrings = Seq(
-    "2015-12-31 23:50:59.123",
-    "2015-12-31 22:49:59.123",
-    "2016-01-01 00:39:59.123",
-    "2016-01-01 01:29:59.123"
-  )
-  // We don't want to mess with timezones inside the tests themselves, since we use a shared
-  // spark context, and then we might be prone to issues from lazy vals for timezones.  Instead,
-  // we manually adjust the timezone just to determine what the desired millis (since epoch, in utc)
-  // is for various "wall-clock" times in different timezones, and then we can compare against those
-  // in our tests.
-  val originalTz = TimeZone.getDefault
-  val timestampTimezoneToMillis = try {
-    (for {
-      timestampString <- desiredTimestampStrings
-      timezone <- Seq("America/Los_Angeles", "Europe/Berlin", "UTC").map {
-        TimeZone.getTimeZone(_)
-      }
-    } yield {
-      TimeZone.setDefault(timezone)
-      val timestamp = Timestamp.valueOf(timestampString)
-      (timestampString, timezone.getID()) -> timestamp.getTime()
-    }).toMap
-  } finally {
-    TimeZone.setDefault(originalTz)
-  }
-
-  private def createRawData(): DataFrame = {
-    val originalTsStrings = Seq(
-      "2015-12-31 23:50:59.123",
-      "2015-12-31 22:49:59.123",
-      "2016-01-01 00:39:59.123",
-      "2016-01-01 01:29:59.123"
-    )
-    val rowRdd = sqlContext.sparkContext.parallelize(originalTsStrings, 1).map { x =>
-      Row(x, java.sql.Timestamp.valueOf(x))
-    }
-    val schema = StructType(Array(
-      StructField("display", StringType, true),
-      StructField("ts", TimestampType, true)))
-    sqlContext.createDataFrame(rowRdd, schema)
-  }
-
-  private def testWriteTablesWithTimezone(
-      baseTable: String,
-      explicitTz: Option[String]): Unit = {
-    val key = ParquetFileFormat.PARQUET_TIMEZONE_TABLE_PROPERTY
-    test(s"SPARK-12297: Write to Parquet tables with Timestamps; explicitTz = $explicitTz") {
-      withTable(s"saveAsTable_$baseTable", s"insert_$baseTable") {
-        val localTz = TimeZone.getDefault()
-        val localTzId = localTz.getID()
-        val defaultTz = None
-        val expectedTableTz = explicitTz.orElse(defaultTz)
-        // check that created tables have correct TBLPROPERTIES
-        val tblProperties = explicitTz.map {
-          tz => raw"""TBLPROPERTIES ("$key"="$tz")"""
-        }.getOrElse("")
-
-
-        val rawData = createRawData()
-        // Check writing data out.
-        // We write data into our tables, and then check the raw parquet files to see whether
-        // the correct conversion was applied.
-        rawData.write.saveAsTable(s"saveAsTable_$baseTable")
-        checkHasTz(s"saveAsTable_$baseTable", defaultTz)
-        sqlContext.sql(
-          raw"""CREATE TABLE insert_$baseTable (
-                |  display string,
-                |  ts timestamp
-                | )
-                | STORED AS PARQUET
-                | $tblProperties
-               """.stripMargin)
-        checkHasTz(s"insert_$baseTable", expectedTableTz)
-        rawData.write.insertInto(s"insert_$baseTable")
-        val readFromTable = sqlContext.table(s"insert_$baseTable").collect()
-          .map(_.toString()).sorted
-        // no matter what, roundtripping via the table should leave the data unchanged
-        assert(readFromTable === rawData.collect().map(_.toString()).sorted)
-
-        // Now we load the raw parquet data on disk, and check if it was adjusted correctly.
-        // Note that we only store the timezone in the table property, so when we read the
-        // data this way, we're bypassing all of the conversion logic, and reading the raw
-        // values in the parquet file.
-        val onDiskLocation = hiveTable(TableIdentifier(s"insert_$baseTable")).location.get
-        val readFromDisk = sqlContext.read.parquet(onDiskLocation).collect()
-        val storageTzId = explicitTz.getOrElse(TimeZone.getDefault().getID())
-        readFromDisk.foreach { row =>
-          val displayTime = row.getAs[String](0)
-          val millis = row.getAs[Timestamp](1).getTime()
-          val expectedMillis = timestampTimezoneToMillis((displayTime, storageTzId))
-          assert(expectedMillis === millis)
-        }
-      }
-    }
-  }
-
-  private def testReadTablesWithTimezone(
-      baseTable: String,
-      explicitTz: Option[String]): Unit = {
-    val key = ParquetFileFormat.PARQUET_TIMEZONE_TABLE_PROPERTY
-    test(s"SPARK-12297: Read from Parquet tables with Timestamps; explicitTz = $explicitTz") {
-      withTable(s"external_$baseTable") {
-        // we intentionally save this data directly, without creating a table, so we can
-        // see that the data is read back differently depending on table properties.
-        // we'll save with adjusted millis, so that it should be the correct millis after reading
-        // back.
-        val localTzID = TimeZone.getDefault().getID()
-        val rawData = createRawData()
-        // to avoid closing over entire class
-        val timestampTimezoneToMillis = this.timestampTimezoneToMillis
-        val adjustedRawData = explicitTz match {
-          case Some(tzId) =>
-            sqlContext.createDataFrame(rawData.map { row =>
-              val displayTime = row.getAs[String](0)
-              val storageMillis = timestampTimezoneToMillis((displayTime, tzId))
-              Row(displayTime, new Timestamp(storageMillis))
-            }, rawData.schema)
-          case _ =>
-            rawData
-        }
-        withTempPath { path =>
-          adjustedRawData.write.parquet(path.getCanonicalPath)
-          val tblProperties = explicitTz.map {
-            tz => raw"""TBLPROPERTIES ("$key"="$tz")"""
-          }.getOrElse("")
-          sqlContext.sql(
-            raw"""CREATE EXTERNAL TABLE external_$baseTable (
-                 | display string,
-                 | ts timestamp
-                 |)
-                 |STORED AS PARQUET
-                 |LOCATION '${path.getCanonicalPath}'
-                 |$tblProperties
-               """.stripMargin)
-          val collectedFromExternal =
-            sqlContext.sql(s"select display, ts from external_$baseTable").collect()
-          collectedFromExternal.foreach { row =>
-            val displayTime = row.getAs[String](0)
-            val millis = row.getAs[Timestamp](1).getTime()
-            assert(millis === timestampTimezoneToMillis((displayTime, localTzID)))
-          }
-
-          // Make sure functions applied to the timestamp don't use the storage time, but the
-          // converted time.  This is particularly important for partitioned tables, which are often
-          // created based on the year, month, day.
-          val extractedYear = sqlContext.sql(s"select year(ts) from external_$baseTable").collect()
-          assert(extractedYear.map{_.getInt(0)}.sorted === Array(2015, 2015, 2016, 2016))
-
-          sqlContext.read.parquet(path.getCanonicalPath).registerTempTable("raw_data")
-
-          // Now test that the behavior is still correct even with a filter which could get
-          // pushed down into parquet.  We don't need special handling for pushed down predicates
-          // because we ignore predicates on TimestampType in ParquetFilters.   This check makes
-          // sure that doesn't change.
-          // These queries should return the entire dataset, but if the predicates were
-          // applied to the raw values in parquet, they would incorrectly filter data out.
-          Seq(
-            ">" -> "2015-12-31 22:00:00",
-            "<" -> "2016-01-01 02:00:00"
-          ).foreach { case (comparison, value) =>
-            val query =
-              s"select ts from external_$baseTable where ts $comparison cast('$value' as timestamp)"
-            val countWithFilter = sqlContext.sql(query).count()
-            assert(countWithFilter === 4, query)
-          }
-        }
-      }
-    }
-  }
-
-  // TODO CDH-52854.  this depends entirely on hive validating the timezone.  When hive implements
-  // HIVE-16469, we should be able to turn this test back on (perhaps tweaking the exact check).
-  ignore("SPARK-12297: exception on bad timezone") {
-    val key = ParquetFileFormat.PARQUET_TIMEZONE_TABLE_PROPERTY
-    val badTzException = intercept[AnalysisException] {
-      sqlContext.sql(
-        raw"""CREATE TABLE bad_tz_table (
-              |  x int
-              | )
-              | STORED AS PARQUET
-              | TBLPROPERTIES ("$key"="Blart Versenwald III")
-            """.stripMargin)
-    }
-    assert(badTzException.getMessage.contains("Blart Versenwald III"))
-    sqlContext.sql(
-      raw"""CREATE TABLE flippidee_floop (
-            |  x int
-            | )
-            | STORED AS PARQUET
-            """.stripMargin)
-    val badTzAlterException = intercept[AnalysisException] {
-      sqlContext.sql(
-        raw"""ALTER TABLE flippidee_floop SET TBLPROPERTIES ("$key"="Blart Versenwald III")""")
-    }
-    assert(badTzAlterException.getMessage.contains("Blart Versenwald III"))
-  }
 }
-- 
1.7.9.5

