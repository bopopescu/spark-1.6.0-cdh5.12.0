From d9b28ed73448d335333da1eb9f3878bdc34168eb Mon Sep 17 00:00:00 2001
From: Salil Surendran <salilsurendran@cloudera.com>
Date: Tue, 7 Feb 2017 17:46:14 -0800
Subject: [PATCH 473/517] CLOUDERA-BUILD. CDH-46754. Call
 QueryExecutionListener callback methods for
 DataFrameWriter methods

Calls the QueryExecutionListener callback methods onSuccess() and onFailure() methods for DataFrameWriter output methods.  It fixes an issue where the callback methods were being called for several of the DataFrame methods like take, head, first, collect etc. but didn't get called for any of the DataFrameWriter methods like saveAsTable, save etc. Added a new property "spark.sql.queryExecutionListeners" that can be used to specify instances of QueryExecutionListeners that should be attached to the SparkSession when the spark application starts up.
---
 .../org/apache/spark/sql/DataFrameWriter.scala     |   51 +++++++--
 .../scala/org/apache/spark/sql/SQLContext.scala    |   16 ++-
 .../spark/sql/util/QueryExecutionListener.scala    |   28 +++--
 .../spark/sql/util/DataFrameCallbackSuite.scala    |  108 +++++++++++++++++---
 .../apache/spark/sql/hive/HiveDataFrameSuite.scala |   37 ++++++-
 5 files changed, 205 insertions(+), 35 deletions(-)

diff --git a/sql/core/src/main/scala/org/apache/spark/sql/DataFrameWriter.scala b/sql/core/src/main/scala/org/apache/spark/sql/DataFrameWriter.scala
index 9afa685..378d4f7 100644
--- a/sql/core/src/main/scala/org/apache/spark/sql/DataFrameWriter.scala
+++ b/sql/core/src/main/scala/org/apache/spark/sql/DataFrameWriter.scala
@@ -24,7 +24,8 @@ import scala.collection.JavaConverters._
 import org.apache.spark.annotation.Experimental
 import org.apache.spark.sql.catalyst.{SqlParser, TableIdentifier}
 import org.apache.spark.sql.catalyst.analysis.{UnresolvedAttribute, UnresolvedRelation}
-import org.apache.spark.sql.catalyst.plans.logical.{Project, InsertIntoTable}
+import org.apache.spark.sql.catalyst.plans.logical.{InsertIntoTable, Project}
+import org.apache.spark.sql.execution.QueryExecution
 import org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils
 import org.apache.spark.sql.execution.datasources.{CreateTableUsingAsSelect, ResolvedDataSource}
 import org.apache.spark.sql.sources.HadoopFsRelation
@@ -130,6 +131,37 @@ final class DataFrameWriter private[sql](df: DataFrame) {
   }
 
   /**
+   * Executes the query and calls the {@link org.apache.spark.sql.util.QueryExecutionListener}
+   * methods. The method takes all the values in the extraOptions map adds the source variable to it
+   * and then the elements in the otherOptions map before calling the methods of the
+   * QueryExecutionListener
+   *
+   * @param funcName A identifier for the method executing the query
+   * @param qe the {@link org.apache.spark.sql.execution.QueryExecution} object associated with the
+   *        query
+   * @param otherOptions a map that gets added to the map passed to the listener methods
+   * @param action the function that executes the query after which the listener methods gets
+   *               called.
+   */
+  private def executeAndCallQEListener(
+      funcName: String,
+      qe: QueryExecution,
+      otherOptions: Map[String, String] = Map())
+      (action: => Unit) = {
+      val options = extraOptions.clone() += ("source" -> source) ++= otherOptions
+      try {
+        val start = System.nanoTime()
+        action
+        val end = System.nanoTime()
+        df.sqlContext.listenerManager.onSuccess(funcName, qe, end - start, options.toMap)
+      } catch {
+        case e: Exception =>
+            df.sqlContext.listenerManager.onFailure(funcName, qe, e, options.toMap)
+            throw e
+      }
+  }
+
+  /**
    * Saves the content of the [[DataFrame]] at the specified path.
    *
    * @since 1.4.0
@@ -145,13 +177,14 @@ final class DataFrameWriter private[sql](df: DataFrame) {
    * @since 1.4.0
    */
   def save(): Unit = {
-    ResolvedDataSource(
+    lazy val dataSource = ResolvedDataSource(
       df.sqlContext,
       source,
       partitioningColumns.map(_.toArray).getOrElse(Array.empty[String]),
       mode,
       extraOptions.toMap,
       df)
+    executeAndCallQEListener("save", df.queryExecution)(dataSource)
   }
 
   /**
@@ -180,13 +213,14 @@ final class DataFrameWriter private[sql](df: DataFrame) {
       Project(inputDataCols ++ inputPartCols, df.logicalPlan)
     }.getOrElse(df.logicalPlan)
 
-    df.sqlContext.executePlan(
+    val qe = df.sqlContext.executePlan(
       InsertIntoTable(
         UnresolvedRelation(tableIdent),
         partitions.getOrElse(Map.empty[String, Option[String]]),
         input,
         overwrite,
-        ifNotExists = false)).toRdd
+        ifNotExists = false))
+    executeAndCallQEListener("insertInto", qe)(qe.toRdd)
   }
 
   private def normalizedParCols: Option[Seq[String]] = partitioningColumns.map { parCols =>
@@ -248,7 +282,8 @@ final class DataFrameWriter private[sql](df: DataFrame) {
             mode,
             extraOptions.toMap,
             df.logicalPlan)
-        df.sqlContext.executePlan(cmd).toRdd
+        val qe = df.sqlContext.executePlan(cmd)
+        executeAndCallQEListener("saveAsTable", qe)(qe.toRdd)
     }
   }
 
@@ -265,7 +300,6 @@ final class DataFrameWriter private[sql](df: DataFrame) {
    * @param connectionProperties JDBC database connection arguments, a list of arbitrary string
    *                             tag/value. Normally at least a "user" and "password" property
    *                             should be included.
-   *
    * @since 1.4.0
    */
   def jdbc(url: String, table: String, connectionProperties: Properties): Unit = {
@@ -307,8 +341,9 @@ final class DataFrameWriter private[sql](df: DataFrame) {
     } finally {
       conn.close()
     }
-
-    JdbcUtils.saveTable(df, url, table, props)
+    val options = connectionProperties.asScala += ("url" -> url) += ("table" -> table)
+    executeAndCallQEListener("jdbc", df.queryExecution,
+      options.toMap)(JdbcUtils.saveTable(df, url, table, props))
   }
 
   /**
diff --git a/sql/core/src/main/scala/org/apache/spark/sql/SQLContext.scala b/sql/core/src/main/scala/org/apache/spark/sql/SQLContext.scala
index b06be78..b9ff788 100644
--- a/sql/core/src/main/scala/org/apache/spark/sql/SQLContext.scala
+++ b/sql/core/src/main/scala/org/apache/spark/sql/SQLContext.scala
@@ -44,10 +44,10 @@ import org.apache.spark.sql.execution.datasources._
 import org.apache.spark.sql.execution.ui.{SQLListener, SQLTab}
 import org.apache.spark.sql.sources.BaseRelation
 import org.apache.spark.sql.types._
-import org.apache.spark.sql.util.ExecutionListenerManager
+import org.apache.spark.sql.util.{ExecutionListenerManager, QueryExecutionListener}
 import org.apache.spark.sql.{execution => sparkexecution}
 import org.apache.spark.util.Utils
-import org.apache.spark.{SparkContext, SparkException}
+import org.apache.spark.{SparkConf, SparkContext, SparkException}
 
 /**
  * The entry point for working with structured data (rows and columns) in Spark.  Allows the
@@ -271,6 +271,18 @@ class SQLContext private[sql](
     properties.asScala.foreach {
       case (key, value) => setConf(key, value)
     }
+
+    for(qeExecutionListener <- getQueryExecutionListeners(sparkContext.getConf))
+      listenerManager.register(qeExecutionListener)
+  }
+
+  private def getQueryExecutionListeners(conf: SparkConf): Seq[QueryExecutionListener] = {
+    val qeListenerClassNames: Seq[String] =
+      conf.get("spark.sql.queryExecutionListeners", "").split(',').map(_.trim).filter(_ != "")
+    qeListenerClassNames
+      .map(Utils.classForName(_))
+      .filter(classOf[QueryExecutionListener].isAssignableFrom(_))
+      .map(_.newInstance().asInstanceOf[QueryExecutionListener])
   }
 
   /**
diff --git a/sql/core/src/main/scala/org/apache/spark/sql/util/QueryExecutionListener.scala b/sql/core/src/main/scala/org/apache/spark/sql/util/QueryExecutionListener.scala
index ac432e2..d5c78dd 100644
--- a/sql/core/src/main/scala/org/apache/spark/sql/util/QueryExecutionListener.scala
+++ b/sql/core/src/main/scala/org/apache/spark/sql/util/QueryExecutionListener.scala
@@ -46,7 +46,11 @@ trait QueryExecutionListener {
    * @param durationNs the execution time for this query in nanoseconds.
    */
   @DeveloperApi
-  def onSuccess(funcName: String, qe: QueryExecution, durationNs: Long): Unit
+  def onSuccess(
+      funcName: String,
+      qe: QueryExecution,
+      durationNs: Long,
+      extraParams: Map[String, String] = Map.empty): Unit
 
   /**
    * A callback function that will be called when a query execution failed.
@@ -58,7 +62,11 @@ trait QueryExecutionListener {
    * @param exception the exception that failed this query.
    */
   @DeveloperApi
-  def onFailure(funcName: String, qe: QueryExecution, exception: Exception): Unit
+  def onFailure(
+      funcName: String,
+      qe: QueryExecution,
+      exception: Exception,
+      extraParams: Map[String, String] = Map.empty): Unit
 }
 
 
@@ -94,18 +102,26 @@ class ExecutionListenerManager private[sql] () extends Logging {
     listeners.clear()
   }
 
-  private[sql] def onSuccess(funcName: String, qe: QueryExecution, duration: Long): Unit = {
+  private[sql] def onSuccess(
+      funcName: String,
+      qe: QueryExecution,
+      duration: Long,
+      extraParams: Map[String, String] = Map()): Unit = {
     readLock {
       withErrorHandling { listener =>
-        listener.onSuccess(funcName, qe, duration)
+          listener.onSuccess(funcName, qe, duration, extraParams)
       }
     }
   }
 
-  private[sql] def onFailure(funcName: String, qe: QueryExecution, exception: Exception): Unit = {
+  private[sql] def onFailure(
+      funcName: String,
+      qe: QueryExecution,
+      exception: Exception,
+      extraParams: Map[String, String] = Map()): Unit = {
     readLock {
       withErrorHandling { listener =>
-        listener.onFailure(funcName, qe, exception)
+          listener.onFailure(funcName, qe, exception, extraParams)
       }
     }
   }
diff --git a/sql/core/src/test/scala/org/apache/spark/sql/util/DataFrameCallbackSuite.scala b/sql/core/src/test/scala/org/apache/spark/sql/util/DataFrameCallbackSuite.scala
index b46b0d2..d6f1705 100644
--- a/sql/core/src/test/scala/org/apache/spark/sql/util/DataFrameCallbackSuite.scala
+++ b/sql/core/src/test/scala/org/apache/spark/sql/util/DataFrameCallbackSuite.scala
@@ -18,9 +18,8 @@
 package org.apache.spark.sql.util
 
 import scala.collection.mutable.ArrayBuffer
-
 import org.apache.spark._
-import org.apache.spark.sql.{functions, QueryTest}
+import org.apache.spark.sql.{DataFrame, QueryTest, functions}
 import org.apache.spark.sql.catalyst.plans.logical.{Aggregate, Project}
 import org.apache.spark.sql.execution.QueryExecution
 import org.apache.spark.sql.test.SharedSQLContext
@@ -33,9 +32,17 @@ class DataFrameCallbackSuite extends QueryTest with SharedSQLContext {
     val metrics = ArrayBuffer.empty[(String, QueryExecution, Long)]
     val listener = new QueryExecutionListener {
       // Only test successful case here, so no need to implement `onFailure`
-      override def onFailure(funcName: String, qe: QueryExecution, exception: Exception): Unit = {}
-
-      override def onSuccess(funcName: String, qe: QueryExecution, duration: Long): Unit = {
+      override def onFailure(
+          funcName: String,
+          qe: QueryExecution,
+          exception: Exception,
+          options: Map[String, String]): Unit = {}
+
+      override def onSuccess(
+          funcName: String,
+          qe: QueryExecution,
+          duration: Long,
+          options: Map[String, String]): Unit = {
         metrics += ((funcName, qe, duration))
       }
     }
@@ -61,12 +68,20 @@ class DataFrameCallbackSuite extends QueryTest with SharedSQLContext {
   test("execute callback functions when a DataFrame action failed") {
     val metrics = ArrayBuffer.empty[(String, QueryExecution, Exception)]
     val listener = new QueryExecutionListener {
-      override def onFailure(funcName: String, qe: QueryExecution, exception: Exception): Unit = {
+      override def onFailure(
+          funcName: String,
+          qe: QueryExecution,
+          exception: Exception,
+          options: Map[String, String]): Unit = {
         metrics += ((funcName, qe, exception))
       }
 
       // Only test failed case here, so no need to implement `onSuccess`
-      override def onSuccess(funcName: String, qe: QueryExecution, duration: Long): Unit = {}
+      override def onSuccess(
+          funcName: String,
+          qe: QueryExecution,
+          duration: Long,
+          options: Map[String, String]): Unit = {}
     }
     sqlContext.listenerManager.register(listener)
 
@@ -89,10 +104,18 @@ class DataFrameCallbackSuite extends QueryTest with SharedSQLContext {
     val metrics = ArrayBuffer.empty[Long]
     val listener = new QueryExecutionListener {
       // Only test successful case here, so no need to implement `onFailure`
-      override def onFailure(funcName: String, qe: QueryExecution, exception: Exception): Unit = {}
-
-      override def onSuccess(funcName: String, qe: QueryExecution, duration: Long): Unit = {
-        metrics += qe.executedPlan.longMetric("numInputRows").value.value
+      override def onFailure(
+          funcName: String,
+          qe: QueryExecution,
+          exception: Exception,
+          options: Map[String, String]): Unit = {}
+
+      override def onSuccess(
+          funcName: String,
+          qe: QueryExecution,
+          duration: Long,
+          options: Map[String, String]): Unit = {
+          metrics += qe.executedPlan.longMetric("numInputRows").value.value
       }
     }
     sqlContext.listenerManager.register(listener)
@@ -110,6 +133,27 @@ class DataFrameCallbackSuite extends QueryTest with SharedSQLContext {
     sqlContext.listenerManager.unregister(listener)
   }
 
+  test("QueryExecutionListener gets called on DataFrameWriter.parquet method") {
+    callSave(new TestQueryExecutionListener("parquet"),
+      (df: DataFrame, path: String) => df.write.parquet(path))
+  }
+
+  test("QueryExecutionListener gets called on DataFrameWriter.json method") {
+    callSave(new TestQueryExecutionListener("json"),
+      (df: DataFrame, path: String) => df.write.json(path))
+  }
+
+  private def callSave(
+    testQueryExecutionListener: TestQueryExecutionListener,
+    callSaveFunction: (DataFrame, String) => Unit) = {
+    sqlContext.listenerManager.register(testQueryExecutionListener)
+    withTempPath { path =>
+      callSaveFunction(Seq(1 -> 100).toDF("x", "y"), path.getAbsolutePath)
+    }
+    assert(testQueryExecutionListener.onWriteSuccessCalled)
+    sqlContext.listenerManager.clear()
+  }
+
   // TODO: Currently some LongSQLMetric use -1 as initial value, so if the accumulator is never
   // updated, we can filter it out later.  However, when we aggregate(sum) accumulator values at
   // driver side for SQL physical operators, these -1 values will make our result smaller.
@@ -119,12 +163,20 @@ class DataFrameCallbackSuite extends QueryTest with SharedSQLContext {
     val metrics = ArrayBuffer.empty[Long]
     val listener = new QueryExecutionListener {
       // Only test successful case here, so no need to implement `onFailure`
-      override def onFailure(funcName: String, qe: QueryExecution, exception: Exception): Unit = {}
-
-      override def onSuccess(funcName: String, qe: QueryExecution, duration: Long): Unit = {
-        metrics += qe.executedPlan.longMetric("dataSize").value.value
-        val bottomAgg = qe.executedPlan.children(0).children(0)
-        metrics += bottomAgg.longMetric("dataSize").value.value
+      override def onFailure(
+          funcName: String,
+          qe: QueryExecution,
+          exception: Exception,
+          options: Map[String, String]): Unit = {}
+
+      override def onSuccess(
+          funcName: String,
+          qe: QueryExecution,
+          duration: Long,
+          options: Map[String, String]): Unit = {
+          metrics += qe.executedPlan.longMetric("dataSize").value.value
+          val bottomAgg = qe.executedPlan.children(0).children(0)
+          metrics += bottomAgg.longMetric("dataSize").value.value
       }
     }
     sqlContext.listenerManager.register(listener)
@@ -155,4 +207,26 @@ class DataFrameCallbackSuite extends QueryTest with SharedSQLContext {
 
     sqlContext.listenerManager.unregister(listener)
   }
+
+  class TestQueryExecutionListener(source: String) extends QueryExecutionListener {
+      var onWriteSuccessCalled = false
+    // Only test successful case here, so no need to implement `onFailure`
+    override def onFailure(
+        funcName: String,
+        qe: QueryExecution,
+        exception: Exception,
+        options: Map[String, String]): Unit = {}
+    override def onSuccess(
+        funcName: String,
+        qe: QueryExecution,
+        durationNs: Long,
+        options: Map[String, String]): Unit = {
+      assert(options.contains("path"))
+      assertResult(Some(source)) {
+        options.get("source")
+      }
+      assert(durationNs > 0)
+      onWriteSuccessCalled = true
+    }
+  }
 }
diff --git a/sql/hive/src/test/scala/org/apache/spark/sql/hive/HiveDataFrameSuite.scala b/sql/hive/src/test/scala/org/apache/spark/sql/hive/HiveDataFrameSuite.scala
index 7fdc5d7..4f7e615 100644
--- a/sql/hive/src/test/scala/org/apache/spark/sql/hive/HiveDataFrameSuite.scala
+++ b/sql/hive/src/test/scala/org/apache/spark/sql/hive/HiveDataFrameSuite.scala
@@ -17,10 +17,17 @@
 
 package org.apache.spark.sql.hive
 
+
 import org.apache.spark.sql.hive.test.TestHiveSingleton
-import org.apache.spark.sql.QueryTest
+import org.apache.spark.sql.{QueryTest}
+import org.apache.spark.sql.execution.QueryExecution
+import org.apache.spark.sql.test.SQLTestUtils
+import org.apache.spark.sql.util.QueryExecutionListener
+
+class HiveDataFrameSuite extends QueryTest with TestHiveSingleton with SQLTestUtils{
+
+  import testImplicits._
 
-class HiveDataFrameSuite extends QueryTest with TestHiveSingleton {
   test("table name with schema") {
     // regression test for SPARK-11778
     hiveContext.sql("create schema usrdb")
@@ -29,4 +36,30 @@ class HiveDataFrameSuite extends QueryTest with TestHiveSingleton {
     hiveContext.sql("drop table usrdb.test")
     hiveContext.sql("drop schema usrdb")
   }
+
+  test("QueryExecutionListener gets called on DataFrameWriter.saveAsTable method") {
+    var onWriteSuccessCalled = false
+
+    hiveContext.listenerManager.register(new QueryExecutionListener {
+      override def onFailure(
+                              funcName: String,
+                              qe: QueryExecution,
+                              exception: Exception,
+                              options: Map[String, String]): Unit = {}
+      override def onSuccess(
+                              funcName: String,
+                              qe: QueryExecution,
+                              durationNs: Long,
+                              options: Map[String, String]): Unit = {
+        assert(durationNs > 0)
+        assert(qe ne null)
+        onWriteSuccessCalled = true
+      }
+    })
+    withTable("bar") {
+      Seq(1 -> 100).toDF("x", "y").write.saveAsTable("bar")
+    }
+    assert(onWriteSuccessCalled)
+    sqlContext.listenerManager.clear()
+  }
 }
-- 
1.7.9.5

